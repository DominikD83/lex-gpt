{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/31treehaus/opt/anaconda3/envs/ml/lib/python3.9/site-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import glob \n",
    "import os,re\n",
    "import yt_dlp\n",
    "import json\n",
    "import httplib2\n",
    "import openai\n",
    "import requests\n",
    "import pinecone \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "from langchain import OpenAI\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "from langchain.llms import OpenAIChat\n",
    "from langchain.chains import VectorDBQA\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.vectorstores import Chroma\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "from langchain.vectorstores import Pinecone\n",
    "from youtubesearchpython import ChannelsSearch\n",
    "from langchain.chains import VectorDBQAWithSourcesChain\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get video metadata - \n",
    " \n",
    "* https://pypi.org/project/youtube-search-python/\n",
    "* `Problem: Only gets first 100!` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Videos Retrieved: 100\n"
     ]
    }
   ],
   "source": [
    "# get channel \n",
    "channelsSearch = ChannelsSearch('lexfridman', limit = 10, region = 'US')\n",
    "r = channelsSearch.result()\n",
    "id = r['result'][0]['id']\n",
    "\n",
    "# get all videos\n",
    "channel_id = \"UCSHZKyawb77ixDdsGog4iWA\"\n",
    "playlist = Playlist(playlist_from_channel_id(channel_id))\n",
    "print(f'Videos Retrieved: {len(playlist.videos)}')\n",
    "\n",
    "# store\n",
    "stor_metadata=pd.DataFrame()\n",
    "for v in playlist.videos:\n",
    "    stor_metadata.loc[v['title'],'link']=v['link']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download audio - \n",
    "\n",
    "* https://github.com/yt-dlp/yt-dlp/wiki/Installation\n",
    "* https://github.com/yt-dlp/yt-dlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ydl_opts = {\n",
    "    'format': 'm4a/bestaudio/best',\n",
    "    # ℹ️ See help(yt_dlp.postprocessor) for a list of available Postprocessors and their arguments\n",
    "    'postprocessors': [{  # Extract audio using ffmpeg\n",
    "        'key': 'FFmpegExtractAudio',\n",
    "        'preferredcodec': 'm4a',\n",
    "    }]\n",
    "}\n",
    "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "    error_code = ydl.download(str(url))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Transcribe`\n",
    "\n",
    "* `Problem`: `Maximum content size limit (26214400) exceeded (56033653 bytes read)` w/ API\n",
    "* Need to do it manually on GPU machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fis = glob.glob(\"audio/*m4a\")\n",
    "fis_s=sorted(fis)\n",
    "fis_s[1]\n",
    "\n",
    "file = open(fis_s[0], \"rb\")\n",
    "transcription = openai.Audio.transcribe(\"whisper-1\", file)\n",
    "print(transcription)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Read existing transcriptions`\n",
    "\n",
    "* https://karpathy.ai/lexicap/index.html\n",
    "* https://langchain.readthedocs.io/en/latest/modules/indexes/chain_examples/qa_with_sources.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://karpathy.ai/lexicap/0001-large.html',\n",
       " 'https://karpathy.ai/lexicap/0002-large.html',\n",
       " 'https://karpathy.ai/lexicap/0003-large.html']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all pages \n",
    "http = httplib2.Http()\n",
    "status, response = http.request(\"https://karpathy.ai/lexicap/\")\n",
    "links = []\n",
    "for link in BeautifulSoup(response, 'html.parser', parse_only=SoupStrainer('a')):\n",
    "    if link.has_attr('href'):\n",
    "        links.append(link['href'])\n",
    "links_tx = [\"https://karpathy.ai/lexicap/\"+l for l in links if \"0\" in l]\n",
    "links_tx[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing: https://karpathy.ai/lexicap/0001-large.html\n",
      "41\n",
      "41\n",
      "Writing: https://karpathy.ai/lexicap/0002-large.html\n",
      "32\n",
      "32\n",
      "Writing: https://karpathy.ai/lexicap/0003-large.html\n",
      "19\n",
      "19\n",
      "Writing: https://karpathy.ai/lexicap/0004-large.html\n",
      "19\n",
      "19\n",
      "Writing: https://karpathy.ai/lexicap/0005-large.html\n",
      "20\n",
      "20\n",
      "Writing: https://karpathy.ai/lexicap/0006-large.html\n",
      "34\n",
      "34\n",
      "Writing: https://karpathy.ai/lexicap/0007-large.html\n",
      "52\n",
      "52\n",
      "Writing: https://karpathy.ai/lexicap/0008-large.html\n",
      "17\n",
      "17\n",
      "Writing: https://karpathy.ai/lexicap/0009-large.html\n",
      "38\n",
      "38\n",
      "Writing: https://karpathy.ai/lexicap/0010-large.html\n",
      "23\n",
      "23\n",
      "Writing: https://karpathy.ai/lexicap/0011-large.html\n",
      "34\n",
      "34\n",
      "Writing: https://karpathy.ai/lexicap/0012-large.html\n",
      "33\n",
      "33\n",
      "Writing: https://karpathy.ai/lexicap/0013-large.html\n",
      "31\n",
      "31\n",
      "Writing: https://karpathy.ai/lexicap/0014-large.html\n",
      "32\n",
      "32\n",
      "Writing: https://karpathy.ai/lexicap/0015-large.html\n",
      "31\n",
      "31\n",
      "Writing: https://karpathy.ai/lexicap/0016-large.html\n",
      "40\n",
      "40\n",
      "Writing: https://karpathy.ai/lexicap/0017-large.html\n",
      "48\n",
      "48\n",
      "Writing: https://karpathy.ai/lexicap/0018-large.html\n",
      "16\n",
      "16\n",
      "Writing: https://karpathy.ai/lexicap/0019-large.html\n",
      "37\n",
      "37\n",
      "Writing: https://karpathy.ai/lexicap/0020-large.html\n",
      "53\n",
      "53\n",
      "Writing: https://karpathy.ai/lexicap/0021-large.html\n",
      "40\n",
      "40\n",
      "Writing: https://karpathy.ai/lexicap/0022-large.html\n",
      "35\n",
      "35\n",
      "Writing: https://karpathy.ai/lexicap/0023-large.html\n",
      "36\n",
      "36\n",
      "Writing: https://karpathy.ai/lexicap/0024-large.html\n",
      "29\n",
      "29\n",
      "Writing: https://karpathy.ai/lexicap/0025-large.html\n",
      "73\n",
      "73\n",
      "Writing: https://karpathy.ai/lexicap/0026-large.html\n",
      "19\n",
      "19\n",
      "Writing: https://karpathy.ai/lexicap/0027-large.html\n",
      "38\n",
      "38\n",
      "Writing: https://karpathy.ai/lexicap/0028-large.html\n",
      "21\n",
      "21\n",
      "Writing: https://karpathy.ai/lexicap/0029-large.html\n",
      "55\n",
      "55\n",
      "Writing: https://karpathy.ai/lexicap/0030-large.html\n",
      "27\n",
      "27\n",
      "Writing: https://karpathy.ai/lexicap/0031-large.html\n",
      "62\n",
      "62\n",
      "Writing: https://karpathy.ai/lexicap/0032-large.html\n",
      "27\n",
      "27\n",
      "Writing: https://karpathy.ai/lexicap/0033-large.html\n",
      "34\n",
      "34\n",
      "Writing: https://karpathy.ai/lexicap/0034-large.html\n",
      "24\n",
      "24\n",
      "Writing: https://karpathy.ai/lexicap/0035-large.html\n",
      "50\n",
      "50\n",
      "Writing: https://karpathy.ai/lexicap/0036-large.html\n",
      "38\n",
      "38\n",
      "Writing: https://karpathy.ai/lexicap/0037-large.html\n",
      "27\n",
      "27\n",
      "Writing: https://karpathy.ai/lexicap/0038-large.html\n",
      "56\n",
      "56\n",
      "Writing: https://karpathy.ai/lexicap/0039-large.html\n",
      "15\n",
      "15\n",
      "Writing: https://karpathy.ai/lexicap/0040-large.html\n",
      "37\n",
      "37\n",
      "Writing: https://karpathy.ai/lexicap/0041-large.html\n",
      "26\n",
      "26\n",
      "Writing: https://karpathy.ai/lexicap/0042-large.html\n",
      "30\n",
      "30\n",
      "Writing: https://karpathy.ai/lexicap/0043-large.html\n",
      "48\n",
      "48\n",
      "Writing: https://karpathy.ai/lexicap/0044-large.html\n",
      "72\n",
      "72\n",
      "Writing: https://karpathy.ai/lexicap/0045-large.html\n",
      "28\n",
      "28\n",
      "Writing: https://karpathy.ai/lexicap/0046-large.html\n",
      "26\n",
      "26\n",
      "Writing: https://karpathy.ai/lexicap/0047-large.html\n",
      "49\n",
      "49\n",
      "Writing: https://karpathy.ai/lexicap/0048-large.html\n",
      "42\n",
      "42\n",
      "Writing: https://karpathy.ai/lexicap/0049-large.html\n",
      "18\n",
      "18\n",
      "Writing: https://karpathy.ai/lexicap/0050-large.html\n",
      "55\n",
      "55\n",
      "Writing: https://karpathy.ai/lexicap/0051-large.html\n",
      "23\n",
      "23\n",
      "Writing: https://karpathy.ai/lexicap/0052-large.html\n",
      "20\n",
      "20\n",
      "Writing: https://karpathy.ai/lexicap/0053-large.html\n",
      "16\n",
      "16\n",
      "Writing: https://karpathy.ai/lexicap/0054-large.html\n",
      "42\n",
      "42\n",
      "Writing: https://karpathy.ai/lexicap/0055-large.html\n",
      "42\n",
      "42\n",
      "Writing: https://karpathy.ai/lexicap/0056-large.html\n",
      "33\n",
      "33\n",
      "Writing: https://karpathy.ai/lexicap/0057-large.html\n",
      "53\n",
      "53\n",
      "Writing: https://karpathy.ai/lexicap/0058-large.html\n",
      "29\n",
      "29\n",
      "Writing: https://karpathy.ai/lexicap/0059-large.html\n",
      "42\n",
      "42\n",
      "Writing: https://karpathy.ai/lexicap/0060-large.html\n",
      "47\n",
      "47\n",
      "Writing: https://karpathy.ai/lexicap/0061-large.html\n",
      "48\n",
      "48\n",
      "Writing: https://karpathy.ai/lexicap/0062-large.html\n",
      "42\n",
      "42\n",
      "Writing: https://karpathy.ai/lexicap/0063-large.html\n",
      "42\n",
      "42\n",
      "Writing: https://karpathy.ai/lexicap/0064-large.html\n",
      "35\n",
      "35\n",
      "Writing: https://karpathy.ai/lexicap/0065-large.html\n",
      "33\n",
      "33\n",
      "Writing: https://karpathy.ai/lexicap/0066-large.html\n",
      "46\n",
      "46\n",
      "Writing: https://karpathy.ai/lexicap/0067-large.html\n",
      "28\n",
      "28\n",
      "Writing: https://karpathy.ai/lexicap/0068-large.html\n",
      "43\n",
      "43\n",
      "Writing: https://karpathy.ai/lexicap/0069-large.html\n",
      "50\n",
      "50\n",
      "Writing: https://karpathy.ai/lexicap/0070-large.html\n",
      "46\n",
      "46\n",
      "Writing: https://karpathy.ai/lexicap/0071-large.html\n",
      "39\n",
      "39\n",
      "Writing: https://karpathy.ai/lexicap/0072-large.html\n",
      "47\n",
      "47\n",
      "Writing: https://karpathy.ai/lexicap/0073-large.html\n",
      "48\n",
      "48\n",
      "Writing: https://karpathy.ai/lexicap/0074-large.html\n",
      "63\n",
      "63\n",
      "Writing: https://karpathy.ai/lexicap/0075-large.html\n",
      "49\n",
      "49\n",
      "Writing: https://karpathy.ai/lexicap/0076-large.html\n",
      "30\n",
      "30\n",
      "Writing: https://karpathy.ai/lexicap/0077-large.html\n",
      "33\n",
      "33\n",
      "Writing: https://karpathy.ai/lexicap/0078-large.html\n",
      "28\n",
      "28\n",
      "Writing: https://karpathy.ai/lexicap/0079-large.html\n",
      "30\n",
      "30\n",
      "Writing: https://karpathy.ai/lexicap/0080-large.html\n",
      "47\n",
      "47\n",
      "Writing: https://karpathy.ai/lexicap/0081-large.html\n",
      "49\n",
      "49\n",
      "Writing: https://karpathy.ai/lexicap/0082-large.html\n",
      "19\n",
      "19\n",
      "Writing: https://karpathy.ai/lexicap/0083-large.html\n",
      "56\n",
      "56\n",
      "Writing: https://karpathy.ai/lexicap/0084-large.html\n",
      "43\n",
      "43\n",
      "Writing: https://karpathy.ai/lexicap/0085-large.html\n",
      "44\n",
      "44\n",
      "Writing: https://karpathy.ai/lexicap/0086-large.html\n",
      "53\n",
      "53\n",
      "Writing: https://karpathy.ai/lexicap/0087-large.html\n",
      "32\n",
      "32\n",
      "Writing: https://karpathy.ai/lexicap/0088-large.html\n",
      "79\n",
      "79\n",
      "Writing: https://karpathy.ai/lexicap/0089-large.html\n",
      "100\n",
      "100\n",
      "Writing: https://karpathy.ai/lexicap/0090-large.html\n",
      "47\n",
      "47\n",
      "Writing: https://karpathy.ai/lexicap/0091-large.html\n",
      "23\n",
      "23\n",
      "Writing: https://karpathy.ai/lexicap/0092-large.html\n",
      "57\n",
      "57\n",
      "Writing: https://karpathy.ai/lexicap/0093-large.html\n",
      "34\n",
      "34\n",
      "Writing: https://karpathy.ai/lexicap/0094-large.html\n",
      "50\n",
      "50\n",
      "Writing: https://karpathy.ai/lexicap/0095-large.html\n",
      "60\n",
      "60\n",
      "Writing: https://karpathy.ai/lexicap/0096-large.html\n",
      "28\n",
      "28\n",
      "Writing: https://karpathy.ai/lexicap/0097-large.html\n",
      "45\n",
      "45\n",
      "Writing: https://karpathy.ai/lexicap/0098-large.html\n",
      "35\n",
      "35\n",
      "Writing: https://karpathy.ai/lexicap/0099-large.html\n",
      "41\n",
      "41\n",
      "Writing: https://karpathy.ai/lexicap/0101-large.html\n",
      "98\n",
      "98\n",
      "Writing: https://karpathy.ai/lexicap/0102-large.html\n",
      "38\n",
      "38\n",
      "Writing: https://karpathy.ai/lexicap/0103-large.html\n",
      "121\n",
      "121\n",
      "Writing: https://karpathy.ai/lexicap/0104-large.html\n",
      "55\n",
      "55\n",
      "Writing: https://karpathy.ai/lexicap/0105-large.html\n",
      "31\n",
      "31\n",
      "Writing: https://karpathy.ai/lexicap/0106-large.html\n",
      "55\n",
      "55\n",
      "Writing: https://karpathy.ai/lexicap/0107-large.html\n",
      "32\n",
      "32\n",
      "Writing: https://karpathy.ai/lexicap/0108-large.html\n",
      "53\n",
      "53\n",
      "Writing: https://karpathy.ai/lexicap/0109-large.html\n",
      "52\n",
      "52\n",
      "Writing: https://karpathy.ai/lexicap/0110-large.html\n",
      "43\n",
      "43\n",
      "Writing: https://karpathy.ai/lexicap/0111-large.html\n",
      "49\n",
      "49\n",
      "Writing: https://karpathy.ai/lexicap/0112-large.html\n",
      "53\n",
      "53\n",
      "Writing: https://karpathy.ai/lexicap/0113-large.html\n",
      "74\n",
      "74\n",
      "Writing: https://karpathy.ai/lexicap/0114-large.html\n",
      "78\n",
      "78\n",
      "Writing: https://karpathy.ai/lexicap/0115-large.html\n",
      "59\n",
      "59\n",
      "Writing: https://karpathy.ai/lexicap/0116-large.html\n",
      "52\n",
      "52\n",
      "Writing: https://karpathy.ai/lexicap/0117-large.html\n",
      "69\n",
      "69\n",
      "Writing: https://karpathy.ai/lexicap/0118-large.html\n",
      "72\n",
      "72\n",
      "Writing: https://karpathy.ai/lexicap/0119-large.html\n",
      "54\n",
      "54\n",
      "Writing: https://karpathy.ai/lexicap/0120-large.html\n",
      "70\n",
      "70\n",
      "Writing: https://karpathy.ai/lexicap/0121-large.html\n",
      "86\n",
      "86\n",
      "Writing: https://karpathy.ai/lexicap/0122-large.html\n",
      "133\n",
      "133\n",
      "Writing: https://karpathy.ai/lexicap/0123-large.html\n",
      "62\n",
      "62\n",
      "Writing: https://karpathy.ai/lexicap/0124-large.html\n",
      "140\n",
      "140\n",
      "Writing: https://karpathy.ai/lexicap/0125-large.html\n",
      "128\n",
      "128\n",
      "Writing: https://karpathy.ai/lexicap/0126-large.html\n",
      "38\n",
      "38\n",
      "Writing: https://karpathy.ai/lexicap/0127-large.html\n",
      "39\n",
      "39\n",
      "Writing: https://karpathy.ai/lexicap/0128-large.html\n",
      "103\n",
      "103\n",
      "Writing: https://karpathy.ai/lexicap/0129-large.html\n",
      "59\n",
      "59\n",
      "Writing: https://karpathy.ai/lexicap/0130-large.html\n",
      "56\n",
      "56\n",
      "Writing: https://karpathy.ai/lexicap/0131-large.html\n",
      "87\n",
      "87\n",
      "Writing: https://karpathy.ai/lexicap/0132-large.html\n",
      "92\n",
      "92\n",
      "Writing: https://karpathy.ai/lexicap/0133-large.html\n",
      "76\n",
      "76\n",
      "Writing: https://karpathy.ai/lexicap/0134-large.html\n",
      "79\n",
      "79\n",
      "Writing: https://karpathy.ai/lexicap/0135-large.html\n",
      "79\n",
      "79\n",
      "Writing: https://karpathy.ai/lexicap/0136-large.html\n",
      "100\n",
      "100\n",
      "Writing: https://karpathy.ai/lexicap/0137-large.html\n",
      "76\n",
      "76\n",
      "Writing: https://karpathy.ai/lexicap/0138-large.html\n",
      "82\n",
      "82\n",
      "Writing: https://karpathy.ai/lexicap/0139-large.html\n",
      "81\n",
      "81\n",
      "Writing: https://karpathy.ai/lexicap/0140-large.html\n",
      "60\n",
      "60\n",
      "Writing: https://karpathy.ai/lexicap/0141-large.html\n",
      "54\n",
      "54\n",
      "Writing: https://karpathy.ai/lexicap/0142-large.html\n",
      "61\n",
      "61\n",
      "Writing: https://karpathy.ai/lexicap/0143-large.html\n",
      "87\n",
      "87\n",
      "Writing: https://karpathy.ai/lexicap/0144-large.html\n",
      "60\n",
      "60\n",
      "Writing: https://karpathy.ai/lexicap/0145-large.html\n",
      "107\n",
      "107\n",
      "Writing: https://karpathy.ai/lexicap/0146-large.html\n",
      "66\n",
      "66\n",
      "Writing: https://karpathy.ai/lexicap/0147-large.html\n",
      "72\n",
      "72\n",
      "Writing: https://karpathy.ai/lexicap/0148-large.html\n",
      "68\n",
      "68\n",
      "Writing: https://karpathy.ai/lexicap/0149-large.html\n",
      "81\n",
      "81\n",
      "Writing: https://karpathy.ai/lexicap/0150-large.html\n",
      "88\n",
      "88\n",
      "Writing: https://karpathy.ai/lexicap/0151-large.html\n",
      "45\n",
      "45\n",
      "Writing: https://karpathy.ai/lexicap/0152-large.html\n",
      "32\n",
      "32\n",
      "Writing: https://karpathy.ai/lexicap/0153-large.html\n",
      "54\n",
      "54\n",
      "Writing: https://karpathy.ai/lexicap/0154-large.html\n",
      "77\n",
      "77\n",
      "Writing: https://karpathy.ai/lexicap/0155-large.html\n",
      "90\n",
      "90\n",
      "Writing: https://karpathy.ai/lexicap/0156-large.html\n",
      "77\n",
      "77\n",
      "Writing: https://karpathy.ai/lexicap/0157-large.html\n",
      "42\n",
      "42\n",
      "Writing: https://karpathy.ai/lexicap/0158-large.html\n",
      "42\n",
      "42\n",
      "Writing: https://karpathy.ai/lexicap/0159-large.html\n",
      "51\n",
      "51\n",
      "Writing: https://karpathy.ai/lexicap/0160-large.html\n",
      "100\n",
      "100\n",
      "Writing: https://karpathy.ai/lexicap/0161-large.html\n",
      "73\n",
      "73\n",
      "Writing: https://karpathy.ai/lexicap/0162-large.html\n",
      "81\n",
      "81\n",
      "Writing: https://karpathy.ai/lexicap/0163-large.html\n",
      "75\n",
      "75\n",
      "Writing: https://karpathy.ai/lexicap/0164-large.html\n",
      "91\n",
      "91\n",
      "Writing: https://karpathy.ai/lexicap/0165-large.html\n",
      "62\n",
      "62\n",
      "Writing: https://karpathy.ai/lexicap/0166-large.html\n",
      "105\n",
      "105\n",
      "Writing: https://karpathy.ai/lexicap/0167-large.html\n",
      "96\n",
      "96\n",
      "Writing: https://karpathy.ai/lexicap/0168-large.html\n",
      "49\n",
      "49\n",
      "Writing: https://karpathy.ai/lexicap/0169-large.html\n",
      "100\n",
      "100\n",
      "Writing: https://karpathy.ai/lexicap/0170-large.html\n",
      "41\n",
      "41\n",
      "Writing: https://karpathy.ai/lexicap/0171-large.html\n",
      "99\n",
      "99\n",
      "Writing: https://karpathy.ai/lexicap/0172-large.html\n",
      "71\n",
      "71\n",
      "Writing: https://karpathy.ai/lexicap/0173-large.html\n",
      "70\n",
      "70\n",
      "Writing: https://karpathy.ai/lexicap/0174-large.html\n",
      "65\n",
      "65\n",
      "Writing: https://karpathy.ai/lexicap/0175-large.html\n",
      "60\n",
      "60\n",
      "Writing: https://karpathy.ai/lexicap/0176-large.html\n",
      "115\n",
      "115\n",
      "Writing: https://karpathy.ai/lexicap/0177-large.html\n",
      "58\n",
      "58\n",
      "Writing: https://karpathy.ai/lexicap/0178-large.html\n",
      "147\n",
      "147\n",
      "Writing: https://karpathy.ai/lexicap/0179-large.html\n",
      "74\n",
      "74\n",
      "Writing: https://karpathy.ai/lexicap/0180-large.html\n",
      "66\n",
      "66\n",
      "Writing: https://karpathy.ai/lexicap/0181-large.html\n",
      "92\n",
      "92\n",
      "Writing: https://karpathy.ai/lexicap/0182-large.html\n",
      "98\n",
      "98\n",
      "Writing: https://karpathy.ai/lexicap/0183-large.html\n",
      "73\n",
      "73\n",
      "Writing: https://karpathy.ai/lexicap/0184-large.html\n",
      "57\n",
      "57\n",
      "Writing: https://karpathy.ai/lexicap/0185-large.html\n",
      "89\n",
      "89\n",
      "Writing: https://karpathy.ai/lexicap/0186-large.html\n",
      "71\n",
      "71\n",
      "Writing: https://karpathy.ai/lexicap/0187-large.html\n",
      "57\n",
      "57\n",
      "Writing: https://karpathy.ai/lexicap/0188-large.html\n",
      "93\n",
      "93\n",
      "Writing: https://karpathy.ai/lexicap/0189-large.html\n",
      "51\n",
      "51\n",
      "Writing: https://karpathy.ai/lexicap/0190-large.html\n",
      "87\n",
      "87\n",
      "Writing: https://karpathy.ai/lexicap/0191-large.html\n",
      "128\n",
      "128\n",
      "Writing: https://karpathy.ai/lexicap/0192-large.html\n",
      "178\n",
      "178\n",
      "Writing: https://karpathy.ai/lexicap/0193-large.html\n",
      "90\n",
      "90\n",
      "Writing: https://karpathy.ai/lexicap/0194-large.html\n",
      "94\n",
      "94\n",
      "Writing: https://karpathy.ai/lexicap/0195-large.html\n",
      "56\n",
      "56\n",
      "Writing: https://karpathy.ai/lexicap/0196-large.html\n",
      "55\n",
      "55\n",
      "Writing: https://karpathy.ai/lexicap/0197-large.html\n",
      "54\n",
      "54\n",
      "Writing: https://karpathy.ai/lexicap/0198-large.html\n",
      "64\n",
      "64\n",
      "Writing: https://karpathy.ai/lexicap/0199-large.html\n",
      "62\n",
      "62\n",
      "Writing: https://karpathy.ai/lexicap/0200-large.html\n",
      "83\n",
      "83\n",
      "Writing: https://karpathy.ai/lexicap/0201-large.html\n",
      "71\n",
      "71\n",
      "Writing: https://karpathy.ai/lexicap/0202-large.html\n",
      "74\n",
      "74\n",
      "Writing: https://karpathy.ai/lexicap/0203-large.html\n",
      "55\n",
      "55\n",
      "Writing: https://karpathy.ai/lexicap/0204-large.html\n",
      "73\n",
      "73\n",
      "Writing: https://karpathy.ai/lexicap/0205-large.html\n",
      "106\n",
      "106\n",
      "Writing: https://karpathy.ai/lexicap/0206-large.html\n",
      "87\n",
      "87\n",
      "Writing: https://karpathy.ai/lexicap/0207-large.html\n",
      "82\n",
      "82\n",
      "Writing: https://karpathy.ai/lexicap/0208-large.html\n",
      "75\n",
      "75\n",
      "Writing: https://karpathy.ai/lexicap/0209-large.html\n",
      "88\n",
      "88\n",
      "Writing: https://karpathy.ai/lexicap/0210-large.html\n",
      "77\n",
      "77\n",
      "Writing: https://karpathy.ai/lexicap/0211-large.html\n",
      "54\n",
      "54\n",
      "Writing: https://karpathy.ai/lexicap/0212-large.html\n",
      "98\n",
      "98\n",
      "Writing: https://karpathy.ai/lexicap/0213-large.html\n",
      "60\n",
      "60\n",
      "Writing: https://karpathy.ai/lexicap/0214-large.html\n",
      "47\n",
      "47\n",
      "Writing: https://karpathy.ai/lexicap/0215-large.html\n",
      "81\n",
      "81\n",
      "Writing: https://karpathy.ai/lexicap/0216-large.html\n",
      "101\n",
      "101\n",
      "Writing: https://karpathy.ai/lexicap/0217-large.html\n",
      "64\n",
      "64\n",
      "Writing: https://karpathy.ai/lexicap/0218-large.html\n",
      "55\n",
      "55\n",
      "Writing: https://karpathy.ai/lexicap/0219-large.html\n",
      "53\n",
      "53\n",
      "Writing: https://karpathy.ai/lexicap/0220-large.html\n",
      "78\n",
      "78\n",
      "Writing: https://karpathy.ai/lexicap/0221-large.html\n",
      "78\n",
      "78\n",
      "Writing: https://karpathy.ai/lexicap/0222-large.html\n",
      "58\n",
      "58\n",
      "Writing: https://karpathy.ai/lexicap/0223-large.html\n",
      "104\n",
      "104\n",
      "Writing: https://karpathy.ai/lexicap/0224-large.html\n",
      "107\n",
      "107\n",
      "Writing: https://karpathy.ai/lexicap/0225-large.html\n",
      "90\n",
      "90\n",
      "Writing: https://karpathy.ai/lexicap/0226-large.html\n",
      "43\n",
      "43\n",
      "Writing: https://karpathy.ai/lexicap/0227-large.html\n",
      "77\n",
      "77\n",
      "Writing: https://karpathy.ai/lexicap/0228-large.html\n",
      "37\n",
      "37\n",
      "Writing: https://karpathy.ai/lexicap/0229-large.html\n",
      "64\n",
      "64\n",
      "Writing: https://karpathy.ai/lexicap/0230-large.html\n",
      "123\n",
      "123\n",
      "Writing: https://karpathy.ai/lexicap/0231-large.html\n",
      "83\n",
      "83\n",
      "Writing: https://karpathy.ai/lexicap/0232-large.html\n",
      "51\n",
      "51\n",
      "Writing: https://karpathy.ai/lexicap/0233-large.html\n",
      "33\n",
      "33\n",
      "Writing: https://karpathy.ai/lexicap/0234-large.html\n",
      "120\n",
      "120\n",
      "Writing: https://karpathy.ai/lexicap/0235-large.html\n",
      "52\n",
      "52\n",
      "Writing: https://karpathy.ai/lexicap/0236-large.html\n",
      "72\n",
      "72\n",
      "Writing: https://karpathy.ai/lexicap/0237-large.html\n",
      "95\n",
      "95\n",
      "Writing: https://karpathy.ai/lexicap/0238-large.html\n",
      "41\n",
      "41\n",
      "Writing: https://karpathy.ai/lexicap/0239-large.html\n",
      "74\n",
      "74\n",
      "Writing: https://karpathy.ai/lexicap/0240-large.html\n",
      "64\n",
      "64\n",
      "Writing: https://karpathy.ai/lexicap/0241-large.html\n",
      "106\n",
      "106\n",
      "Writing: https://karpathy.ai/lexicap/0242-large.html\n",
      "78\n",
      "78\n",
      "Writing: https://karpathy.ai/lexicap/0243-large.html\n",
      "84\n",
      "84\n",
      "Writing: https://karpathy.ai/lexicap/0244-large.html\n",
      "82\n",
      "82\n",
      "Writing: https://karpathy.ai/lexicap/0245-large.html\n",
      "27\n",
      "27\n",
      "Writing: https://karpathy.ai/lexicap/0246-large.html\n",
      "68\n",
      "68\n",
      "Writing: https://karpathy.ai/lexicap/0247-large.html\n",
      "136\n",
      "136\n",
      "Writing: https://karpathy.ai/lexicap/0248-large.html\n",
      "62\n",
      "62\n",
      "Writing: https://karpathy.ai/lexicap/0249-large.html\n",
      "31\n",
      "31\n",
      "Writing: https://karpathy.ai/lexicap/0250-large.html\n",
      "90\n",
      "90\n",
      "Writing: https://karpathy.ai/lexicap/0251-large.html\n",
      "38\n",
      "38\n",
      "Writing: https://karpathy.ai/lexicap/0252-large.html\n",
      "65\n",
      "65\n",
      "Writing: https://karpathy.ai/lexicap/0253-large.html\n",
      "100\n",
      "100\n",
      "Writing: https://karpathy.ai/lexicap/0254-large.html\n",
      "68\n",
      "68\n",
      "Writing: https://karpathy.ai/lexicap/0255-large.html\n",
      "56\n",
      "56\n",
      "Writing: https://karpathy.ai/lexicap/0256-large.html\n",
      "89\n",
      "89\n",
      "Writing: https://karpathy.ai/lexicap/0257-large.html\n",
      "131\n",
      "131\n",
      "Writing: https://karpathy.ai/lexicap/0258-large.html\n",
      "83\n",
      "83\n",
      "Writing: https://karpathy.ai/lexicap/0259-large.html\n",
      "59\n",
      "59\n",
      "Writing: https://karpathy.ai/lexicap/0260-large.html\n",
      "90\n",
      "90\n",
      "Writing: https://karpathy.ai/lexicap/0261-large.html\n",
      "79\n",
      "79\n",
      "Writing: https://karpathy.ai/lexicap/0262-large.html\n",
      "46\n",
      "46\n",
      "Writing: https://karpathy.ai/lexicap/0263-large.html\n",
      "59\n",
      "59\n",
      "Writing: https://karpathy.ai/lexicap/0264-large.html\n",
      "90\n",
      "90\n",
      "Writing: https://karpathy.ai/lexicap/0265-large.html\n",
      "57\n",
      "57\n",
      "Writing: https://karpathy.ai/lexicap/0266-large.html\n",
      "55\n",
      "55\n",
      "Writing: https://karpathy.ai/lexicap/0267-large.html\n",
      "61\n",
      "61\n",
      "Writing: https://karpathy.ai/lexicap/0269-large.html\n",
      "124\n",
      "124\n",
      "Writing: https://karpathy.ai/lexicap/0270-large.html\n",
      "61\n",
      "61\n",
      "Writing: https://karpathy.ai/lexicap/0271-large.html\n",
      "58\n",
      "58\n",
      "Writing: https://karpathy.ai/lexicap/0272-large.html\n",
      "108\n",
      "108\n",
      "Writing: https://karpathy.ai/lexicap/0273-large.html\n",
      "85\n",
      "85\n",
      "Writing: https://karpathy.ai/lexicap/0274-large.html\n",
      "94\n",
      "94\n",
      "Writing: https://karpathy.ai/lexicap/0275-large.html\n",
      "43\n",
      "43\n",
      "Writing: https://karpathy.ai/lexicap/0276-large.html\n",
      "104\n",
      "104\n",
      "Writing: https://karpathy.ai/lexicap/0277-large.html\n",
      "108\n",
      "108\n",
      "Writing: https://karpathy.ai/lexicap/0278-large.html\n",
      "65\n",
      "65\n",
      "Writing: https://karpathy.ai/lexicap/0279-large.html\n",
      "131\n",
      "131\n",
      "Writing: https://karpathy.ai/lexicap/0280-large.html\n",
      "33\n",
      "33\n",
      "Writing: https://karpathy.ai/lexicap/0281-large.html\n",
      "64\n",
      "64\n",
      "Writing: https://karpathy.ai/lexicap/0284-large.html\n",
      "134\n",
      "134\n",
      "Writing: https://karpathy.ai/lexicap/0285-large.html\n",
      "92\n",
      "92\n",
      "Writing: https://karpathy.ai/lexicap/0286-large.html\n",
      "53\n",
      "53\n",
      "Writing: https://karpathy.ai/lexicap/0288-large.html\n",
      "115\n",
      "115\n",
      "Writing: https://karpathy.ai/lexicap/0289-large.html\n",
      "69\n",
      "69\n",
      "Writing: https://karpathy.ai/lexicap/0290-large.html\n",
      "63\n",
      "63\n",
      "Writing: https://karpathy.ai/lexicap/0292-large.html\n",
      "130\n",
      "130\n",
      "Writing: https://karpathy.ai/lexicap/0293-large.html\n",
      "94\n",
      "94\n",
      "Writing: https://karpathy.ai/lexicap/0294-large.html\n",
      "82\n",
      "82\n",
      "Writing: https://karpathy.ai/lexicap/0295-large.html\n",
      "75\n",
      "75\n",
      "Writing: https://karpathy.ai/lexicap/0296-large.html\n",
      "74\n",
      "74\n",
      "Writing: https://karpathy.ai/lexicap/0297-large.html\n",
      "82\n",
      "82\n",
      "Writing: https://karpathy.ai/lexicap/0298-large.html\n",
      "58\n",
      "58\n",
      "Writing: https://karpathy.ai/lexicap/0299-large.html\n",
      "71\n",
      "71\n",
      "Writing: https://karpathy.ai/lexicap/0300-large.html\n",
      "49\n",
      "49\n",
      "Writing: https://karpathy.ai/lexicap/0301-large.html\n",
      "89\n",
      "89\n",
      "Writing: https://karpathy.ai/lexicap/0302-large.html\n",
      "67\n",
      "67\n",
      "Writing: https://karpathy.ai/lexicap/0303-large.html\n",
      "107\n",
      "107\n",
      "Writing: https://karpathy.ai/lexicap/0304-large.html\n",
      "56\n",
      "56\n",
      "Writing: https://karpathy.ai/lexicap/0305-large.html\n",
      "61\n",
      "61\n",
      "Writing: https://karpathy.ai/lexicap/0306-large.html\n",
      "62\n",
      "62\n",
      "Writing: https://karpathy.ai/lexicap/0307-large.html\n",
      "83\n",
      "83\n",
      "Writing: https://karpathy.ai/lexicap/0308-large.html\n",
      "81\n",
      "81\n",
      "Writing: https://karpathy.ai/lexicap/0309-large.html\n",
      "167\n",
      "167\n",
      "Writing: https://karpathy.ai/lexicap/0310-large.html\n",
      "118\n",
      "118\n",
      "Writing: https://karpathy.ai/lexicap/0311-large.html\n",
      "100\n",
      "100\n",
      "Writing: https://karpathy.ai/lexicap/0312-large.html\n",
      "94\n",
      "94\n",
      "Writing: https://karpathy.ai/lexicap/0313-large.html\n",
      "87\n",
      "87\n",
      "Writing: https://karpathy.ai/lexicap/0314-large.html\n",
      "111\n",
      "111\n",
      "Writing: https://karpathy.ai/lexicap/0315-large.html\n",
      "66\n",
      "66\n",
      "Writing: https://karpathy.ai/lexicap/0316-large.html\n",
      "17\n",
      "17\n",
      "Writing: https://karpathy.ai/lexicap/0317-large.html\n",
      "88\n",
      "88\n",
      "Writing: https://karpathy.ai/lexicap/0318-large.html\n",
      "109\n",
      "109\n",
      "Writing: https://karpathy.ai/lexicap/0319-large.html\n",
      "83\n",
      "83\n",
      "Writing: https://karpathy.ai/lexicap/0320-large.html\n",
      "60\n",
      "60\n",
      "Writing: https://karpathy.ai/lexicap/0321-large.html\n",
      "39\n",
      "39\n",
      "Writing: https://karpathy.ai/lexicap/0322-large.html\n",
      "76\n",
      "76\n",
      "Writing: https://karpathy.ai/lexicap/0323-large.html\n",
      "70\n",
      "70\n",
      "Writing: https://karpathy.ai/lexicap/0324-large.html\n",
      "83\n",
      "83\n",
      "Writing: https://karpathy.ai/lexicap/0325-large.html\n",
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "# Get text -\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(string=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    return u\" \".join(t.strip() for t in visible_texts)\n",
    "\n",
    "def get_text_and_title(url):\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    t=(text_from_html(html))\n",
    "    title=t.split(\"|\")[0].split(\"back to index\")[1].strip()\n",
    "    return t, title\n",
    "\n",
    "# Get links -\n",
    "def get_links(URL):\n",
    "    http = httplib2.Http()\n",
    "    status, response = http.request(URL)\n",
    "    links = []\n",
    "    for link in BeautifulSoup(response, 'html.parser', parse_only=SoupStrainer('a')):\n",
    "        if link.has_attr('href'):\n",
    "            links.append(link['href'])\n",
    "    links_clean = [l for l in links if \"https\" in l]\n",
    "    return links_clean\n",
    "\n",
    "# Get image -\n",
    "def get_img(URL,title,episode_id):\n",
    "    response = requests.get(URL)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    img_tags = soup.find_all('img')\n",
    "    urls = [img['src'] for img in img_tags]\n",
    "    for url in urls:\n",
    "        response = requests.get(url)\n",
    "        imgpath=\"img/%s.jpg\"%episode_id\n",
    "        with open(imgpath, 'wb') as f:\n",
    "            if 'http' not in url:\n",
    "                # sometimes an image source can be relative \n",
    "                # if it is provide the base url which also happens \n",
    "                # to be the site variable atm. \n",
    "                url = '{}{}'.format(site, url)\n",
    "            response = requests.get(url)\n",
    "            f.write(response.content)\n",
    "    return imgpath\n",
    "\n",
    "# Full pipeline - \n",
    "def pre_process(URL,episode_id):\n",
    "\n",
    "    t,title=get_text_and_title(URL)\n",
    "    links=get_links(URL)\n",
    "    img=get_img(URL,title,episode_id)\n",
    "    stor_chunk = pd.DataFrame()\n",
    "    stor_chunk['chunks']= t.split(\"link |\")\n",
    "    stor_chunk['clean_chunks']=stor_chunk['chunks'].apply(lambda x: re.sub(r\"[^a-zA-Z ]+\", '', x)).apply(lambda x: x.strip())\n",
    "    stor_chunk['links']=links\n",
    "    all_text = stor_chunk['clean_chunks'].str.cat(sep=' ')\n",
    "    return all_text, links, title\n",
    "\n",
    "def make_splits(chunks,URL):\n",
    "\n",
    "    # ID\n",
    "    episode_id=URL.split(\"/\")[-1].split(\"-\")[0]\n",
    "\n",
    "    # Pre-processing\n",
    "    texts,links,title=pre_process(URL,episode_id)\n",
    "    \n",
    "    # Splits \n",
    "    # text_splitter = CharacterTextSplitter(chunk_size=chunks, chunk_overlap=50, separator=\" \") \n",
    "    # texts = text_splitter.split_text(texts)\n",
    "    # print(len(texts)) \n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunks, chunk_overlap=50) \n",
    "    texts_recusive = text_splitter.split_text(texts)\n",
    "    print(len(texts_recusive)) \n",
    "\n",
    "    # Metadata \n",
    "    N = len(texts_recusive) \n",
    "    bins = np.linspace(0, len(links)-1, N, dtype=int)\n",
    "    sampled_links = [links[i] for i in bins]\n",
    "    # Here we can add \"link\", \"title\", etc that can be fetched in the app \n",
    "    metadatas=[{\"source\":title + \" \" +link,\"id\":episode_id,\"link\":link,\"title\":title} for link in sampled_links]\n",
    "    print(len(metadatas))\n",
    "\n",
    "    return texts_recusive,metadatas,title,episode_id\n",
    "\n",
    "# *** Chunk size: key parameter *** \n",
    "chunks = 1500\n",
    "# *** Chunk size: key parameter *** \n",
    "splits_all = [ ]\n",
    "metadatas_all = [ ]\n",
    "titles = [ ]\n",
    " \n",
    "# Iterate \n",
    "stor=pd.DataFrame()\n",
    "for page in links_tx:\n",
    "    # try:\n",
    "    print(\"Writing: %s\"%page)\n",
    "    splits,metadatas,title,episode_id=make_splits(chunks,page)\n",
    "    stor.loc[episode_id,'title']=title \n",
    "    with open('docs/%s.txt'%episode_id, \"w\") as f:\n",
    "        for string in splits:\n",
    "            f.write(string + \"\\n\") \n",
    "    f.close()\n",
    "    with open('metadatas/%s.json'%episode_id, \"w\") as f:\n",
    "        json.dump(metadatas, f)\n",
    "    f.close()\n",
    "    splits_all.append(splits)\n",
    "    metadatas_all.append(metadatas)\n",
    "    # except:\n",
    "    #    print(\"Error on page: %s\"%page)\n",
    "    \n",
    "# Combine all \n",
    "splits_combined = []\n",
    "for sublist in splits_all:\n",
    "    splits_combined.extend(sublist)\n",
    "metadatas_combined = []\n",
    "for sublist in metadatas_all:\n",
    "    metadatas_combined.extend(sublist)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Read in the saved files`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_saved_doc(doc):\n",
    "\n",
    "    dir_path, file_name = os.path.split(doc)\n",
    "    name, ext = os.path.splitext(file_name)\n",
    "\n",
    "    with open(doc, \"r\") as f:\n",
    "        my_strings = f.readlines()\n",
    "    f.close()\n",
    "\n",
    "    doc_to_read_txt = [string.strip() for string in my_strings]\n",
    "\n",
    "    with open('metadatas/%s.json'%name, \"r\") as f:\n",
    "        doc_to_read_metadatas = json.load(f)\n",
    "    f.close()\n",
    "\n",
    "    return doc_to_read_txt,doc_to_read_metadatas\n",
    "\n",
    "# Read files in \n",
    "docs = glob.glob(\"docs/*\")\n",
    "splits_all_read_fromdisk = []\n",
    "metadatas_all_readfromdisk = []\n",
    "for doc_to_read in docs:\n",
    "    doc_to_read_txt,doc_to_read_metadatas = get_saved_doc(doc_to_read)\n",
    "    splits_all_read_fromdisk.append(doc_to_read_txt)\n",
    "    metadatas_all_readfromdisk.append(doc_to_read_metadatas)\n",
    "\n",
    "# Combine \n",
    "splits_combined_fromdisk = []\n",
    "metadatas_combined_fromdisk = []\n",
    "for sublist in splits_all_read_fromdisk:\n",
    "    splits_combined_fromdisk.extend(sublist)\n",
    "for sublist in metadatas_all_readfromdisk:\n",
    "    metadatas_combined_fromdisk.extend(sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20632\n",
      "back to index Nic Carter Bitcoin Core Values Layered Scaling and Blocksize Debates  Lex Fridman Podcast  small model  large model The following is a conversation with Nick Carter who is a partner at Castle Island Ventures cofounder of CoinMetricsio and previously a crypto asset research analyst at Fidelity Investments Hes a prominent writer speaker and podcaster on topics around decentralized finance and especially Bitcoin Quick mention of our sponsors The Information Athletic Greens Four Sigmatic and Blinkist Check them out in the description to support this podcast This conversation with Nick Carter is part of a series of episodes on cryptocurrency that is a small journey of exploration Im on because I find decentralized finance and especially Bitcoin fascinating technically and philosophically especially because it may be the very mechanism that achieves a global decentralization of power giving more sovereignty to the individual and making our systems more resilient to corruption manipulation and in general to the darker side of human nature Please let me also address something for a few minutes that happened recently thats been weighing heavy on me If you find me annoying to listen to please skip to the actual conversation with Nick I had a recent podcast episode with Anthony Pompliano where we spoke about Bitcoin and life in general for three hours I was curious inspired positive or at least I tried to be as I usually do Someone clipped out out of context a short segment of me mumbling something about having a PhD and I started getting mocked online because that made it convenient for people to mock me for being yet another quote unquote expert who learns about Bitcoin and thinks he knows everything I almost never mentioned that I have a PhD except to make fun\n",
      "20632\n",
      "{'source': 'Nic Carter: Bitcoin Core Values, Layered Scaling, and Blocksize Debates https://www.youtube.com/watch?v=mDyBbGCiBUU', 'id': '0173', 'link': 'https://www.youtube.com/watch?v=mDyBbGCiBUU', 'title': 'Nic Carter: Bitcoin Core Values, Layered Scaling, and Blocksize Debates'}\n"
     ]
    }
   ],
   "source": [
    "print(len(splits_combined_fromdisk))\n",
    "print(splits_combined_fromdisk[0])\n",
    "print(len(metadatas_combined_fromdisk))\n",
    "print(metadatas_combined_fromdisk[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20632\n",
      "back to index Max Tegmark Life   Lex Fridman Podcast  small model  large model As part of MIT course S Artificial General Intelligence Ive gotten the chance to sit down with Max Tegmark He is a professor here at MIT Hes a physicist spent a large part of his career studying the mysteries of our cosmological universe But hes also studied and delved into the beneficial possibilities and the existential risks of artificial intelligence Amongst many other things he is the cofounder of the Future of Life Institute author of two books both of which I highly recommend First Our Mathematical Universe Second is Life Hes truly an out of the box thinker and a fun personality so I really enjoy talking to him If youd like to see more of these videos in the future please subscribe and also click the little bell icon to make sure you dont miss any videos Also Twitter LinkedIn agimitedu if you wanna watch other lectures or conversations like this one Better yet go read Maxs book Life Chapter seven on goals is my favorite Its really where philosophy and engineering come together and it opens with a quote by Dostoevsky The mystery of human existence lies not in just staying alive but in finding something to live for Lastly I believe that every failure rewards us with an opportunity to learn and in that sense Ive been very fortunate to fail in so many new and exciting ways and this conversation was no different Ive learned about something called radio frequency interference RFI look it up Apparently music and conversations from local radio stations can bleed into the audio that youre recording in such a way that it almost completely ruins that audio Its an exceptionally difficult sound source to remove So Ive gotten the opportunity to learn how to avoid RFI in the future during recording sessions Ive also\n",
      "20632\n",
      "{'source': 'Max Tegmark: Life 3.0 https://www.youtube.com/watch?v=Gi8LUnhP5yU', 'id': '0001', 'link': 'https://www.youtube.com/watch?v=Gi8LUnhP5yU', 'title': 'Max Tegmark: Life 3.0'}\n"
     ]
    }
   ],
   "source": [
    "print(len(splits_combined))\n",
    "print(splits_combined[0])\n",
    "print(len(metadatas_combined))\n",
    "print(metadatas_combined[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n",
      "62\n",
      "133\n",
      "133\n"
     ]
    }
   ],
   "source": [
    "oriol_text,oriol_metadatas=get_saved_doc('docs/0306.txt')\n",
    "demis_text,demis_metadatas=get_saved_doc('docs/0299.txt')\n",
    "txt_test=oriol_text+demis_text\n",
    "metadatas_test=oriol_metadatas+demis_metadatas\n",
    "print(len(oriol_text))\n",
    "print(len(oriol_metadatas))\n",
    "print(len(txt_test))\n",
    "print(len(metadatas_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Embedding: small-scale test`\n",
    "\n",
    "In Python, we have many options:\n",
    "\n",
    "https://langchain.readthedocs.io/en/latest/reference/modules/vectorstore.html\n",
    "\n",
    "Only Pinecone, Chroma can be used w/ JS: \n",
    "\n",
    "https://hwchase17.github.io/langchainjs/docs/modules/indexes/vector_stores/supabase/\n",
    "\n",
    "Let's test a small DB w/ Pinecone and Chroma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Chroma using direct local API.\n",
      "Using DuckDB in-memory for database. Data will be transient.\n"
     ]
    }
   ],
   "source": [
    "# Test for Chroma \n",
    "embeddings = OpenAIEmbeddings()\n",
    "c_test = Chroma.from_texts(txt_test, embeddings, metadatas=metadatas_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "f_test = FAISS.from_texts(txt_test, embeddings, metadatas=metadatas_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for pinecone \n",
    "pinecone.init(\n",
    "    api_key=\"66b41af0-4796-4bae-84a0-2409e6babab6\",  \n",
    "    environment=\"us-east1-gcp\"  \n",
    ")\n",
    "index_name = \"lex-test\"\n",
    "embeddings = OpenAIEmbeddings()\n",
    "# p_test = Pinecone.from_texts(txt_test, embeddings, index_name=index_name, metadatas=metadatas_test)\n",
    "p_test = Pinecone.from_existing_index(index_name=index_name,embedding=embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Embed full dataset`\n",
    "\n",
    "* Took 1hr to index ~6k vectors (44% of the data)\n",
    "* See `JSONDecodeError` with Chroma and FAISS; needs JSON strings \n",
    "* But it is not reproduced; e.g., I did not see it the second time I tried FAISS\n",
    "\n",
    "https://docs.pinecone.io/docs/insert-data\n",
    "https://langchain.readthedocs.io/en/latest/modules/indexes/how_to_guides.html\n",
    "https://langchain.readthedocs.io/en/latest/reference/modules/vectorstore.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, dict in enumerate(metadatas_combined):\n",
    "    \n",
    "    try:\n",
    "        # encode the dictionary into a JSON string\n",
    "        json_str = json.dumps(dict)\n",
    "        # decode the JSON string into a dictionary\n",
    "        # json_obj = json.loads(json_str)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSONDecodeError in object {i}: {e}\")\n",
    "\n",
    "metadatas_combined_json_str=[json.dumps(dict) for dict in metadatas_combined]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting: Cleaning up .chroma directory\n"
     ]
    }
   ],
   "source": [
    "faiss_docstore = FAISS.from_texts(splits_combined, embeddings, metadatas=metadatas_combined)\n",
    "faiss_docstore.save_local('index/lex_faiss')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~ 1 hr to hit `index 55`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write final settings to Pinecone for prod \n",
    "pinecone.init(\n",
    "    api_key=\"66b41af0-4796-4bae-84a0-2409e6babab6\",  \n",
    "    environment=\"us-east1-gcp\"  \n",
    ")\n",
    "index_name = \"lex-gpt\"\n",
    "embeddings = OpenAIEmbeddings()\n",
    "# p = Pinecone.from_texts(splits_combined[0:2], embeddings, index_name=index_name, metadatas=metadatas_combined[0:2])\n",
    "p = Pinecone.from_existing_index(index_name=index_name,embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add all data \n",
    "import math, time \n",
    "chunk_size = 100\n",
    "num_chunks = math.ceil(len(splits_combined) / chunk_size)\n",
    "for i in range(last_chunk,num_chunks):\n",
    "    \n",
    "    print(i)\n",
    "    start_time = time.time()\n",
    "    # try:\n",
    "\n",
    "    # Calculate the start and end indices for the current chunk\n",
    "    start_idx = i * chunk_size\n",
    "    end_idx = min(start_idx + chunk_size, len(splits_combined))\n",
    "\n",
    "    # Extract the current chunk\n",
    "    current_splits = splits_combined[start_idx:end_idx]\n",
    "    current_metadatas = metadatas_combined[start_idx:end_idx]\n",
    "\n",
    "    # Add the current chunk to the vector database\n",
    "    p.add_texts(texts = current_splits, metadatas=current_metadatas)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "    print(\"--------\")\n",
    "\n",
    "    # except:\n",
    "\n",
    "    #    print(\"Error at chunk: %s\"%i)\n",
    "    #    time.sleep(5)\n",
    "\n",
    "# docsearch_demis = Pinecone.from_texts(splits_combined, embeddings, index_name=index_name, metadatas=metadatas_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CHROMA\")\n",
    "embeddings = OpenAIEmbeddings()\n",
    "chroma_docstore = Chroma.from_texts(splits_combined, embeddings, metadatas=metadatas_combined_json_str,persist_directory=\"index/\",collection_name=\"lex-gpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15296\n",
      "15296\n"
     ]
    }
   ],
   "source": [
    "print(len(splits_combined))\n",
    "print(len(metadatas_combined))\n",
    "lex_index = Pinecone.from_existing_index(index_name=index_name,embedding=embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Run chain`\n",
    "\n",
    "https://langchain.readthedocs.io/en/latest/modules/indexes/chain_examples/qa_with_sources.html#\n",
    "https://langchain.readthedocs.io/en/latest/modules/indexes/chain_examples/vector_db_qa_with_sources.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "\n",
    "def run_qa_chain(llm,query,docstore):\n",
    "\n",
    "    start_time = time.time()\n",
    "    docs = docstore.similarity_search(query)\n",
    "    chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "    a = chain.run(input_documents=docs, question=query)\n",
    "    print(a[\"output_text\"])\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "    print(\"--------\")\n",
    "\n",
    "def run_qa_sources_chain(llm,query,docstore):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    docs = docstore.similarity_search(query)\n",
    "    chain = load_qa_with_sources_chain(llm, chain_type=\"stuff\")\n",
    "    a = chain({\"input_documents\": docs, \"question\": query})\n",
    "    print(a[\"output_text\"])\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "    print(\"--------\")\n",
    "\n",
    "def run_vectordbqa_chain(llm,query,docstore):\n",
    "\n",
    "    start_time = time.time()\n",
    "    chain = VectorDBQA.from_chain_type(llm, chain_type=\"stuff\", vectorstore=docstore)\n",
    "    a = chain({\"query\": query})\n",
    "    print(a['result'])\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "    print(\"--------\")\n",
    "\n",
    "def run_vectordb_sources_chain(llm,query,docstore):\n",
    "\n",
    "    start_time = time.time()\n",
    "    chain = VectorDBQAWithSourcesChain.from_chain_type(llm, chain_type=\"stuff\", vectorstore=docstore)\n",
    "    a = chain({\"question\": query},return_only_outputs=True)\n",
    "    print(a[\"answer\"])\n",
    "    print(a[\"sources\"])\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "    print(\"--------\")\n",
    "\n",
    "llm = OpenAIChat(temperature=0)\n",
    "\n",
    "# run_qa_chain(llm,q,p_oriol)\n",
    "# run_qa_sources_chain(llm,q,p_oriol) \n",
    "# run_vectordbqa_chain(llm,q,p_oriol)\n",
    "#run_vectordb_sources_chain(llm,q,p_test)\n",
    "#run_vectordb_sources_chain(llm,q,p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Gato model is a transformer neural network that models sequences of actions, words, and observations, and is trained through a massive imitation learning algorithm that imitates data sets of observations. It is a general agent that can take actions in an environment and generate the next step. It can be thought of as inputs of images, text, video, and actions, and outputs words and actions. One possible next step for the Gato model is to convert even images into language through a crude semantic segmentation. \n",
      "\n",
      "Oriol Vinyals: Deep Learning and Artificial General Intelligence https://www.youtube.com/watch?v=aGBLRlLe7X8&t=1461, https://www.youtube.com/watch?v=aGBLRlLe7X8&t=1557, https://www.youtube.com/watch?v=aGBLRlLe7X8&t=2715\n",
      "Elapsed time: 13.23695421218872 seconds\n",
      "--------\n",
      "The Gato model is a transformer neural network that models sequences of actions, words, and observations, and is trained through a massive imitation learning algorithm that imitates data sets of observations. It is a general agent that can take actions in an environment and generate the next step. It can be thought of as inputs of images, text, video, and actions, and outputs words and actions. One possible next step for the Gato model is to convert even images into language through a crude semantic segmentation. \n",
      "\n",
      "Oriol Vinyals: Deep Learning and Artificial General Intelligence https://www.youtube.com/watch?v=aGBLRlLe7X8&t=1461, https://www.youtube.com/watch?v=aGBLRlLe7X8&t=1557, https://www.youtube.com/watch?v=aGBLRlLe7X8&t=2715\n",
      "Elapsed time: 12.829474925994873 seconds\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "q = \"What does the Gato model do?\"\n",
    "run_vectordb_sources_chain(llm,q,p_test)\n",
    "run_vectordb_sources_chain(llm,q,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future path for AGI involves creating benchmarks to test generalizability and scaling up models to be more powerful. Language is likely to be a key area where AGI will shine. There are still breakthroughs to be made, but progress is being made towards creating a single model that can be taught new tasks interactively. The Turing test is still seen as a critical benchmark for AGI. \n",
      "\n",
      "Oriol Vinyals: Deep Learning and Artificial General Intelligence https://www.youtube.com/watch?v=aGBLRlLe7X8&t=7665, Oriol Vinyals: Deep Learning and Artificial General Intelligence https://www.youtube.com/watch?v=aGBLRlLe7X8&t=3696, Oriol Vinyals: Deep Learning and Artificial General Intelligence https://www.youtube.com/watch?v=aGBLRlLe7X8&t=1230, Demis Hassabis: DeepMind - AI, Superintelligence & the Future of Humanity https://www.youtube.com/watch?v=Gfr50f6ZBvo&t=368\n",
      "Elapsed time: 15.502749919891357 seconds\n",
      "--------\n",
      "The future path for AGI involves interactive teaching and the development of more powerful models, with progress being benchmarked towards performance across all tasks. Breakthroughs are still needed, but there is excitement about the potential for automation to improve access to resources and quality of life. The true nature of reality is a question that could be asked of an AGI system, but the answer may be beyond human understanding. \n",
      "\n",
      "Oriol Vinyals: Deep Learning and Artificial General Intelligence https://www.youtube.com/watch?v=aGBLRlLe7X8&t=7665, Oriol Vinyals: Deep Learning and Artificial General Intelligence https://www.youtube.com/watch?v=aGBLRlLe7X8&t=3696, Oriol Vinyals: Deep Learning and Artificial General Intelligence https://www.youtube.com/watch?v=aGBLRlLe7X8&t=1230, Demis Hassabis: DeepMind - AI, Superintelligence & the Future of Humanity https://www.youtube.com/watch?v=Gfr50f6ZBvo&t=7714\n",
      "Elapsed time: 15.11668610572815 seconds\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "q = \"What is the future path for AGI?\"\n",
    "#run_vectordb_sources_chain(llm,q,p_test)\n",
    "#run_vectordb_sources_chain(llm,q,p)\n",
    "run_vectordb_sources_chain(llm,q,c_test)\n",
    "run_vectordb_sources_chain(llm,q,f_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future path for AGI involves creating benchmarks to test generalizability and scaling up models to be more powerful. Language is likely to be a key area where AGI will shine. There are still breakthroughs to be made, but progress is being made towards a single model that can be taught new tasks interactively. The Turing test is still seen as a critical benchmark for AGI. \n",
      "SOURCES: Oriol Vinyals: Deep Learning and Artificial General Intelligence https://www.youtube.com/watch?v=aGBLRlLe7X8&t=7665, Oriol Vinyals: Deep Learning and Artificial General Intelligence https://www.youtube.com/watch?v=aGBLRlLe7X8&t=3696, Oriol Vinyals: Deep Learning and Artificial General Intelligence https://www.youtube.com/watch?v=aGBLRlLe7X8&t=1230, Demis Hassabis: DeepMind - AI, Superintelligence & the Future of Humanity https://www.youtube.com/watch?v=Gfr50f6ZBvo&t=368\n",
      "Elapsed time: 16.62952184677124 seconds\n",
      "--------\n",
      "The future path for AGI involves interactive teaching and the development of more powerful models, with progress being benchmarked towards performance across all tasks. Breakthroughs are still needed, but there is excitement about the potential for automation to improve access to resources and quality of life. The true nature of reality is a question that could be asked of an AGI system, but the answer may be beyond human understanding. \n",
      "SOURCES: Oriol Vinyals: Deep Learning and Artificial General Intelligence https://www.youtube.com/watch?v=aGBLRlLe7X8&t=7665, Oriol Vinyals: Deep Learning and Artificial General Intelligence https://www.youtube.com/watch?v=aGBLRlLe7X8&t=3696, Oriol Vinyals: Deep Learning and Artificial General Intelligence https://www.youtube.com/watch?v=aGBLRlLe7X8&t=1230, Demis Hassabis: DeepMind - AI, Superintelligence & the Future of Humanity https://www.youtube.com/watch?v=Gfr50f6ZBvo&t=7714\n",
      "Elapsed time: 15.809668779373169 seconds\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "run_qa_sources_chain(llm,q,c_test)\n",
    "run_qa_sources_chain(llm,q,f_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
