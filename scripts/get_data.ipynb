{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob \n",
    "import os,re\n",
    "import yt_dlp\n",
    "import json\n",
    "import time\n",
    "import httplib2\n",
    "import openai\n",
    "import requests\n",
    "import pinecone \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "from langchain import OpenAI\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "from langchain.llms import OpenAIChat\n",
    "from langchain.chains import VectorDBQA\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.vectorstores import Chroma\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "from langchain.vectorstores import Pinecone\n",
    "from youtubesearchpython import ChannelsSearch\n",
    "from langchain.chains import VectorDBQAWithSourcesChain\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Get video metadata -` \n",
    " \n",
    "* https://pypi.org/project/youtube-search-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get channel \n",
    "channelsSearch = ChannelsSearch('lexfridman', limit = 10, region = 'US')\n",
    "r = channelsSearch.result()\n",
    "id = r['result'][0]['id']\n",
    "\n",
    "# get all videos\n",
    "channel_id = \"UCSHZKyawb77ixDdsGog4iWA\"\n",
    "playlist = Playlist(playlist_from_channel_id(channel_id))\n",
    "print(f'Videos Retrieved: {len(playlist.videos)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all episode data\n",
    "stor_metadata=pd.DataFrame()\n",
    "for v in playlist.videos:\n",
    "    try:\n",
    "        ep_number = int(v['title'].split(\"|\")[-1].split(\"#\")[-1])\n",
    "        stor_metadata.loc[v['title'],'number']=ep_number\n",
    "        stor_metadata.loc[v['title'],'link']=v['link']\n",
    "        stor_metadata.loc[v['title'],'title']=v['title']\n",
    "    except:\n",
    "        print(\"Failed on %s\", v['title'])\n",
    "\n",
    "# Filter for those you want to dump audio\n",
    "# Using existing @karpathy transcriptions for 1-325\n",
    "# https://karpathy.ai/lexicap/0058-large.html\n",
    "new_ep = stor_metadata[stor_metadata.number > 325]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Download audio -` \n",
    "\n",
    "* https://github.com/yt-dlp/yt-dlp/wiki/Installation\n",
    "* https://github.com/yt-dlp/yt-dlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save img and audio\n",
    "for ix in new_ep.index:\n",
    "    # get data \n",
    "    ep_number=int(new_ep.loc[ix,'number'])\n",
    "    print(\"EPISODE: %s\"%ep_number)\n",
    "    img_url=new_ep.loc[ix,'img']\n",
    "    ep_link=new_ep.loc[ix,'link']\n",
    "    # write img \n",
    "    with open(\"img/%s.jpg\"%str(ep_number), 'wb') as f:\n",
    "        response = requests.get(img_url)\n",
    "        f.write(response.content)\n",
    "    # write audio\n",
    "    ydl_opts = {\n",
    "    'format': 'm4a/bestaudio/best',\n",
    "    'outtmpl': 'audio/%s.m4a'%str(ep_number),\n",
    "    'noplaylist': True,\n",
    "    'postprocessors': [{  \n",
    "        'key': 'FFmpegExtractAudio',\n",
    "        'preferredcodec': 'm4a',\n",
    "    }]}\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        error_code = ydl.download(ep_link)\n",
    "\n",
    "# Save \n",
    "new_ep.reset_index().to_csv(\"audio_transcription/episodes.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Transcribe newer episodes`\n",
    " \n",
    "* Do this on GPU, ideally \n",
    "* ~10-20 min / video on 2080Ti with `medium` model size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python run_whisper.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Scrape existing transcriptions`\n",
    "\n",
    "* https://karpathy.ai/lexicap/index.html\n",
    "* https://langchain.readthedocs.io/en/latest/modules/indexes/chain_examples/qa_with_sources.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all pages \n",
    "http = httplib2.Http()\n",
    "status, response = http.request(\"https://karpathy.ai/lexicap/\")\n",
    "links = []\n",
    "for link in BeautifulSoup(response, 'html.parser', parse_only=SoupStrainer('a')):\n",
    "    if link.has_attr('href'):\n",
    "        links.append(link['href'])\n",
    "links_tx = [\"https://karpathy.ai/lexicap/\"+l for l in links if \"0\" in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get text -\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(string=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    return u\" \".join(t.strip() for t in visible_texts)\n",
    "\n",
    "def get_text_and_title(url):\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    t=(text_from_html(html))\n",
    "    title=t.split(\"|\")[0].split(\"back to index\")[1].strip()\n",
    "    return t, title\n",
    "\n",
    "# Get links -\n",
    "def get_links(URL):\n",
    "    http = httplib2.Http()\n",
    "    status, response = http.request(URL)\n",
    "    links = []\n",
    "    for link in BeautifulSoup(response, 'html.parser', parse_only=SoupStrainer('a')):\n",
    "        if link.has_attr('href'):\n",
    "            links.append(link['href'])\n",
    "    links_clean = [l for l in links if \"https\" in l]\n",
    "    return links_clean\n",
    "\n",
    "# Get image -\n",
    "def get_img(URL,title,episode_id):\n",
    "    response = requests.get(URL)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    img_tags = soup.find_all('img')\n",
    "    urls = [img['src'] for img in img_tags]\n",
    "    for url in urls:\n",
    "        response = requests.get(url)\n",
    "        imgpath=\"../public/0%s.jpg\"%episode_id\n",
    "        with open(imgpath, 'wb') as f:\n",
    "            if 'http' not in url:\n",
    "                url = '{}{}'.format(site, url)\n",
    "            response = requests.get(url)\n",
    "            f.write(response.content)\n",
    "    return imgpath\n",
    "\n",
    "# Full pipeline - \n",
    "def pre_process(URL,episode_id):\n",
    "\n",
    "    t,title=get_text_and_title(URL)\n",
    "    links=get_links(URL)\n",
    "    img=get_img(URL,title,episode_id)\n",
    "    stor_chunk = pd.DataFrame()\n",
    "    stor_chunk['chunks']= t.split(\"link |\")\n",
    "    stor_chunk['clean_chunks']=stor_chunk['chunks'].apply(lambda x: re.sub(r\"[^a-zA-Z ]+\", '', x)).apply(lambda x: x.strip())\n",
    "    stor_chunk['links']=links\n",
    "    all_text = stor_chunk['clean_chunks'].str.cat(sep=' ')\n",
    "    return all_text, links, title\n",
    "\n",
    "# Make splits - \n",
    "def make_splits(chunks,URL):\n",
    "\n",
    "    # ID\n",
    "    episode_id=URL.split(\"/\")[-1].split(\"-\")[0]\n",
    "\n",
    "    # Pre-processing\n",
    "    texts,links,title=pre_process(URL,episode_id)\n",
    "    \n",
    "    # Splits \n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunks, \n",
    "                                                   chunk_overlap=50) \n",
    "    texts_recusive = text_splitter.split_text(texts)\n",
    "    print(len(texts_recusive)) \n",
    "\n",
    "    # Metadata \n",
    "    N = len(texts_recusive) \n",
    "    bins = np.linspace(0, len(links)-1, N, dtype=int)\n",
    "    sampled_links = [links[i] for i in bins]\n",
    "    # Here we can add \"link\", \"title\", etc that can be fetched in the app \n",
    "    metadatas=[{\"source\":title + \" \" +link,\"id\":episode_id,\"link\":link,\"title\":title} for link in sampled_links]\n",
    "    print(len(metadatas))\n",
    "    return texts_recusive,metadatas,title,episode_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** Chunk size: key parameter *** \n",
    "chunks = 1500\n",
    "# *** Chunk size: key parameter *** \n",
    "splits_all = [ ]\n",
    "metadatas_all = [ ]\n",
    " \n",
    "# Iterate \n",
    "stor=pd.DataFrame()\n",
    "for page in links_tx:\n",
    "    try:\n",
    "        print(\"Writing: %s\"%page)\n",
    "        splits,metadatas,title,episode_id=make_splits(chunks,page)\n",
    "        stor.loc[episode_id,'title']=title \n",
    "        with open('docs/%s.txt'%episode_id, \"w\") as f:\n",
    "            for string in splits:\n",
    "                f.write(string + \"\\n\") \n",
    "        f.close()\n",
    "        with open('metadatas/%s.json'%episode_id, \"w\") as f:\n",
    "            json.dump(metadatas, f)\n",
    "        f.close()\n",
    "        splits_all.append(splits)\n",
    "        metadatas_all.append(metadatas)\n",
    "    except:\n",
    "        print(\"Error on page: %s\"%page)\n",
    "    \n",
    "# Combine all \n",
    "splits_combined = []\n",
    "for sublist in splits_all:\n",
    "    splits_combined.extend(sublist)\n",
    "metadatas_combined = []\n",
    "for sublist in metadatas_all:\n",
    "    metadatas_combined.extend(sublist)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Get newer transcripts`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the csv file\n",
    "new_ep=pd.read_csv(\"audio_transcription/episodes.csv\",index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** Chunk size: key parameter *** \n",
    "chunks = 1500\n",
    "# *** Chunk size: key parameter *** \n",
    "splits_all_new = [ ]\n",
    "metadatas_all_new = [ ]\n",
    "\n",
    "for ix in new_ep.index:\n",
    "\n",
    "    # Get data\n",
    "    title=new_ep.loc[ix,'title']\n",
    "    ep_number=int(new_ep.loc[ix,'number'])\n",
    "    episode_id=\"0\"+str(ep_number) # consistency w/ convention used in Karpathy transcription\n",
    "    out_file_path='audio_transcription/%s.txt'%str(episode_id)\n",
    "    transcript=pd.read_csv(out_file_path,sep='\\t',header=None)\n",
    "    transcript.columns=['links','time','chunks']\n",
    "    transcript['clean_chunks']=transcript['chunks'].astype(str).apply(lambda x: x.strip())\n",
    "    links = list(transcript['links'])\n",
    "    texts = transcript['clean_chunks'].str.cat(sep=' ')\n",
    "\n",
    "    # Splits \n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunks, \n",
    "                                                   chunk_overlap=50) \n",
    "    splits = text_splitter.split_text(texts)\n",
    "    print(len(splits)) \n",
    "\n",
    "    # Metadata \n",
    "    N = len(splits) \n",
    "    bins = np.linspace(0, len(links)-1, N, dtype=int)\n",
    "    sampled_links = [links[i] for i in bins]\n",
    "    \n",
    "    # Here we can add \"link\", \"title\", etc that can be fetched in the app \n",
    "    metadatas=[{\"source\":title + \" \" +link,\"id\":episode_id,\"link\":link,\"title\":title} for link in sampled_links]\n",
    "    print(len(metadatas)) \n",
    "\n",
    "    splits_all_new.append(splits)\n",
    "    metadatas_all_new.append(metadatas)\n",
    "\n",
    "# Combine all \n",
    "splits_combined_new = []\n",
    "for sublist in splits_all_new:\n",
    "    splits_combined_new.extend(sublist)\n",
    "metadatas_combined_new = []\n",
    "for sublist in metadatas_all_new:\n",
    "    metadatas_combined_new.extend(sublist)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Read in the saved files`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_saved_doc(doc):\n",
    "\n",
    "    dir_path, file_name = os.path.split(doc)\n",
    "    name, ext = os.path.splitext(file_name)\n",
    "\n",
    "    with open(doc, \"r\") as f:\n",
    "        my_strings = f.readlines()\n",
    "    f.close()\n",
    "\n",
    "    doc_to_read_txt = [string.strip() for string in my_strings]\n",
    "\n",
    "    with open('metadatas/%s.json'%name, \"r\") as f:\n",
    "        doc_to_read_metadatas = json.load(f)\n",
    "    f.close()\n",
    "\n",
    "    return doc_to_read_txt,doc_to_read_metadatas\n",
    "\n",
    "# Read files in \n",
    "docs = glob.glob(\"docs/*\")\n",
    "splits_all_read_fromdisk = []\n",
    "metadatas_all_readfromdisk = []\n",
    "for doc_to_read in docs:\n",
    "    doc_to_read_txt,doc_to_read_metadatas = get_saved_doc(doc_to_read)\n",
    "    splits_all_read_fromdisk.append(doc_to_read_txt)\n",
    "    metadatas_all_readfromdisk.append(doc_to_read_metadatas)\n",
    "\n",
    "# Combine \n",
    "splits_combined_fromdisk = []\n",
    "metadatas_combined_fromdisk = []\n",
    "for sublist in splits_all_read_fromdisk:\n",
    "    splits_combined_fromdisk.extend(sublist)\n",
    "for sublist in metadatas_all_readfromdisk:\n",
    "    metadatas_combined_fromdisk.extend(sublist)\n",
    "\n",
    "# Smaller index for testing\n",
    "oriol_text,oriol_metadatas=get_saved_doc('docs/0306.txt')\n",
    "demis_text,demis_metadatas=get_saved_doc('docs/0299.txt')\n",
    "txt_test=oriol_text+demis_text\n",
    "metadatas_test=oriol_metadatas+demis_metadatas\n",
    "print(len(oriol_text))\n",
    "print(len(oriol_metadatas))\n",
    "print(len(txt_test))\n",
    "print(len(metadatas_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Embedding: small-scale test`\n",
    "\n",
    "PY support - \n",
    "\n",
    "https://langchain.readthedocs.io/en/latest/reference/modules/vectorstore.html\n",
    "\n",
    "JS support - \n",
    "\n",
    "https://hwchase17.github.io/langchainjs/docs/modules/indexes/vector_stores/supabase/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for Chroma \n",
    "embeddings = OpenAIEmbeddings()\n",
    "c_test = Chroma.from_texts(txt_test, embeddings, metadatas=metadatas_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for FAISS\n",
    "embeddings = OpenAIEmbeddings()\n",
    "f_test = FAISS.from_texts(txt_test, embeddings, metadatas=metadatas_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for pinecone \n",
    "pinecone.init(\n",
    "    api_key=os.environ.get('PINECONE_API_KEY'),  \n",
    "    environment=\"us-east1-gcp\"  \n",
    ")\n",
    "index_name = \"lex-test\"\n",
    "embeddings = OpenAIEmbeddings()\n",
    "# Write new DB\n",
    "# p_test = Pinecone.from_texts(txt_test, embeddings, index_name=index_name, metadatas=metadatas_test)\n",
    "# Read existing DB\n",
    "p_test = Pinecone.from_existing_index(index_name=index_name,embedding=embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Embed full dataset`\n",
    "\n",
    "* Takes 1-2 hrs w/ some babysitting (due to OpenAI rate limits, etc)\n",
    "\n",
    "https://docs.pinecone.io/docs/insert-data\n",
    "https://langchain.readthedocs.io/en/latest/modules/indexes/how_to_guides.html\n",
    "https://langchain.readthedocs.io/en/latest/reference/modules/vectorstore.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chroma\n",
    "for i, dict in enumerate(metadatas_combined):\n",
    "    try:\n",
    "        json_str = json.dumps(dict)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSONDecodeError in object {i}: {e}\")\n",
    "metadatas_combined_json_str=[json.dumps(dict) for dict in metadatas_combined]\n",
    "embeddings = OpenAIEmbeddings()\n",
    "chroma_docstore = Chroma.from_texts(splits_combined, embeddings, metadatas=metadatas_combined_json_str,persist_directory=\"index/\",collection_name=\"lex-gpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAISS\n",
    "faiss_docstore = FAISS.from_texts(splits_combined, embeddings, metadatas=metadatas_combined)\n",
    "faiss_docstore.save_local('index/lex_faiss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pinecone\n",
    "pinecone.init(\n",
    "    api_key=os.environ.get('PINECONE_API_KEY'),  \n",
    "    environment=\"us-east1-gcp\"  \n",
    ")\n",
    "index_name = \"lex-gpt\"\n",
    "embeddings = OpenAIEmbeddings()\n",
    "# p = Pinecone.from_texts(splits_combined[0:2], embeddings, index_name=index_name, metadatas=metadatas_combined[0:2])\n",
    "p = Pinecone.from_existing_index(index_name=index_name,embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add all data \n",
    "import math, time \n",
    "chunk_size = 100\n",
    "num_chunks = math.ceil(len(splits_combined) / chunk_size)\n",
    "for i in range(last_chunk,num_chunks):\n",
    "    \n",
    "    print(i)\n",
    "    start_time = time.time()\n",
    "    start_idx = i * chunk_size\n",
    "    end_idx = min(start_idx + chunk_size, len(splits_combined))\n",
    "    # Extract the current chunk\n",
    "    current_splits = splits_combined[start_idx:end_idx]\n",
    "    current_metadatas = metadatas_combined[start_idx:end_idx]\n",
    "    # Add the current chunk to the vector database\n",
    "    p.add_texts(texts = current_splits, metadatas=current_metadatas)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "    print(\"--------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pinecone\n",
    "pinecone.init(\n",
    "    api_key=os.environ.get('PINECONE_API_KEY'),  \n",
    "    environment=\"us-east1-gcp\"  \n",
    ")\n",
    "index_name = \"lex-gpt-new\"\n",
    "embeddings = OpenAIEmbeddings()\n",
    "# p = Pinecone.from_texts(splits_combined_new[0:2], embeddings, index_name=index_name, metadatas=metadatas_combined_new[0:2])\n",
    "# p = Pinecone.from_existing_index(index_name=index_name,embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add all data \n",
    "import math, time \n",
    "chunk_size = 100\n",
    "last_chunk = 14\n",
    "num_chunks = math.ceil(len(splits_combined_new) / chunk_size)\n",
    "for i in range(last_chunk,num_chunks):\n",
    "    \n",
    "    print(i)\n",
    "    start_time = time.time()\n",
    "    start_idx = i * chunk_size\n",
    "    end_idx = min(start_idx + chunk_size, len(splits_combined_new))\n",
    "    # Extract the current chunk\n",
    "    current_splits = splits_combined_new[start_idx:end_idx]\n",
    "    current_metadatas = metadatas_combined_new[start_idx:end_idx]\n",
    "    # Add the current chunk to the vector database\n",
    "    p.add_texts(texts = current_splits, metadatas=current_metadatas)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "    print(\"--------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Run chain`\n",
    "\n",
    "https://langchain.readthedocs.io/en/latest/modules/indexes/chain_examples/qa_with_sources.html#\n",
    "https://langchain.readthedocs.io/en/latest/modules/indexes/chain_examples/vector_db_qa_with_sources.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_qa_chain(llm,query,docstore):\n",
    "\n",
    "    start_time = time.time()\n",
    "    docs = docstore.similarity_search(query)\n",
    "    chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "    a = chain.run(input_documents=docs, question=query)\n",
    "    print(a[\"output_text\"])\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "    print(\"--------\")\n",
    "\n",
    "def run_qa_sources_chain(llm,query,docstore):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    docs = docstore.similarity_search(query)\n",
    "    chain = load_qa_with_sources_chain(llm, chain_type=\"stuff\")\n",
    "    a = chain({\"input_documents\": docs, \"question\": query})\n",
    "    print(a[\"output_text\"])\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "    print(\"--------\")\n",
    "\n",
    "def run_vectordbqa_chain(llm,query,docstore):\n",
    "\n",
    "    start_time = time.time()\n",
    "    chain = VectorDBQA.from_chain_type(llm, chain_type=\"stuff\", vectorstore=docstore)\n",
    "    a = chain({\"query\": query})\n",
    "    print(a['result'])\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "    print(\"--------\")\n",
    "\n",
    "def run_vectordb_sources_chain(llm,query,docstore):\n",
    "\n",
    "    start_time = time.time()\n",
    "    chain = VectorDBQAWithSourcesChain.from_chain_type(llm, chain_type=\"stuff\", vectorstore=docstore)\n",
    "    a = chain({\"question\": query},return_only_outputs=True)\n",
    "    print(a[\"answer\"])\n",
    "    print(a[\"sources\"])\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "    print(\"--------\")\n",
    "\n",
    "llm = OpenAIChat(temperature=0)\n",
    "q = \"What is the future path for AGI?\"\n",
    "run_qa_chain(llm,q,p_oriol)\n",
    "run_qa_sources_chain(llm,q,p_oriol) \n",
    "run_vectordbqa_chain(llm,q,p_oriol)\n",
    "run_vectordb_sources_chain(llm,q,p_test)\n",
    "run_vectordb_sources_chain(llm,q,p)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
