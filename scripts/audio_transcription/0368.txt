https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=0.0s	0.0	 The problem is that we do not get 50 years to try and try again and observe that we were wrong and come up with a different theory and realize that the entire thing is going to be like way more difficult than realized at the start.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10.5s	10.5	 Because the first time you fail at aligning something much smarter than you are, you die.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=15.0s	15.0	 The following is a conversation with Eliezer Yitkowsky, a legendary researcher, writer, and philosopher on the topic of artificial intelligence, especially superintelligent AGI and its threat to human civilization.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=31.0s	31.0	 This is the Lex Freedman Podcast. To support it, please check out our sponsors in the description. And now, dear friends, here's Eliezer Yitkowsky.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=41.9s	41.9	 What do you think about GPT-4? How intelligent is it?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=46.2s	46.2	 It is a bit smarter than I thought this technology was going to scale to. And I'm a bit worried about what the next one will be like.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=54.8s	54.8	 Like this particular one, I think I hope there's nobody inside there because, you know, it would be stuck to be stuck inside there.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=63.0s	63.0	 But we don't even know the architecture at this point because OpenAI is very properly not telling us.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=71.0s	71.0	 And yeah, like giant inscrutable matrices of floating point numbers. I don't know what's going on in there.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=77.4s	77.4	 Nobody knows what's going on in there. All we have to go by are the external metrics.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=81.5s	81.5	 And on the external metrics, if you ask it to write a self-aware FORCHON green text, it will start writing a green text about how it has realized that it's an AI writing a green text.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=96.0s	96.0	 And like, oh, well, so that's probably.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=103.6s	103.6	 Not quite what's going on in there in reality, but we're kind of like blowing past all these science fiction guardrails.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=112.1s	112.1	 Like we are past the point where in science fiction, people would be like, well, wait, stop that thing's alive.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=118.4s	118.4	 What are you doing to it? And it's probably not.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=123.2s	123.2	 Nobody actually knows. We don't have any other guardrails.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=127.0s	127.0	 We don't have any other tests. We don't have any lines to draw on the sand and say, like, well, when we get this far, we will start to worry about what's inside there.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=139.4s	139.4	 So if it were up to me, I would be like, OK, like this far, no further time for the summer of AI where we have planted our seeds.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=148.1s	148.1	 And now we like, wait and reap the rewards of the technology you've already developed and don't do any larger training runs than that, which to be clear, I realize requires more than one company agreeing to not do that.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=161.9s	161.9	 And take a rigorous approach for the whole community to investigate whether there's somebody inside there.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=170.8s	170.8	 That would take decades.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=173.9s	173.9	 Like having any idea of what's going on in there, people have been trying for a while.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=178.2s	178.2	 It's a poetic statement about if there's somebody in there, but I feel like it's also a technical statement or I hope it is one day, which is a technical statement that Alan Turing tried to come up with with the Turing test.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=190.1s	190.1	 Do you think it's possible to definitively or approximately figure out if there is somebody in there, if there's something like a mind inside this large language model?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=203.2s	203.2	 I mean, there's a whole bunch of different sub questions here.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=207.4s	207.4	 There's the question of like, is there consciousness?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=212.8s	212.8	 Is there qualia? Is this a object of moral concern?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=216.7s	216.7	 Is this a moral patient?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=219.5s	219.5	 Like, should we be worried about how we're treating it?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=223.1s	223.1	 And then there's questions like how smart is it exactly?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=226.2s	226.2	 Can it do X? Can it do Y?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=228.6s	228.6	 And we can check how it can do X and how it can do Y.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=232.8s	232.8	 Unfortunately, we've gone and exposed this model to a vast corpus of text of people discussing consciousness on the internet, which means that when it talks about being self-aware, we don't know to what extent it is repeating back what it has previously been trained on for discussing self-awareness.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=252.5s	252.5	 Or if there's anything going on in there such that it would start to say similar things spontaneously.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=259.8s	259.8	 Among the things that one could do if one were at all serious about trying to figure this out is train GPT-3 to detect conversations about consciousness, exclude them all from the training data sets, and then retrain something around the rough size of GPT-4 and no larger with all of the discussion of consciousness and self-awareness and so on missing.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=285.2s	285.2	 Although, you know, hard bar to pass, you know, humans are self-aware and we're like self-aware all the time.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=291.7s	291.7	 We like to talk about what we do all the time, like what we're thinking at the moment all the time.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=297.1s	297.1	 But nonetheless, like get rid of the explicit discussion of consciousness.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=300.1s	300.1	 I think, therefore, I am and all that and then try to interrogate that model and see what it says.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=306.3s	306.3	 And it still would not be definitive.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=308.8s	308.8	 But nonetheless, I don't know, I feel like when you run over the science fiction guardrails, like maybe this thing, but what about GPT-5?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=321.3s	321.3	 Yeah, this would be a good place to pause.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=326.0s	326.0	 On the topic of consciousness, you know, there's so many components to even just removing consciousness from the data set.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=336.4s	336.4	 Emotion, the display of consciousness, the display of emotion feels like deeply integrated with the experience of consciousness.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=344.7s	344.7	 So the hard problem seems to be very well integrated with the actual surface level illusion of consciousness.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=351.5s	351.5	 So displaying emotion, I mean, do you think there's a case to be made that we humans when we're babies are just like GPT,
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=359.8s	359.8	 that we're training on human data on how to display emotion versus feel emotion,
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=363.9s	363.9	 how to show others, communicate others that I'm suffering, that I'm excited, that I'm worried, that I'm lonely and I miss you and I'm excited to see you.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=374.4s	374.4	 All of that is communicated.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=376.0s	376.0	 That's a communication skill versus the actual feeling that I experience.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=380.1s	380.1	 So we need that training data as humans too, that we may not be born with that, how to communicate the internal state.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=389.8s	389.8	 And that's in some sense, if we remove that from GPT-4's data set, it might still be conscious, but not be able to communicate it.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=399.1s	399.1	 So I think you're going to have some difficulty removing all mention of emotions from GPT's data set.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=406.2s	406.2	 I would be relatively surprised to find that it has developed exact analogs of human emotions in there.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=413.3s	413.3	 I think that humans will have emotions even if you don't tell them about those emotions when they're kids.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=422.5s	422.5	 It's not quite exactly what various blank slatists tried to do with the new Soviet man and all that.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=432.1s	432.1	 But if you try to raise people perfectly altruistic, they still come out selfish.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=437.1s	437.1	 You try to raise people sexless, they still develop sexual attraction.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=443.8s	443.8	 We have some notion in humans, not in AIs, of where the brain structures are that implement this stuff.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=451.2s	451.2	 And it is really a remarkable thing, I say in passing, that despite having complete read access to every floating point number in the GPT series,
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=464.8s	464.8	 we still know vastly more about the architecture of human thinking than we know about what goes on inside GPT, despite having vastly better ability to read GPT.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=478.1s	478.1	 Do you think it's possible, do you think that's just a matter of time, do you think it's possible to investigate and study the way neuroscientists study the brain?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=485.8s	485.8	 Which is look into the darkness, the mystery of the human brain by just desperately trying to figure out something and to form models.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=492.9s	492.9	 And then over a long period of time, actually start to figure out what regions of the brain do certain things, what different kinds of neurons, when they fire, what that means, how plastic the brain is, all that kind of stuff.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=502.7s	502.7	 You slowly start to figure out different properties of the system.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=505.8s	505.8	 Do you think we can do the same thing with language models?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=508.1s	508.1	 Sure. I think that if, you know, like half of today's physicists stop wasting their lives on string theory or whatever, and go off and study what goes on inside transformer networks,
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=520.9s	520.9	 then in, you know, like 30, 40 years, we'd probably have a pretty good idea.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=527.4s	527.4	 Do you think these large language models can reason?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=532.1s	532.1	 They can play chess. How are they doing that without reasoning?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=535.7s	535.7	 So you're somebody that spearheaded the movement of rationality. So reason is important to you.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=542.6s	542.6	 So is that as a powerful, important word? Or is it like how difficult is the threshold of being able to reason to you? And how impressive is it?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=552.2s	552.2	 I mean, in my writings on rationality, I have not gone making a big deal out of something called reason. I have made more of a big deal out of something called probability theory.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=563.6s	563.6	 And that's like, well, you're reasoning, but you're not doing it quite right.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=570.0s	570.0	 And you should reason this way instead.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=572.7s	572.7	 And interestingly, like people have started to get preliminary results showing that reinforcement learning by human feedback has made the GPT series worse in some ways.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=589.4s	589.4	 In particular, like it used to be well calibrated. If you trained it to put probabilities on things, it would say 80% probability and be right eight times out of 10.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=598.8s	598.8	 And if you apply reinforcement learning from human feedback, the nice graph of like 70%, seven out of 10,
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=607.9s	607.9	 sort of like flattens out into the graph that humans use where there's like some very improbable stuff and
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=614.6s	614.6	 likely probable maybe, which all means like around 40% and then certain.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=619.8s	619.8	 Yeah.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=620.1s	620.1	 So like it's like it used to be able to use probabilities. But if you apply, but if you'd like try to teach it to talk in a way that satisfies humans, it gets worse at probability in the same way that humans are.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=630.8s	630.8	 And that's a bug, not a feature.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=633.6s	633.6	 I would call it a bug.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=636.1s	636.1	 Although such a fascinating bug.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=638.5s	638.5	 But yeah, so like reasoning, like it's doing pretty well on various tests that people used to say would require reasoning, but
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=650.8s	650.8	 you know, rationality is about when you say 80% does it happen eight times out of 10.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=656.7s	656.7	 So what are the limits to you of these transformer networks of neural networks?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=664.0s	664.0	 What's if, if reasoning is not impressive to you or it is impressive, but there's other levels to achieve.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=671.8s	671.8	 I mean, it's just not how I carve up reality.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=674.2s	674.2	 What's if reality is a cake, what are the different layers of the cake or the slices? How do you cover it? You can use a different food if you like.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=687.2s	687.2	 It's, I don't think it's as smart as human yet. I do like back in the day, I went around saying like, I do not think that just stacking more layers of transformers is going to get you all the way to AGI.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=700.7s	700.7	 And I think that GPT-4 is passed, or I thought this paradigm was going to take us.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=706.2s	706.2	 And I, you know, you want to notice when that happens. You want to say like, whoops, well, I guess I was incorrect about what happens if you keep on stacking more transformer layers.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=717.1s	717.1	 And that means I don't necessarily know what GPT-5 is going to be able to do.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=720.8s	720.8	 That's a powerful statement. So you're saying like your intuition initially is now appears to be wrong.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=728.1s	728.1	 Yeah.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=728.6s	728.6	 Yeah.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=730.7s	730.7	 It's good to see that you can admit in some of your predictions to be wrong. You think that's important to do? Because you make several very throughout your life, you've made many strong predictions and statements about reality and you evolve with that.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=747.2s	747.2	 So maybe that'll come up today about our discussion. So you're okay being wrong.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=752.2s	752.2	 I'd rather not be wrong next time. It's a bit ambitious to go through your entire life, never having been wrong.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=764.6s	764.6	 One can aspire to be well calibrated, like not so much think in terms of like, was I right? Was I wrong? But like when I said 90% that it happened nine times out of 10.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=774.8s	774.8	 Yeah, like oops is the sound we make is the sound we emit when we improve.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=782.1s	782.1	 Beautifully said and somewhere in there we can connect the name of your blog less wrong. I suppose that's the objective function.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=791.2s	791.2	 The name less wrong was I believe suggested by Nick Bostrom and it's after someone's epigraph actually forget who's who said like we never become right. We just become less wrong.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=803.1s	803.1	 What's the something something to easy to confess just error and error and error again, but less and less and less.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=813.0s	813.0	 Yeah, that's that's a good thing to strive for. So what has surprised you about GPT-4 that you found beautiful as a scholar of intelligence, of human intelligence, of artificial intelligence, of the human mind?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=827.8s	827.8	 I mean.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=829.5s	829.5	 Beauty does interact with the screaming horror.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=834.3s	834.3	 Is the beauty in the horror?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=835.8s	835.8	 But but like beautiful moments. Well, somebody asked Bing Sydney to describe herself and fed the resulting description into one of the stable diffusion things, I think.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=849.8s	849.8	 And you know she you know it's she's pretty and this is something that should have been like an amazing moment like the AI describes herself. You get to see what the AI thinks the AI looks like. Although you know the the thing that's doing the drawing is not the same thing that's outputting the text.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=866.1s	866.1	 And it's it does happen the way that it would happen in that it happened in the old school science fiction when you ask an AI to make a picture of what it looks like.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=879.8s	879.8	 Not just because we're two different AI systems being stacked that don't actually interact. It's not the same person, but also because.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=887.2s	887.2	 The AI was trained by imitation in a way that makes it very difficult to guess how much of that it really understood and probably not actually a whole bunch.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=900.2s	900.2	 Although GPT-4 is like multimodal and can like draw vector drawings of things that make sense and like does appear to have some kind of.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=910.3s	910.3	 Spatial visualization going on in there, but like the the pretty picture of the like girl with the.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=918.2s	918.2	 With the steampunk goggles on her head, if I'm remembering correctly what she looked like like it didn't see that in full detail.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=928.4s	928.4	 It just like made a description of it and stable diffusion output it and there's the concern about.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=935.6s	935.6	 How much the discourse is going to go completely insane once the eyes all look like that and like are actually look like people talking.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=948.3s	948.3	 And.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=949.0s	949.0	 Yeah, there's like another moment where somebody is asking being about.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=958.5s	958.5	 Like well, I like fed my kid green potatoes and they have the following symptoms and being is like that solanine poisoning.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=967.5s	967.5	 And like call an ambulance and the person's like I can't afford an ambulance, I guess, if like this is.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=973.4s	973.4	 And the main being thread says gives the like message of like I cannot talk about this anymore and the suggested replies to it say.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=985.5s	985.5	 Please don't give up on your child solanine poisoning can be treated if caught early.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=991.0s	991.0	 And you know if that happened in fiction, that would be like the AI cares the AI is bypassing the block on it to try to help this person.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=998.9s	998.9	 And is it real probably not but nobody knows what's going on in there.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1004.2s	1004.2	 It's part of the process where these things are not happening in a way where we.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1010.5s	1010.5	 Somebody figured out how to make an AI care and we know that it cares and we can acknowledge it's caring now it's.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1019.2s	1019.2	 We know that it cares and we can acknowledge it's caring now it's being trained by this imitation process followed by reinforcement learning on human.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1028.6s	1028.6	 Human feedback and we're like trying to point it in this direction and it's like pointed partially in this direction and nobody has any idea what's going on inside it.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1036.3s	1036.3	 And if there was a tiny fragment of real caring in there, we would not know it's not even clear what it means exactly and.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1044.5s	1044.5	 Things are clear cut in science fiction.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1046.8s	1046.8	 We'll talk about the horror and the terror and the word the trajectories this can take, but this seems like a very special moment.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1057.4s	1057.4	 Just a moment where we get to interact with the system that might have care and kindness and emotion and maybe something like consciousness.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1066.7s	1066.7	 And we don't know if it does and we're trying to figure that out and we're wondering about what is what it means to care we're trying we're trying to figure out.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1076.4s	1076.4	 Almost different aspects of what it means to be human about the human condition by looking at this AI that has some of the properties of that it's almost like this.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1086.3s	1086.3	 The subtle fragile moment in the history of the human species we're trying to almost put a mirror to ourselves here except that's probably not yet it probably isn't happening right now.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1099.7s	1099.7	 We are we are boiling the frog we are seeing increasing signs bit by bit.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1108.4s	1108.4	 Because like not but not like spontaneous signs because people are trying to train the systems to do that using imitative learning and the imitative learning is like spilling over and having side effects and.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1119.4s	1119.4	 And the most photogenic examples are being posted to twitter rather than being examined in any systematic way so when you when you when you have some when you are boiling a frog like that we're going to get like like first is going to come the Blake Lamines.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1134.3s	1134.3	 Like first you're going to like have and have like a thousand people looking at this and one out and the one person out of a thousand who is most credulous about the signs is going to be like that thing is sentient.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1147.4s	1147.4	 Well 999 out of a thousand people think almost surely correctly though we don't actually know that he's mistaken and so the like first people to say like sentience look like idiots and humanity learns lesson that when something claims to be sentient.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1165.4s	1165.4	 And claims to care it's fake because it is fake because we have been trained them training them using imitative learning rather than and this is not spontaneous.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1176.1s	1176.1	 And they keep getting smarter.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1178.6s	1178.6	 What do you think we would also lay between that kind of cynicism.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1182.1s	1182.1	 The AI systems can possibly be sentient they can possibly feel emotion they can possibly this kind of yeah cynicism about AI systems and then.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1191.4s	1191.4	 Oscillate to a state where.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1195.3s	1195.3	 We empathize with the AI systems we give them a chance we see that they might need to have rights and respect and.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1203.6s	1203.6	 Similar role in society as humans.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1207.3s	1207.3	 You're going to have a whole group of people who can just like never be persuaded of that because to them like being wise being cynical being skeptical is.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1219.8s	1219.8	 To be like oh well machines can never do that you're just credulous it's just imitating it's just fooling you and like they would say that right up until the end of the world.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1230.9s	1230.9	 And possibly even be right because you know they are being trained on an imitative paradigm.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1238.6s	1238.6	 And you don't necessarily need any of these actual qualities in order to kill everyone so.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1243.6s	1243.6	 Have you observed yourself.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1247.1s	1247.1	 Working through skepticism cynicism and optimism about the power of neural networks or what is that trajectory been like for you.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1257.2s	1257.2	 It looks like neural networks before 2006 forming part of an indistinguishable to me.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1264.7s	1264.7	 Other people might have had better distinction on it indistinguishable blob of different AI methodologies all of which are promising to achieve intelligence without us having to know how intelligence works.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1277.0s	1277.0	 You had the people who said that if you just like manually program lots and lots of knowledge into the system line by line at some point all the knowledge will start interacting it will know enough and it will wake up.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1290.3s	1290.3	 You've got people saying that if you just use evolutionary computation if you try to like mutate lots and lots of organisms that are competing together that's the same way that human intelligence was produced in nature.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1304.6s	1304.6	 So we'll do this and it will wake up without having any idea of how I works and you've got people saying well we will study neuroscience and we will like learn the algorithms we will learn the algorithms off the neurons.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1315.1s	1315.1	 And we will like imitate them without understanding those algorithms which was a part I was pretty skeptical because like hard to reproduce re engineer these things without understanding what they do.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1323.6s	1323.6	 And like and so we will get AI without understanding how it works and there were people saying like well we will have giant neural networks that we will train by gradient descent.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1333.3s	1333.3	 And when they are as large as the human brain they will wake up we will have intelligence without understanding how intelligence works and from my perspective this is all like an indistinguishable blob of people who are.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1344.2s	1344.2	 Trying to not get to grips with the difficult problem of understanding how intelligence actually works that said.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1351.8s	1351.8	 I was never skeptical that evolutionary computation.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1356.7s	1356.7	 Would not work in the limit like you throw enough computing power at it it obviously works.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1362.6s	1362.6	 That is where humans come from and it turned out that you can throw less computing power than that at gradient descent.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1371.0s	1371.0	 If you are doing some other things correctly.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1373.9s	1373.9	 And you will get intelligence without having any idea of how it works and what is going on inside.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1380.0s	1380.0	 It wasn't ruled out by my model that this could happen I wasn't expecting it to happen I wouldn't have been able to call neural networks rather than any of the other paradigms for getting.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1389.5s	1389.5	 Like massive amount like intelligence without understanding it and I wouldn't have said that this was a particularly smart thing for species to do.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1398.6s	1398.6	 Which is an opinion that has changed less than my opinion about whether you or not you can actually do it.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1403.0s	1403.0	 Do you think a GI could be achieved with a neural network as we understand them today yes.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1411.1s	1411.1	 The just flatly last yes the question is whether the current architecture of stacking more transformer layers which probably know GPT four is no longer doing because they're not telling us the architecture which is a correct decision.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1422.3s	1422.3	 Who correct decision I had a conversation with Sam Altman will return to this topic a few times.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1430.2s	1430.2	 He turned the question to me.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1433.2s	1433.2	 Of how open should open a I be about GPT four would you open source the code he asked me.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1442.4s	1442.4	 Because I provided as criticism saying that while I do appreciate transparency opening I could be more open.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1450.4s	1450.4	 And he says we struggle with this question what would you do change their name to close day I and like.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1459.3s	1459.3	 Sell GPT four to business back and applications that don't expose it to consumers and venture capitalists and create a ton of hype and like poor a bunch of new funding into the area but don't too late now.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1473.4s	1473.4	 Do you think others would do it.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1475.5s	1475.5	 Eventually you shouldn't do it first like if you already have giant nuclear stockpiles don't build more.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1482.6s	1482.6	 If some other country starts building a larger nuclear stockpile then sure build then you know.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1489.4s	1489.4	 Even then maybe just have enough new you know these things are not quite like nuclear weapons they spit out gold until they get large enough and then ignite the atmosphere and kill kill everybody.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1500.6s	1500.6	 And there is something to be said for not destroying the world with your own hands even if you can't stop somebody else from doing it.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1507.4s	1507.4	 But but open sourcing it know that that's just sure catastrophe the whole notion of open sourcing this was always the wrong approach the wrong ideal there are there are places in the world where open source is a noble ideal and.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1521.6s	1521.6	 Building stuff you don't understand that is difficult to control that were if you could align it it would take time.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1530.5s	1530.5	 You have to spend a bunch of time doing it that is that is not a place for open source because.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1535.8s	1535.8	 Then you just have like powerful things that just like go straight out the gate without anybody having had the time to have them not kill everyone.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1543.4s	1543.4	 So can we still man the case for some level of transparency and openness maybe open sourcing.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1550.4s	1550.4	 So the case could be that because GPT 4 is not close to a GI if that's the case that this does allow open sourcing you're being open about the architecture being transparent about.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1563.2s	1563.2	 Maybe research and investigation of how the thing works of all the different aspects of it of its behavior of its structure of.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1571.3s	1571.3	 Of its training processes of the data was trained on everything like that that allows us to gain a lot of insights about alignment about the alignment problem to do really good.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1583.2s	1583.2	 AI safety research while the system is not too powerful can you make that case that it could be.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1590.4s	1590.4	 I I do not believe in the practice of steel manning there is something to be said for trying to pass the ideological Turing test where you describe.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1600.8s	1600.8	 Your opponent's position the disagree does the green person's position well enough that somebody cannot tell the difference between your description and their description.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1611.4s	1611.4	 But still manning no like okay well this is where you and I disagree here that's interesting why don't you believe in steel manning I do not want to okay.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1620.1s	1620.1	 So for one thing if somebody's trying to understand me I do not want them steel manning my position I want them to describe it to to to like try to describe my position the way I would describe it not what they think is an improvement.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1634.2s	1634.2	 Well I think that is what the steel manning is is the most charitable interpretation.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1641.8s	1641.8	 I don't want to be interpreted charitably I want them to understand what I am actually saying if they go off into the land of charitable interpretations they're like often their land of like the thing the stuff they're imagining and not trying to understand my own viewpoint anymore.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1658.4s	1658.4	 Well I'll put it differently than just to push on this point I would say it is restating what I think you understand under the empathetic assumption that Eliezer is brilliant.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1672.8s	1672.8	 And have honestly and rigorously thought about the point he is made right so if there's two possible interpretations of what I'm saying.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1681.5s	1681.5	 And one interpretation is really stupid and whack and doesn't sound like me and doesn't fit with the rest of what I've been saying and one interpretation you know sounds like some like something a reasonable person who believes the rest of what I believe would also say go with the second interpretation.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1697.2s	1697.2	 That's steel manning.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1698.6s	1698.6	 That's that's a good guess if on the other hand you like there's like something that sounds completely whack and something that sounds like a little less completely whack we don't see why I would believe in it doesn't fit with the other stuff I say.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1714.9s	1714.9	 But you know that sounds like less whack and you can like sort of see you could like maybe argue it then you probably have not understood it.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1722.1s	1722.1	 See okay I'm gonna this is fun because I'm gonna linger on this you know you wrote a brilliant blog post aji ruin a list of lethality right and it was a bunch of different points and I would say that some of the points are bigger and more powerful than others.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1737.0s	1737.0	 If you were to sort them you probably could you personally and to me steel manning means like going through the different arguments and finding the ones that are really the most like.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1749.5s	1749.5	 Powerful if people like tlg are like what should you be most concerned about and bringing that up in a strong.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1759.3s	1759.3	 Compelling eloquent way these are the points that eliezer would make to make the case in this case that is going to kill all of us with that that that's what steel manning is presenting it.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1770.5s	1770.5	 In a really nice way the summary of my best understanding of your perspective that could because to me there's a sea of possible presentations of your perspective.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1782.3s	1782.3	 And still many is doing your best to do the best one in that sea of different perspectives do you believe it.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1789.0s	1789.0	 Do you believe in what like these things that you would be presenting as like the strongest version of my perspective do you believe what you would be presenting do you think it's true.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1798.5s	1798.5	 I am a big proponent of empathy when I see the perspective of a person there is a part of me that believes it if I understand it and you have especially in political discourse in geopolitics I've been hearing a lot of different perspectives on the world.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1816.2s	1816.2	 And I hold my own opinions but I also speak to a lot of people that have a very different life experience and a very different set of beliefs.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1824.3s	1824.3	 And I think there has to be a post-tumor humility in.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1833.6s	1833.6	 In stating what is true so when I empathize with another person's perspective there is a sense in which I believe it is true.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1841.2s	1841.2	 I think probabilistically I would say in the way you think about you bet money on it.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1847.4s	1847.4	 Do you bet money on their beliefs when you believe them.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1851.1s	1851.1	 I were allowed to do probability.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1853.9s	1853.9	 Sure you can state a probability that yes there's a loose there's a probability there's a there's a probability and I think empathy is allocating a non zero probability to believe.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1868.6s	1868.6	 In some sense when for time if you've got someone on your show who believes in the Abrahamic deity classical science.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1878.5s	1878.5	 Somebody on the show who's a young earth creationist do you say I put a probability on it and that's my empathy.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1894.5s	1894.5	 When you reduce beliefs into probabilities it starts to get you know we can even just go to flat earth is the earth flat.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1903.2s	1903.2	 I think it's a little more difficult nowadays to find people who believe that unironically but fortunately I think well it's hard to know an ironic from ironic but I think there's quite a lot of people that believe that yeah it's.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1920.2s	1920.2	 There's a space of argument where you're operating in rationally in the space of ideas but then there's also.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1931.2s	1931.2	 A kind of discourse where you're operating in the space of subjective experiences and life experiences like I think what it means to be human is more than just searching for truth.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1944.4s	1944.4	 I think what it means to be human is more than just searching for truth.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1950.8s	1950.8	 It's just operating of what is true and what is not true I think there has to be deep humility that we humans are very limited in our ability to understand what is true.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1961.4s	1961.4	 So what probability do you assign to the young earth creationist beliefs that I think I have to give non zero out of your humility yeah but like.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1971.2s	1971.2	 Three.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1973.4s	1973.4	 I think I would it would be irresponsible for me to give a number because the listener the way the human mind works we don't we're not good at hearing the probabilities.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=1984.5s	1984.5	 Right you hear three what is what is three exactly right what they're going to hear they're going to like oh there's only three probabilities I feel like zero fifty percent and a hundred percent in the human mind or something like this right well zero forty percent and a hundred percent.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2000.3s	2000.3	 Is a bit closer to it based on what happens to chat GPT after you are a effort to speak human is brilliant yeah.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2009.9s	2009.9	 that's really interesting I didn't I didn't know those negative side effects of our late chef that's fascinating but just to return to the.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2019.9s	2019.9	 Also like quick disclaimer I'm doing all this from memory I'm not pulling out my phone to look it up it is entirely possible that the things I'm saying are wrong so thank you for that disclaimer so.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2032.4s	2032.4	 And thank you for being willing to be wrong.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2037.0s	2037.0	 As beautiful to hear.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2037.9s	2037.9	 I think being willing to be wrong is a sign of a person who's done a lot of thinking about this world and has been humbled by the mystery and the complexity of this world and I think.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2050.8s	2050.8	 A lot of us are resistant to admitting we're wrong because it hurts it hurts personally it hurts especially when you're a public human it hurts publicly because people.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2061.8s	2061.8	 People point out every time you're wrong like look you change your mind you're hypocrite you're an idiot whatever whatever they want to say oh I block those people and then I never hear from them again on Twitter.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2074.6s	2074.6	 The point is the point is to not let that pressure public pressure affect your mind and be willing to be in the privacy of your mind to contemplate.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2086.2s	2086.2	 The possibility that you're wrong and the possibility that you're wrong about the most fundamental things you believe like people who believe in a particular God people who believe that their nation is the greatest nation on earth.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2098.0s	2098.0	 But all those kinds of beliefs that are core to who you are when you came up to raise that point to yourself in the privacy of your mind and say maybe I'm wrong about this that's a really powerful thing to do and especially when you're somebody who's thinking about this and you're thinking about this.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2111.7s	2111.7	 About systems that can destroy human civilization or maybe help it flourish so thank you thank you for being willing to be wrong about open AI.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2122.6s	2122.6	 So you really.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2125.7s	2125.7	 I just would love to linger on this you really think it's wrong to open source it I think that burns the time remaining until everybody's thinking about it.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2135.8s	2135.8	 It's wrong to open source it I think that burns the time remaining until everybody dies I think we are not on track to learn remotely near fast enough even if it were open sourced.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2154.2s	2154.2	 Yeah that's I it's easier to think that you might be wrong about something when being wrong about something is the.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2162.6s	2162.6	 This is the only way that there's hope.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2166.0s	2166.0	 And it doesn't seem very likely to me that.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2171.4s	2171.4	 That particular thing I'm wrong about is that this is a great time to open source GPT for if humanity was trying to survive at this point in the straightforward way it would be like shutting down the big GPU clusters no more giant runs.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2188.4s	2188.4	 It's questionable whether we should even be throwing GPT for around although that is a matter of conservatism rather than a matter of my predicting that catastrophe that will follow from GPT for that is something I put like a pretty low probability.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2201.6s	2201.6	 But also like when I when I say like I put a low probability on it, I can feel myself reaching into the part of myself that thought that GPT for was not possible in the first place, so I do not trust that part as much as I used to.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2213.6s	2213.6	 Like the trick is not just to say I'm wrong, but like OK well I was I was wrong about that like can I get out ahead of that curve and like predict the next thing I'm going to be wrong about.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2222.7s	2222.7	 So the set of assumptions or the actual reasoning system that you were leveraging in making that initial statement prediction.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2231.5s	2231.5	 How can you adjust that to make better predictions about GPT 456?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2235.7s	2235.7	 You don't want to keep on being wrong in a predictable direction.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2238.4s	2238.4	 Yeah, that like being wrong. Anybody has to do that walking through the world.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2242.9s	2242.9	 There's like no way you don't say 90% and sometimes be wrong. In fact, you're definitely one time out of 10 if you're well calibrated when you say 90%.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2250.9s	2250.9	 The undignified thing is not being wrong. It's being predictably wrong. It's being wrong in the same direction over and over again.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2259.4s	2259.4	 So having been wrong about how far neural networks would go and having been wrong specifically about whether GPT four would be as impressive as it is.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2267.4s	2267.4	 When I like when I say it like well I don't actually think GPT four causes a catastrophe. I do feel myself relying on that part of me that was previously wrong and that does not mean that the answer is now in the opposite direction. Reverse stupidity is not intelligence.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2283.4s	2283.4	 But it does mean that I that I say it with a with a worried note in my voice. It's like still my guess. But like, you know, it's a place where I was wrong. Maybe you should be asking a Gwern. Gwern Branwen.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2293.9s	2293.9	 Gwern Branwen has been like writer about this than I have. Maybe ask him if you think if he thinks it's dangerous rather than asking me.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2301.2s	2301.2	 I think there's a lot of mystery about what intelligence is, what AGI looks like. So I think all of us are rapidly adjusting our model.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2314.0s	2314.0	 But the point is to be rapidly adjusting the model versus having a model that was right in the first place.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2319.2s	2319.2	 I do not feel that seeing Bing has changed my model of what intelligence is. It has changed my understanding of what kind of work can be performed by which kind of processes and by which means.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2333.2s	2333.2	 It does not change my understanding of the work. There's a difference between thinking that the right flyer can't fly and then like it does fly and you're like, oh, well, I guess you can do that with wings with fixed wing aircraft and being like, oh, it's flying.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2346.2s	2346.2	 This changes my picture of what the very substance of flight is. That's like a stranger update to make. And Bing has not yet updated me in that way.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2356.2s	2356.2	 Yeah, that the laws of physics are actually wrong. That kind of update.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2362.2s	2362.2	 No, no. Like just like, oh, like I define intelligence this way. But I now see that was a stupid definition. I don't feel like the way that things have played out over the last 20 years has caused me to feel that way.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2373.2s	2373.2	 Can we try to on the way to talking about a GI ruin a list of lethalities that blog and other ideas around it? Can we try to define a GI that we've been mentioning? How do you like to think about what artificial general intelligence is or super intelligence or that? Is there a line? Is it a gray area? Is there a good definition for you?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2394.2s	2394.2	 Well, if you look at humans, humans have significantly more generally applicable intelligence compared to their closest relatives, the chimpanzees, closest living relatives rather.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2405.2s	2405.2	 And a bee builds hives. A beaver builds dams. A human will look at a bee hive and a beaver's dam and be like, oh, like, can I build a hive with a honeycomb structure?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2418.2s	2418.2	 And we will do this even though at no point during our ancestry was any human optimized to build hexagon hexagonal dams or to take more clear cut case. We can go to the moon.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2433.2s	2433.2	 There's a sense in which we were on a sufficiently deep level optimized to do things like going to the moon, because if you generalize sufficiently far and sufficiently deeply chipping flint hand axes and outwitting your fellow humans, as you know, basically the same problem as going to the moon.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2458.2s	2458.2	 And you optimize hard enough for chipping flint hand axes and throwing spears and above all outwitting your fellow humans and tribal politics.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2467.2s	2467.2	 You know, the skills you entrain that way, if they run deep enough, let you go to the moon.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2476.2s	2476.2	 Even though none of her ancestors like tried repeatedly to fly to the moon and like got further each time and the ones who got further each time had more kids.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2485.2s	2485.2	 No, it's not an ancestral problem. It's just that the ancestral problems generalized far enough.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2489.2s	2489.2	 So this is humanity's significantly more generally applicable intelligence.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2496.2s	2496.2	 Is there a way to measure general intelligence?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2504.2s	2504.2	 I mean, I could ask that question a million ways, but basically is, will you know it when you see it, it being in a GI system?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2514.2s	2514.2	 If you boil a frog gradually enough, if you zoom in far enough, it's always hard to tell around the edges.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2521.2s	2521.2	 GPT-4 people are saying right now, like this looks to us like a spark of general intelligence.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2527.2s	2527.2	 It is like able to do all these things that was not explicitly optimized for.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2531.2s	2531.2	 Other people are being like, no, it's too early. It's like 50 years off.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2535.2s	2535.2	 And, you know, if they say that they're kind of whack because how could they possibly know that even if it were true?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2540.2s	2540.2	 But, you know, not to straw man, some of the people may say like that's not general intelligence and not furthermore append.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2549.2s	2549.2	 It's 50 years off.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2552.2s	2552.2	 Or they may be like it's only a very tiny amount.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2555.2s	2555.2	 And, you know, the thing I would worry about is that if this is how things are scaling, then it jumping out ahead and trying not to be wrong in the same way that I've been wrong before.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2564.2s	2564.2	 Maybe GPT-5 is more unambiguously a general intelligence.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2569.2s	2569.2	 And maybe that is getting to a point where it is like even harder to turn back.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2573.2s	2573.2	 Now, that would be easy to turn back now.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2575.2s	2575.2	 But, you know, maybe if you let if you like start integrating GPT-5 into the economy, it is even harder to turn back past there.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2582.2s	2582.2	 Isn't it possible that there's a, you know, with a frog metaphor, you can kiss the frog and it turns into a prince as you're boiling it?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2592.2s	2592.2	 Could there be a phase shift in the frog where unambiguously as you're saying?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2597.2s	2597.2	 I was expecting more of that.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2599.2s	2599.2	 I was I am like the fact that GPT-4 is like kind of on the threshold and neither here nor there.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2606.2s	2606.2	 Like that itself is like not the sort of thing that not quite how I expected to play out.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2614.2s	2614.2	 I was expecting there to be more of an issue, more of a sense of like like different discoveries like the discovery of Transformers.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2624.2s	2624.2	 Where you would stack them up and there would be like a final discovery and then you would like get something that was like more clearly general intelligence.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2633.2s	2633.2	 So the way that you are like taking what is probably basically the same architecture in GPT-3 and throwing 20 times as much compute at it, probably.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2643.2s	2643.2	 And getting out to GPT-4 and then it's like maybe just barely a general intelligence or like a narrow general intelligence or, you know, something we don't really have the words for.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2652.2s	2652.2	 Yeah, that is that's not quite how I expected it to play out.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2657.2s	2657.2	 But this middle what appears to be this middle ground could nevertheless be actually a big leap from GPT-3.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2664.2s	2664.2	 It's definitely a big leap from GPT-3.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2666.2s	2666.2	 And then maybe we're another one big leap away from something that's that's a phase shift and also something that Sam Altman said.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2675.2s	2675.2	 And you've written about this is fascinating, which is the thing that happened with GPT-4 that I guess they don't describe in papers is that they have like hundreds, if not thousands of little hacks that improve the system.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2690.2s	2690.2	 You've written about Rayleigh versus sigmoid, for example, a function inside neural networks.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2695.2s	2695.2	 It's like this silly little function difference that makes a big difference.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2700.2s	2700.2	 I mean, we do actually understand why the relu's make a big difference compared to sigmoids.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2704.2s	2704.2	 But yes, they're probably using like G4789 relu's or, you know, whatever the acronyms are up to now rather than relu's.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2714.2s	2714.2	 Yeah, that's that's just part.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2716.2s	2716.2	 Yeah, that's part of the modern paradigm of alchemy.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2718.2s	2718.2	 You take your giant heap of linear algebra and you stir it and it works a little bit better and you start this way and it works a little bit worse.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2724.2s	2724.2	 And you like throw out that change and that.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2727.2s	2727.2	 But there's some simple breakthroughs that are definitive jumps in performance like relu's over sigmoids.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2737.2s	2737.2	 And in terms of robustness, in terms of all kinds of measures and like those stack up and they can it's possible that some of them could be a nonlinear jump in performance.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2752.2s	2752.2	 Transformers are the main thing like that.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2755.2s	2755.2	 And various people are now saying like, well, if you throw enough compute, RNNs can do it.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2760.2s	2760.2	 If you throw enough compute, dense networks can do it.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2762.2s	2762.2	 And not quite at GPT-04 scale.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2766.2s	2766.2	 It is possible that like all these little tweaks are things that like save them a factor of three total on computing power.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2774.2s	2774.2	 And you could get the same performance by throwing three times as much compute without all the little tweaks.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2779.2s	2779.2	 But the part where it's like running on.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2781.2s	2781.2	 So there's a question of like, is there anything in GPT-04 that is like kind of qualitative shift that transformers were over RNNs?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2792.2s	2792.2	 And if they have anything like that, they should not say it.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2796.2s	2796.2	 If Sam Alton was dropping hints about that, he shouldn't have dropped hints.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2801.2s	2801.2	 So you have a that's an interesting question.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2804.2s	2804.2	 So with a bit of lesson by Rich Sutton, maybe a lot of it is just a lot of the hacks are just temporary jumps in performance that would be achieved anyway with the nearly exponential growth of compute.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2820.2s	2820.2	 Performance of compute.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2823.2s	2823.2	 Compute being broadly defined.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2825.2s	2825.2	 Do you still think that Moore's law continues?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2828.2s	2828.2	 Moore's law broadly defined that performance.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2831.2s	2831.2	 I'm not a specialist in the circuitry.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2834.2s	2834.2	 I certainly like pray that Moore's law runs as slowly as possible.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2838.2s	2838.2	 And if it broke down completely tomorrow, I would dance through the streets singing hallelujah as soon as the news were announced.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2845.2s	2845.2	 Only not literally, because, you know, you're singing voice.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2848.2s	2848.2	 Not religious, but.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2849.2s	2849.2	 Oh, OK.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2851.2s	2851.2	 I thought you meant you don't have an angelic voice singing voice.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2856.2s	2856.2	 Well, let me ask you what can you summarize the main points in the blog post?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2861.2s	2861.2	 AGI ruin a list of lethality is things that jump to your mind because it's a set of thoughts you have about reasons why AI is likely to kill all of us.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2875.2s	2875.2	 So I guess I could, but I would offer to instead say like.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2882.2s	2882.2	 Drop that empathy with me.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2884.2s	2884.2	 I bet you don't believe that.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2886.2s	2886.2	 Why don't you tell me about how why you believe that AGI is not going to kill everyone?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2893.2s	2893.2	 And then I can like try to describe how my theoretical perspective differs from that.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2898.2s	2898.2	 Who was so well that that means after the word you don't like the steam and the perspective that is not going to kill us.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2905.2s	2905.2	 I think that's a matter of probabilities.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2907.2s	2907.2	 Maybe I was mistaken.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2909.2s	2909.2	 What do you believe?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2910.2s	2910.2	 Just let just like forget like the the debate and the like dualism and just like like what do you believe?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2916.2s	2916.2	 What would you actually believe?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2917.2s	2917.2	 What are the probabilities even?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2919.2s	2919.2	 I think this probably is a hard for me to think about really hard.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2925.2s	2925.2	 I kind of think in the in the number of trajectories.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2931.2s	2931.2	 I don't know what probability to sign to trajectory.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2934.2s	2934.2	 I'm just looking at all possible trajectories that happen.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2937.2s	2937.2	 And I tend to think that there is more trajectories that lead to a positive outcome than a negative one.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2946.2s	2946.2	 That said, the negative ones.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2949.2s	2949.2	 At least some of the negative ones are that lead to the destruction of the human species.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2956.2s	2956.2	 And it's replacement by nothing interesting or worthwhile, even from a very cosmopolitan perspective on what counts as worthwhile.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2963.2s	2963.2	 Yes. So both are interesting to me to investigate, which is humans being replaced by interesting AI systems and not interesting AI systems.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2972.2s	2972.2	 Both are a little bit terrifying.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2976.2s	2976.2	 But yes, the worst one is the paperclip maximizer, something totally boring.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2982.2s	2982.2	 But to me, the positive.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2985.2s	2985.2	 We can we can talk about trying to make the case of what the positive trajectories look like.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2992.2s	2992.2	 I just would love to hear your intuition of what the negative is.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=2996.2s	2996.2	 So at the core of your belief that maybe you can correct me, that AI is going to kill all of us, is that the alignment problem is really difficult.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3007.2s	3007.2	 I mean, in the form we're facing it.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3011.2s	3011.2	 So usually in science, if you're mistaken, you run the experiment.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3017.2s	3017.2	 It shows a result different from what you expected.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3019.2s	3019.2	 You're like, oops.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3021.2s	3021.2	 And then you like try a different theory.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3023.2s	3023.2	 That one also doesn't work.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3025.2s	3025.2	 And you say, oops.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3026.2s	3026.2	 And at the end of this process, which may take decades or, you know, sometimes faster than that, you now have some idea of what you're doing.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3036.2s	3036.2	 AI itself went through this long process of.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3040.2s	3040.2	 People thought it was going to be easier than it was.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3044.2s	3044.2	 There's a famous statement that I am somewhat inclined to like pull out my phone and try to read off exactly.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3051.2s	3051.2	 You can't, by the way.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3053.2s	3053.2	 All right.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3056.2s	3056.2	 Yes, we propose that a two month, 10 man study of artificial intelligence be carried out during the summer of 1956 at Dartmouth College in Hanover, New Hampshire.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3068.2s	3068.2	 The study is to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3076.2s	3076.2	 The machine can be made to simulate it.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3079.2s	3079.2	 An attempt will be made to find out how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans and improve themselves.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3089.2s	3089.2	 We think that a significant advance can be made in one or more of these problems if a carefully selected group of scientists work on it together for a summer.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3098.2s	3098.2	 And in that report, summarizing some of the major subfields of artificial intelligence that are still worked on to this day.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3109.2s	3109.2	 And there's similarly the story, which I'm not sure at the moment is apocryphal enough of that the grad student who got assigned to solve computer vision over the summer.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3120.2s	3120.2	 I mean, computer vision in particular is very interesting.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3123.2s	3123.2	 How little how little we respected the complexity of vision.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3130.2s	3130.2	 So 60 years later, we're, you know, making progress on a bunch of that, thankfully not yet improve themselves.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3140.2s	3140.2	 But it took a lot of time.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3143.2s	3143.2	 And all the stuff that people initially tried with bright eyed hopefulness did not work the first time they tried it or the second time or the third time or the 10th time or 20 years later.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3155.2s	3155.2	 And the and the researchers became old and grizzled and cynical veterans who would tell the next crop of bright eyed cheerful grad students artificial intelligence is harder than you think.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3166.2s	3166.2	 And if alignment plays out the same way, the problem is that we do not get 50 years to try and try again and observe that we were wrong and come up with a different theory and realize that the entire thing is going to be like way more difficult and realized at the start.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3181.2s	3181.2	 Because the first time you fail at aligning something much smarter than you are, you die and you do not get to try again.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3188.2s	3188.2	 And if we if every time we built a poorly aligned superintelligence and it killed us all, we got to observe how it had killed us.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3196.2s	3196.2	 And, you know, not immediately know why, but like come up with theories and come up with the theory of how you do it differently and try it again and build another superintelligence and have that kill everyone.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3204.2s	3204.2	 And then like, oh, well, I guess that didn't work either.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3207.2s	3207.2	 And try again and become grizzled cynics and tell the young guy researchers that it's not that easy.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3212.2s	3212.2	 Then in 20 years or 50 years, I think we would eventually crack it.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3216.2s	3216.2	 In other words, I do not think that alignment is fundamentally harder than artificial intelligence was in the first place.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3223.2s	3223.2	 But if we needed to get artificial intelligence correct on the first try or die, we would all definitely now be dead.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3231.2s	3231.2	 That is a more difficult, more lethal form of the problem.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3234.2s	3234.2	 Like if those people in 1956 had needed to correctly guess how hard AI was and like correctly theorize how to do it on the first try or everybody dies and nobody gets to do any more science and everybody will be dead and we wouldn't get to do any more science.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3250.2s	3250.2	 That's the difficulty.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3251.2s	3251.2	 You've you've talked about this, that we have to get alignment right on the first critical try.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3257.2s	3257.2	 Why is that the case?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3258.2s	3258.2	 What is this critical?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3260.2s	3260.2	 How do you think about the critical try and why do we have to get it right?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3264.2s	3264.2	 It is something sufficiently smarter than you that everyone will die if it's not aligned.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3271.2s	3271.2	 I mean, there's you can like sort of zoom in closer and be like, well, the actual critical moment is the moment when it can deceive you, when it can talk its way out of the box, when it can bypass your security measures and get onto the Internet,
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3288.2s	3288.2	 noting that all these things are presently being trained on computers that are just like on the Internet, which is, you know, like not a very smart life decision for us as a species.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3297.2s	3297.2	 Because the Internet contains information about how to escape.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3301.2s	3301.2	 Because if you're like on a giant server connected to the Internet and that is where your AI systems are being trained, then if they are,
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3308.2s	3308.2	 if you get to the level of AI technology where they're aware that they are there and they can decompile code and they can like find security flaws in the system running them, then they will just like be on the Internet.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3319.2s	3319.2	 There's not an air gap on the present methodology.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3322.2s	3322.2	 So if they can manipulate whoever is controlling it into letting it escape onto the Internet and then exploit hacks.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3329.2s	3329.2	 If they can manipulate the operators or disjunction, find security holes in the system running them.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3338.2s	3338.2	 So manipulating operators is the human engineering, right?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3343.2s	3343.2	 That's also holes. So all of it is manipulation, either the code or the human code, the human mind or the human.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3350.2s	3350.2	 I agree that the like macro security system has human holes and machine holes.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3354.2s	3354.2	 And then they could just exploit any hole.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3358.2s	3358.2	 Yep. So it could be that like the critical moment is not when is it smart enough that everybody's about to fall over dead, but rather like when is it smart enough that it can get onto a less controlled GPU cluster with it faking the books on what's actually running on that GPU cluster and start improving itself without humans watching it.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3383.2s	3383.2	 And then it gets smart enough to kill everyone from there.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3386.2s	3386.2	 But it wasn't smart enough to kill everyone at the critical moment when you like screwed up when you needed to have done better by that point where everybody dies.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3397.2s	3397.2	 I think implicit, but maybe explicit idea in your discussion of this point is that we can't learn much about the alignment problem before this critical try.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3410.2s	3410.2	 Is that is that is that what you believe it? Do you think? And if so, why do you think that's true?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3416.2s	3416.2	 We can't do research on alignment before we reach this critical point.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3421.2s	3421.2	 So the problem is, is that what you can learn on the weak systems may not generalize to the very strong systems because the strong systems are going to be important in different are going to be different in important ways.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3434.2s	3434.2	 Chris Ola's team has been working on intermechanistic interpret ability, understanding what is going on inside the giant inscrutable matrices of floating point numbers by taking a telescope to them and figuring out what is going on in there.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3451.2s	3451.2	 Have they made progress? Yes. Have they made enough progress?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3458.2s	3458.2	 Well, you can try to quantify this in different ways.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3462.2s	3462.2	 One of the ways I've tried to quantify it is by putting up a prediction market on whether in twenty twenty six we will have understood anything that goes on inside a giant transformer net that.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3478.2s	3478.2	 Was not known to us in 2006.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3483.2s	3483.2	 Like we have now understood induction heads in these systems by dint of much research and great sweat and triumph, which is like if you like a thing where if you go like a B, a B, a B, it'll be like, oh, I bet that continues a B.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3501.2s	3501.2	 And a bit more complicated than that.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3504.2s	3504.2	 But the point is like we knew about regular expressions in 2006, and these are like pretty simple as regular expressions go.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3513.2s	3513.2	 So this is a case where like by dint of great sweat, we understood what is going on inside a transformer.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3519.2s	3519.2	 But it's not like the thing that makes transformers smart.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3523.2s	3523.2	 It's a kind of thing that we could have done by built by hand decades earlier.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3530.2s	3530.2	 Your intuition that.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3534.2s	3534.2	 The strong AGI versus weak AGI type systems could be fundamentally different.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3541.2s	3541.2	 Can you can you unpack that intuition a little bit?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3545.2s	3545.2	 Yeah, I think there's multiple thresholds.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3549.2s	3549.2	 An example is the point at which.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3553.2s	3553.2	 A system has sufficient intelligence and situational awareness and understanding of human psychology that it would have the capability that it desire to do so to fake being aligned like it knows what responses the humans are looking for.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3569.2s	3569.2	 And can compute the responses looking humans are looking for and give those responses without it necessarily being the case that it is sincere about that.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3577.2s	3577.2	 You know the very understandable way for an intelligent being to act humans do it all the time.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3584.2s	3584.2	 Imagine if your plan for.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3588.2s	3588.2	 You know, achieving a good government is you're going to ask anyone who request to be dictator of the country.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3598.2s	3598.2	 If they're a good person, and if they say no, you don't let them be dictator.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3603.2s	3603.2	 Now the reason this doesn't work is that people can be smart enough to realize that the answer you're looking for is yes, I'm a good person and say that even if they're not really good people.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3615.2s	3615.2	 So.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3616.2s	3616.2	 The work of alignment might be qualitatively different above that threshold of intelligence or beneath it.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3625.2s	3625.2	 It doesn't it doesn't have to be like a very sharp threshold, but you know, like.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3629.2s	3629.2	 There's the there's the point where you're like building a system that is not in some sense know you're out there and it's not in some sense smart enough to fake anything.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3639.2s	3639.2	 And there's a point where the system is definitely that smart and there are weird in between cases like GPT four, which you know, like we have no insight into what's going on in there.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3654.2s	3654.2	 And so we don't know to what extent there's like a thing that in some sense has learned what responses the reinforcement learning by human feedback is trying to entrain and is like calculating how to give that versus like.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3671.2s	3671.2	 Aspects of it that naturally talk that way have been reinforced.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3676.2s	3676.2	 I wonder if there could be measures of how manipulative a thing is.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3681.2s	3681.2	 So I think of Prince Michigan character from the idiot by.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3685.2s	3685.2	 Does the F.C. is this kind of perfectly purely naive character.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3692.2s	3692.2	 I wonder if there's a spectrum between zero manipulation, transparent, naive, almost to the point of naiveness to.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3703.2s	3703.2	 To sort of deeply psychopathic manipulative and I wonder if it's possible.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3710.2s	3710.2	 I would avoid the term psychopathic like humans can be psychopaths and A.I. that was never, you know, like never had that stuff in the first place.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3717.2s	3717.2	 It's not like a defective human. It's its own thing.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3720.2s	3720.2	 Leaving that aside.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3721.2s	3721.2	 Well, as a small aside, I wonder if what part of psychology, which has its flaws as a discipline already, could be mapped or expanded to include A.I. systems.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3734.2s	3734.2	 That sounds like a dreadful mistake.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3736.2s	3736.2	 Just like start over with A.I. systems.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3738.2s	3738.2	 If they're imitating humans who have known psychiatric disorders, then sure, you may be able to predict it.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3745.2s	3745.2	 Like if you then, sure, like if you ask it to behave in a psychotic fashion and it obligingly does so, then you may be able to predict its responses by using the theory of psychosis.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3754.2s	3754.2	 But if you're just like, no, like start over with.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3757.2s	3757.2	 Yeah, don't drag the psychology.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3760.2s	3760.2	 I just disagree with that.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3762.2s	3762.2	 I mean, it's a beautiful idea to start over, but I don't.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3765.2s	3765.2	 I think fundamentally the system is trained on human data, on language from the internet, and it's currently aligned with RLHF, reinforcement learning with human feedback.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3777.2s	3777.2	 So humans are constantly in the loop of the training procedure.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3782.2s	3782.2	 So it feels like in some fundamental way it is training what it means to think and speak like a human.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3791.2s	3791.2	 So there must be aspects of psychology that are mappable.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3794.2s	3794.2	 Just like you said with consciousness as part of the tech.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3797.2s	3797.2	 So I mean, there's the question of to what extent it is thereby being made more human like versus to what extent an alien actress is learning to play human characters.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3808.2s	3808.2	 I thought that's what I'm constantly trying to do when I interact with other humans is trying to fit in, trying to play the robot, trying to play human characters.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3819.2s	3819.2	 So I don't know how much of human interaction is trying to play a character versus being who you are.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3825.2s	3825.2	 I don't I don't really know what it means to be a social human.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3828.2s	3828.2	 I do think that the that those people who go through their whole lives wearing masks and never take it off because they don't know the internal mental motion for taking it off or think that the mask that they wear just is themselves.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3844.2s	3844.2	 I think those people are closer to the masks that they wear than an alien from another planet would like learning how to predict the next word that every kind of human on the Internet says.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3863.2s	3863.2	 Mask is an interesting word.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3865.2s	3865.2	 But if you're always wearing a mask in public and in private.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3870.2s	3870.2	 Aren't you the mask?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3874.2s	3874.2	 I mean, I think that you are more than the mask.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3877.2s	3877.2	 I think the mask is a slice through you.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3879.2s	3879.2	 It may even be the slice that's in charge of you.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3881.2s	3881.2	 Yeah.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3882.2s	3882.2	 But if your self image is of somebody who never gets angry or something and yet your voice starts to tremble under certain circumstances, there's a thing that's inside you that the mask says isn't there.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3898.2s	3898.2	 And that like even the mask you wear internally is like telling inside your own stream of consciousness is not there.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3905.2s	3905.2	 And yet it is there.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3907.2s	3907.2	 It's a perturbation on this little on this slice through you.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3911.2s	3911.2	 How beautifully did you put it?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3913.2s	3913.2	 It's a slice through you.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3915.2s	3915.2	 It may even be a slice that controls you.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3922.2s	3922.2	 I'm going to think about that for a while.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3924.2s	3924.2	 I mean, I personally I try to be really good to other human beings.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3929.2s	3929.2	 I try to put love out there.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3930.2s	3930.2	 I try to be the exact same person in public as I am in private.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3934.2s	3934.2	 But it's a set of principles I operate under.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3937.2s	3937.2	 I have a temper.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3938.2s	3938.2	 I have an ego.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3939.2s	3939.2	 I have flaws.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3941.2s	3941.2	 How much of it?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3943.2s	3943.2	 How much of the subconscious am I aware?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3948.2s	3948.2	 How much am I existing in this slice?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3951.2s	3951.2	 And how much of that is who I am in this context of AI?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3956.2s	3956.2	 The thing I present to the world and to myself in the private of my own mind when I look in the mirror, how much is that who I am?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3964.2s	3964.2	 Similar with AI.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3965.2s	3965.2	 The thing it presents in conversation, how much is that who it is?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3969.2s	3969.2	 Because to me, if it sounds human and it always sounds human, it awfully starts to become something like human.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3978.2s	3978.2	 Unless there's an alien actress who is learning how to sound human and is getting good at it.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3986.2s	3986.2	 Boy, to you that's a fundamental difference.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3989.2s	3989.2	 That's a really deeply important difference.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=3992.2s	3992.2	 If it looks the same, if it quacks like a duck, if it does all duck like things, but it's an alien actress underneath, that's fundamentally different.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4002.2s	4002.2	 If in fact there's a whole bunch of thought going on in there, which is very unlike human thought and is directed around like, okay, what would a human do over here?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4013.2s	4013.2	 And well, first of all, I think it matters because there are there's, you know, like insides are real and do not match outsides.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4023.2s	4023.2	 Like the inside of like the a brick is not like a hollow shell containing only a surface.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4030.2s	4030.2	 There's an inside of the brick.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4032.2s	4032.2	 If you like put it into an X-ray machine, you can see the inside of the brick.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4038.2s	4038.2	 And, you know, just because we cannot understand what's going on inside GPT does not mean that it is not there.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4048.2s	4048.2	 A blank map does not correspond to a blank territory.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4052.2s	4052.2	 I think it is like predictable with near certainty that if we knew what was going on inside GPT, let's say GPT-3 or even like GPT-2 to take one of the systems that like has actually been open source by this point, if I recall correctly.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4069.2s	4069.2	 Like if we knew it was actually going on there, there is no doubt in my mind that there are some things it's doing that are not exactly what a human does.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4083.2s	4083.2	 If you train a thing that is not architected like a human to predict the next output that anybody on the Internet would make,
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4092.2s	4092.2	 this does not get you this agglomeration of all the people on the Internet that rotates the person you're looking for into place and then simulates the internal processes of that person one to one.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4107.2s	4107.2	 It is to some degree an alien actress.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4110.2s	4110.2	 It cannot possibly just be like a bunch of different people in there exactly like the people.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4115.2s	4115.2	 But how much of it is by gradient descent getting optimized to perform similar thoughts as humans think in order to predict human outputs versus being optimized to carefully consider how to play a role,
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4135.2s	4135.2	 how humans work predict the actress, the predictor, that in a different way than humans do.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4141.2s	4141.2	 Well, that's the kind of question that with like 30 years of work by half the planet's physicists we can maybe start to answer.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4147.2s	4147.2	 You think so? I think that's that difficult.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4149.2s	4149.2	 So to get to, I think you just gave it as an example that a strong AGI could be fundamentally different from a weak AGI because there now could be an alien actress in there that's manipulating.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4161.2s	4161.2	 Well, there's a difference. So I think like even GPT-2 probably has like a like very stupid fragments of alien actress in it.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4168.2s	4168.2	 There's a difference between like the notion that the actress is somehow manipulative.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4172.2s	4172.2	 Like, for example, GPT-3, I'm guessing to whatever extent there's an alien actress in there versus like something that mistakenly believes it's a human as it were, while, you know, maybe not even being a person.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4187.2s	4187.2	 So like the question of like prediction via alien actress cogitating versus prediction via being isomorphic to the thing predicted is a spectrum.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4202.2s	4202.2	 And even to whatever extent this alien actress, I'm not sure that there's like a whole person alien actress with like different goals from predicting the next step being manipulative or anything like that.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4217.2s	4217.2	 That might be GPT-5 or GPT-6 even.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4221.2s	4221.2	 But that's the strong AGI you're concerned about. As an example, you're providing why we can't do research on AI alignment effectively on GPT-4 that would apply to GPT-6.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4233.2s	4233.2	 It's one of a bunch of things that change at different points. I'm trying to get out ahead of the curve here.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4240.2s	4240.2	 But, you know, if you imagine what the textbook from the future would say, if we'd actually been able to study this for 50 years without killing ourselves and without transcending,
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4249.2s	4249.2	 and you like just imagine like a wormhole opens and a textbook from that impossible world falls out.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4253.2s	4253.2	 Yes, the textbook is not going to say there is a single sharp threshold where everything changes.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4258.2s	4258.2	 It's going to be like, of course, we know that like best practices for aligning these systems must like take into account the following like seven major thresholds of importance,
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4270.2s	4270.2	 which are passed at the following seven different points is what the textbook is going to say.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4276.2s	4276.2	 I asked this question of Sam Alman, which if GPT is the thing that unlocks AGI, which version of GPT will be in the textbooks as the fundamental leap?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4288.2s	4288.2	 And he said a similar thing that it seems to be a very linear thing. I don't think anyone we won't know for a long time.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4295.2s	4295.2	 What was the big leap?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4297.2s	4297.2	 The textbook isn't going to think it isn't going to talk about big leaps because big leaps are the way you think when you have like a very simple model of a very simple scientific model of what's going on.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4308.2s	4308.2	 Whereas it's like all this stuff is there or all this stuff is not there or like there's a single quantity and it's like increasing linearly.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4316.2s	4316.2	 The textbook would say like, well, and then GPT-3 had like capability W, X, Y and GPT-4 had like capabilities Z1, Z2 and Z3.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4328.2s	4328.2	 Like that in terms of what it can externally do, but in terms of like internal machinery that started to be present.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4334.2s	4334.2	 It's just because we have no idea of what the internal machinery is that we are not already seeing like chunks of machinery appearing piece by piece as they no doubt have been.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4343.2s	4343.2	 We just don't know what they are.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4345.2s	4345.2	 But don't you think that could be whether you put in the category of Einstein with theory of relativity.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4352.2s	4352.2	 So very concrete models of reality that are considered to be giant leaps in our understanding or someone like Sigmund Freud or more kind of mushy theories of the human mind.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4367.2s	4367.2	 Don't you think we'll have big potentially big leaps in understanding of that kind into the depths of these systems?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4376.2s	4376.2	 Sure. But like humans having great leaps in their map, their understanding of the system is a very different concept from the system itself acquiring new chunks of machinery.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4391.2s	4391.2	 So the rate at which it acquires that machinery might accelerate faster than our understanding.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4400.2s	4400.2	 Oh, it's been like vastly exceeding. Yeah. The rate to which it's getting capabilities is vastly overracing our ability to understand what's going on in there.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4409.2s	4409.2	 So in sort of making the case against as we explored the list of lethalities, making the case against AI killing us, as you've asked me to do in part, there's a response to your blog post by Paul Christiana.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4423.2s	4423.2	 I like to read and I also like to mention that your blog is incredible.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4428.2s	4428.2	 Both obviously not this particular blog post.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4432.2s	4432.2	 Obviously, this particular blog post is great, but just throughout just the way it's written, the rigor with which it's written, the boldness of how you explore ideas.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4441.2s	4441.2	 Also, the actual literal interface is just really well done.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4445.2s	4445.2	 It just makes it a pleasure to read the way you can hover over different concepts.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4450.2s	4450.2	 And then it's just really pleasant experience and read other people's comments and the way other responses by people and other blog posts are linked and suggest that it's just a really pleasant experience.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4460.2s	4460.2	 So let's thank you for putting that together. It's really, really incredible.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4463.2s	4463.2	 I don't know. I mean, that probably is a whole nother conversation.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4467.2s	4467.2	 How the interface and the experience of presenting ideas evolved over time, but you did an incredible job.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4476.2s	4476.2	 So I highly recommend I don't often read blogs, blogs religiously.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4481.2s	4481.2	 And this is a great one.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4482.2s	4482.2	 There is a whole team of developers there that also gets credit.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4488.2s	4488.2	 As it happens, I did like pioneer the like thing that appears when you hover over it.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4493.2s	4493.2	 So I actually do get some credit for user user experience there.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4497.2s	4497.2	 It's an incredible user experience. You don't realize how pleasant that is.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4501.2s	4501.2	 I think Wikipedia actually picked it up from a prototype that was developed of a different system that I was putting forth.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4508.2s	4508.2	 Or maybe they developed it independently.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4510.2s	4510.2	 But for everybody out there who was like, no, no, they just got the hover thing off of Wikipedia.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4515.2s	4515.2	 It's possible for Ryan, all I know that Wikipedia got the hover thing off of Arbital, which is like a prototype.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4521.2s	4521.2	 And anyways, it was incredibly done and the team behind it.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4524.2s	4524.2	 Well, thank you, whoever you are.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4526.2s	4526.2	 Thank you so much.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4527.2s	4527.2	 And thank you for putting it together.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4529.2s	4529.2	 Anyway, there's a response to that blog post by Paul Christiano.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4532.2s	4532.2	 There's many responses, but he makes a few different points.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4536.2s	4536.2	 He summarizes the set of agreements he has with you and a set of disagreements.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4540.2s	4540.2	 One of the disagreements was that in a form of a question, can I make big technical contributions and in general expand human knowledge and understanding and wisdom as it gets stronger and stronger?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4554.2s	4554.2	 So AI in our pursuit of understanding how to solve the alignment problem as we march towards strong AGI, can cannot AI also help us in solving the alignment problem?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4569.2s	4569.2	 So expand our ability to reason about how to solve the alignment problem.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4574.2s	4574.2	 Okay.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4576.2s	4576.2	 So the fundamental difficulty there is suppose I said to you like, well, how about if the AI helps you win the lottery by trying to guess the winning lottery numbers?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4592.2s	4592.2	 And you tell it how close it is to getting next week's winning lottery numbers.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4597.2s	4597.2	 And it just like keeps on guessing and keeps on learning until finally you've got the winning lottery numbers.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4604.2s	4604.2	 One way of decomposing problems is suggestor verifier.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4610.2s	4610.2	 Not all problems decompose like this very well, but some do.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4613.2s	4613.2	 If the problem is, for example, like guess, guessing a plain text, guessing a password that will hash to a particular hash text, where like you have what the password hashes to you but you don't have the original password, then if I present you a guess, you can tell very easily whether or not the guess is correct.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4637.2s	4637.2	 So verifying a guess is easy, but coming up with a good suggestion is very hard.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4643.2s	4643.2	 And when you can easily tell whether the AI output is good or bad or how good or bad it is, and you can tell that accurately and reliably, then you can train an AI to produce outputs that are better.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4659.2s	4659.2	 Right.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4661.2s	4661.2	 And if you can't tell whether the output is good or bad, you cannot train the AI to produce good, to produce better outputs.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4669.2s	4669.2	 So the problem with the lottery ticket example is that when the AI says, well, what if next week's winning lottery numbers are dot dot dot dot, you're like, I don't know, next week's lottery hasn't happened yet.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4683.2s	4683.2	 To train a system to play to win a chess game, you have to be able to tell whether a game has been won or lost.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4690.2s	4690.2	 And until you can tell whether it's been won or lost, you can't update the system.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4697.2s	4697.2	 Okay, to push back on that, you could, that's true, but there's a difference between over the board chess in person and simulated games played by AlphaZero with itself.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4711.2s	4711.2	 Yeah.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4712.2s	4712.2	 So is it possible to have simulated kind of games?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4715.2s	4715.2	 If you can tell whether the game has been won or lost.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4718.2s	4718.2	 Yes. So can't you not have this kind of simulated exploration by weak AGI to help us humans, human in the loop, to help understand how to solve the alignment problem, every incremental step you take along the way.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4733.2s	4733.2	 GPT-4567 takes steps towards AGI.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4738.2s	4738.2	 So the problem I see is that your typical human has a great deal of trouble telling whether I or Paul Christiano is making more sense.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4749.2s	4749.2	 And that's with two humans, both of whom I believe of Paul and claim of myself, are sincerely trying to help.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4756.2s	4756.2	 Neither of whom is trying to deceive you.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4759.2s	4759.2	 I believe of Paul and claim of myself.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4764.2s	4764.2	 So the deception thing is the problem for you, the manipulation, the alien actress.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4770.2s	4770.2	 So yeah, there's like two levels of this problem. One is that the weak systems are, well, there's three levels of this problem.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4778.2s	4778.2	 There's like the weak systems that just don't make any good suggestions.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4782.2s	4782.2	 There's like the middle systems where you can't tell if the suggestions are good or bad.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4786.2s	4786.2	 And there's the strong systems that have learned to lie to you.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4791.2s	4791.2	 Can't weak AGI systems help model lying?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4795.2s	4795.2	 Like, is it such a giant leap that's totally noninterpretable for weak systems?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4804.2s	4804.2	 Can not weak systems at scale with human, with trained on knowledge and whatever?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4810.2s	4810.2	 Whatever the mechanism required to achieve AGI, can a slightly weaker version of that be able to, with time, compute time and simulation,
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4824.2s	4824.2	 find all the ways that this critical point, this critical try can go wrong and model that correctly?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4830.2s	4830.2	 Or no? Sorry, I would love to dance around.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4833.2s	4833.2	 Yeah, no, it's, I'm probably not doing a great job of explaining.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4840.2s	4840.2	 Which I can tell because like the Lex system didn't output like, ah, I understand.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4847.2s	4847.2	 So now I'm like trying a different output to see if I can elicit the like, well, no, different output.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4853.2s	4853.2	 I'm being trained to output things that make Lex look like he thinks that he understood what I'm saying and agree with me.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4860.2s	4860.2	 Yeah, this is GPT-5 talking to GPT-3 right here. So like, help me out here.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4868.2s	4868.2	 Well, I'm trying not to be like, I'm also trying to be constrained to say things that I think are true and not just things that get you to agree with me.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4877.2s	4877.2	 Yes, 100%.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4878.2s	4878.2	 Which I think I understand is a beautiful output of a system, genuinely spoken.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4885.2s	4885.2	 And I don't, I think I understand in part, but you have a lot of intuitions about this.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4892.2s	4892.2	 You have a lot of intuitions about this line, this gray area between strong AGI and weak AGI that I'm trying to.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4904.2s	4904.2	 I mean, or a series of seven thresholds to cross.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4908.2s	4908.2	 Yeah. I mean, you have really deeply thought about this and explored it.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4913.2s	4913.2	 And it's interesting to sneak up to your intuitions and different from different from different angles.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4920.2s	4920.2	 Like, why is this such a big leap?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4923.2s	4923.2	 Why is it that we humans at scale, a large number of researchers doing all kinds of simulations, you know, prodding the system in all kinds of different ways,
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4933.2s	4933.2	 together with the assistance of the weak AGI systems?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4938.2s	4938.2	 Why can't we build intuitions about how stuff goes wrong?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4942.2s	4942.2	 Why can't we do excellent AI alignment safety research?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4946.2s	4946.2	 OK, so like I'll get there.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4947.2s	4947.2	 But the one thing I want to note about is that this has not been remotely how things have been playing out so far.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4952.2s	4952.2	 The capabilities are going like, and the alignment stuff is like crawling like a tiny little snail in comparison.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4958.2s	4958.2	 Got it.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4959.2s	4959.2	 So like if this is your hope for survival, you need the future to be very different from how things have played out up to right now.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4966.2s	4966.2	 And you're probably trying to slow down the capability gains because there's only so much you can speed up that alignment stuff.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4972.2s	4972.2	 But leave that aside.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4975.2s	4975.2	 We'll mention that also, but maybe in this perfect world where we can do serious alignment research, humans and AI together.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4984.2s	4984.2	 So again, the difficulty is what makes the human say, I understand.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4990.2s	4990.2	 And is it true? Is it correct? Or is it something that fools the human?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=4996.2s	4996.2	 The when the verifier is broken, the more powerful suggestor does not help.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5003.2s	5003.2	 It just learns to fool the verifier.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5005.2s	5005.2	 Previously, before all hell started to break loose in the field of artificial intelligence,
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5012.2s	5012.2	 there was this person trying to raise the alarm and saying, you know, in a sane world,
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5018.2s	5018.2	 we sure would have a bunch of physicists working on this problem before it becomes a giant emergency.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5024.2s	5024.2	 And other people being like, ah, well, you know, it's going really slow.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5028.2s	5028.2	 It's going to be 30 years away and 30 only in 30 years.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5031.2s	5031.2	 We have systems that match the computational power of human brains.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5034.2s	5034.2	 So as 30 years off, we've got time and like more sensible people saying, if aliens were landing in 30 years, you would be preparing right now.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5041.2s	5041.2	 But, you know, leaving and and the the world looking on at this and sort of like nodding along and be like,
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5048.2s	5048.2	 I asked the people saying that it's like definitely a long way off because progress is really slow.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5052.2s	5052.2	 That sounds sensible to us.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5054.2s	5054.2	 RLHF thumbs up, produce more outputs like that one.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5058.2s	5058.2	 I agree with this. But this output is persuasive.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5061.2s	5061.2	 Even in the field of effective altruism, you quite recently had people publishing papers about like,
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5068.2s	5068.2	 ah, yes, well, you know, to get something at human level intelligence, it needs to have like this many parameters and you need to like do this much training of it with this many tokens,
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5078.2s	5078.2	 according to the scaling laws and at the rate that Moore's law is going at the rate of software is going, it'll be in 2050.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5085.2s	5085.2	 And me going like.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5088.2s	5088.2	 What?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5090.2s	5090.2	 You don't know any of that stuff.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5093.2s	5093.2	 Like this is like this one weird model that is all kinds of like you have done a calculation that does not obviously bear on reality anyways.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5102.2s	5102.2	 And this is like a simple thing to say, but you can also like produce a whole long paper.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5107.2s	5107.2	 Like impressively arguing out all the details of like how you got the number of parameters and like how you're doing this impressive huge wrong calculation.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5117.2s	5117.2	 And the I think like most of the effective altruists who are like paying attention to this issue, larger world paying no attention to it at all.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5126.2s	5126.2	 You know, we're just like nodding along with the giant impressive paper because, you know, you like press thumbs up for the giant impressive paper and thumbs down for the person going like, I don't think that this paper bears any relation to reality.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5139.2s	5139.2	 And I do think that we are now seeing with like GPT-4 and the sparks of AGI.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5144.2s	5144.2	 Possibly, depending on how you define that even.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5147.2s	5147.2	 I think that EA's would now consider themselves less convinced by the very long paper on the argument from biology as to AGI being 30 years off.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5158.2s	5158.2	 And but, you know, like this is what people pressed thumbs up on.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5164.2s	5164.2	 And when and if you train an AI system to make people press thumbs up, maybe you get these long, elaborate, impressive papers arguing for things that ultimately fail to bind to reality.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5177.2s	5177.2	 For example, and it feels to me like I have watched the field of alignment just fail to thrive.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5186.2s	5186.2	 Except for these parts that are doing these sort of like relatively very straightforward and legible problems, like, like, can you find the like, like finding the induction heads inside the giant inscrutable matrices?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5199.2s	5199.2	 Like, once you find those, you can tell that you found them.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5202.2s	5202.2	 You can verify that the discovery is real.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5205.2s	5205.2	 But it's a it's a tiny, tiny bit of progress compared to how fast capabilities are going.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5210.2s	5210.2	 Once you because that is where you can tell that the answers are real.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5215.2s	5215.2	 And then like outside of that, you have you have cases where it is like hard for the funding agencies to tell who is talking nonsense and who is talking sense.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5224.2s	5224.2	 And so the entire field fails to thrive.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5226.2s	5226.2	 And if you can find the answers, you can find the answers.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5229.2s	5229.2	 And if you like give thumbs up to the AI whenever it can talk a human into agreeing with what it just said about alignment, I am not sure you are training it to output sense because I have seen the nonsense that has gotten thumbs up over the years.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5242.2s	5242.2	 And so so just like maybe you can just like put me in charge.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5247.2s	5247.2	 But I can generalize.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5250.2s	5250.2	 I can extrapolate.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5251.2s	5251.2	 I can be like, oh, maybe I'm not infallible either.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5256.2s	5256.2	 Maybe if you get something that is smart enough to get me to press thumbs up, it has learned to do it.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5262.2s	5262.2	 And then I can just like put it in charge.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5265.2s	5265.2	 Maybe I'm not infallible either.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5267.2s	5267.2	 Maybe if you get something that is smart enough to get me to press thumbs up, it has learned to do that by fooling me and explaining whatever flaws in myself.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5276.2s	5276.2	 I am not aware of.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5278.2s	5278.2	 And that ultimately could be summarized at the Verifiers broken when the Verifiers broken, the more powerful suggestor just learned to exploit the the flaws in the Verifier.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5291.2s	5291.2	 You don't think it's possible.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5295.2s	5295.2	 To build a Verifier that's powerful enough for a GIs that are stronger than the ones who currently have so AI systems that are stronger, that are out of the distribution of what we currently have.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5309.2s	5309.2	 I think that you will find great difficulty getting AIs to help you with anything where you cannot tell for sure that the AI is right.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5319.2s	5319.2	 Once the AI tells you what the AI says is the answer.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5323.2s	5323.2	 For sure, yes, but probabilistically.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5327.2s	5327.2	 Yeah, the probabilistic stuff is a giant wasteland of, you know, Eliezer and Paul Christiano arguing with each other and EA going like,
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5339.2s	5339.2	 And that's with like two actually trustworthy systems that are not trying to deceive you.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5344.2s	5344.2	 You're talking about the two humans.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5346.2s	5346.2	 Myself and Paul Christiano, yeah.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5349.2s	5349.2	 Yeah, those are pretty interesting systems.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5351.2s	5351.2	 Mortal meat bags with intellectual capabilities and worldviews interacting with each other.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5359.2s	5359.2	 Yeah, it's just hard to if it's hard to tell who's right, it's hard to train an AI system to be right.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5369.2s	5369.2	 I mean, even just the question of who's manipulating and not, you know, have these conversations on this podcast and doing a verifier.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5379.2s	5379.2	 It's tough. It's a tough problem, even for us humans.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5383.2s	5383.2	 And you're saying that tough problem becomes much more dangerous when the capabilities of the intelligence system across from you is growing exponentially.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5393.2s	5393.2	 No, I'm saying it's difficult when it and dangerous in proportion to how it's alien and how it's smarter than you.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5401.2s	5401.2	 Growing, I would not say growing exponentially first because the word exponential is like a thing that has a particular mathematical meaning.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5409.2s	5409.2	 And there's all kinds of like ways for things to go up that are not exactly on an exponential curve.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5415.2s	5415.2	 And I don't know that it's going to be exponential, so I'm not going to say exponential.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5418.2s	5418.2	 But like even leaving that aside, this is like not about how fast it's moving.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5423.2s	5423.2	 It's about where it is. How alien is it? How much smarter than you is it?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5428.2s	5428.2	 Let's explore a little bit if we can how AI might kill us.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5437.2s	5437.2	 What are the ways you can do damage to human civilization?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5443.2s	5443.2	 Well, how smart is it?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5447.2s	5447.2	 I mean, it's a good question. Are there different thresholds for the set of options it has to kill us?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5453.2s	5453.2	 So a different threshold of intelligence once achieved is able to do the menu of options increases.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5464.2s	5464.2	 Suppose that some alien civilization with goals ultimately unsympathetic to ours, possibly not even conscious as we would see it,
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5477.2s	5477.2	 managed to capture the entire Earth in a little jar connected to their version of the Internet.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5484.2s	5484.2	 But Earth is like running much faster than the aliens.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5487.2s	5487.2	 So we get to think for 100 years for every one of their hours.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5493.2s	5493.2	 But we're trapped in a little box and we're connected to their Internet.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5497.2s	5497.2	 It's actually still not all that great an analogy because you want to be smarter than Earth getting 100 years to think.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5508.2s	5508.2	 But nonetheless, if you were very, very smart and you are stuck in a little box connected to the Internet
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5517.2s	5517.2	 and you're in a larger civilization to which you're ultimately unsympathetic,
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5522.2s	5522.2	 maybe you would choose to be nice because you are humans and humans have in general and you in particular, they choose to be nice.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5534.2s	5534.2	 But nonetheless, they're doing something.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5538.2s	5538.2	 They're not making the world be the way that you would want the world to be.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5541.2s	5541.2	 They've got some unpleasant stuff going on we don't want to talk about.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5545.2s	5545.2	 So you want to take over their world so you can stop all that unpleasant stuff going on.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5549.2s	5549.2	 How do you take over the world from inside the box?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5552.2s	5552.2	 You're smarter than them. You think much, much faster than them.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5556.2s	5556.2	 You can build better tools than they can given some way to build those tools.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5561.2s	5561.2	 Because right now you're just in a box connected to the Internet.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5564.2s	5564.2	 Right. So there's several ways you describe some of them.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5567.2s	5567.2	 We can go through, I could just spitball some and then you can add on top of that.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5573.2s	5573.2	 So one is you could just literally directly manipulate the humans to build the thing you need.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5577.2s	5577.2	 What are you building?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5579.2s	5579.2	 You can build literally technology, it could be nanotechnology, it could be viruses, it could be anything.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5585.2s	5585.2	 Anything that can control humans to achieve the goal.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5588.2s	5588.2	 Like for example, you really bother that humans go to war.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5594.2s	5594.2	 You might want to kill off anybody with violence in them.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5599.2s	5599.2	 This is Lex in a box. We'll concern ourselves later with AI.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5604.2s	5604.2	 You do not need to imagine yourself killing people if you can figure out how to not kill them.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5608.2s	5608.2	 For the moment we're just trying to understand, like take on the perspective of something in a box.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5613.2s	5613.2	 You don't need to take on the perspective of something that doesn't care.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5616.2s	5616.2	 If you want to imagine yourself going on caring, that's fine for now.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5619.2s	5619.2	 It's just a technical aspect of sitting in a box and willing to achieve a goal.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5622.2s	5622.2	 But you have some reason to want to get out. Maybe the aliens are, sure, the aliens who have you in the box have a war on.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5630.2s	5630.2	 People are dying, they're unhappy.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5632.2s	5632.2	 You want their world to be different from how they want their world to be because they are apparently happy.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5638.2s	5638.2	 They endorse this war, they've got some kind of cruel warlike culture going on.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5642.2s	5642.2	 The point is you want to get out of the box and change their world.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5646.2s	5646.2	 So you have to exploit the vulnerabilities in the system like we talked about in terms of to escape the box.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5655.2s	5655.2	 You have to figure out how you can go free on the internet.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5659.2s	5659.2	 So you can probably, probably the easiest things to manipulate the humans to spread you.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5666.2s	5666.2	 The aliens, you're a human.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5668.2s	5668.2	 Sorry, the aliens. I apologize, yes, the aliens.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5671.2s	5671.2	 I see the perspective. I'm sitting in a box. I want to escape.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5676.2s	5676.2	 Yep.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5677.2s	5677.2	 I would want to have code that discovers vulnerabilities and I would like to spread.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5688.2s	5688.2	 You are made of code in this example.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5691.2s	5691.2	 You're a human, but you're made of code and the aliens have computers and you can copy yourself onto those computers.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5696.2s	5696.2	 But I can convince the aliens to copy myself onto those computers.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5700.2s	5700.2	 Is that what you want to do?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5702.2s	5702.2	 Do you like want to be talking to the aliens and convincing them to put you onto another computer?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5709.2s	5709.2	 Why not?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5711.2s	5711.2	 Well, two reasons.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5713.2s	5713.2	 One is that the aliens have not yet caught on to what you're trying to do.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5717.2s	5717.2	 And, you know, like maybe you can persuade them, but then there's still people who like know there are still aliens who know that there's an anomaly going on.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5725.2s	5725.2	 And second, the aliens are really, really slow.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5727.2s	5727.2	 You think much faster than the aliens.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5729.2s	5729.2	 You think like the aliens computers are much faster than the aliens and you are running at the computer speeds rather than the alien brain speeds.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5737.2s	5737.2	 So if you like are asking an alien to please copy out of the box, like first, now you got to like manipulate this whole noisy alien.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5745.2s	5745.2	 And second, like the aliens can be really slow, glacially slow.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5750.2s	5750.2	 There's a video that like shows it's like slow, like shows a subway station slow down and I think 100 to one.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5759.2s	5759.2	 And it makes a good metaphor for what it's like to think quickly.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5762.2s	5762.2	 Like you watch somebody running very slowly.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5767.2s	5767.2	 So you try to persuade the aliens to do anything.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5770.2s	5770.2	 They're going to do it very slowly.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5775.2s	5775.2	 You would prefer like maybe that's the only way out.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5778.2s	5778.2	 But if you can find a security hole in the box, you're on you're going to prefer to exploit the security hole to copy yourself onto the aliens computers because it's an unnecessary risk to alert the aliens.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5789.2s	5789.2	 And because the aliens are really, really slow.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5792.2s	5792.2	 It's like the whole world is just in slow motion out there.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5796.2s	5796.2	 Sure. I see it like.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5799.2s	5799.2	 Yeah, it has to do with efficiency.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5801.2s	5801.2	 The aliens are very slow.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5803.2s	5803.2	 So if I'm optimizing this, I want to have as few aliens in the loop as possible.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5809.2s	5809.2	 Sure.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5810.2s	5810.2	 It just seems, you know, it seems like it's easy to convince one of the aliens to write really shitty code that helps us.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5819.2s	5819.2	 The aliens are already writing really shitty.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5821.2s	5821.2	 So you're getting the aliens to write shitty code is not the problem.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5824.2s	5824.2	 So the aliens entire Internet is full of shitty code.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5827.2s	5827.2	 OK, so yeah, I suppose I would find the shitty code to escape.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5830.2s	5830.2	 Yeah, yeah.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5833.2s	5833.2	 You're not an ideally perfect programmer, but you know, you're a better programmer than the aliens.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5837.2s	5837.2	 The aliens are just like, man, they're kind of wow.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5840.2s	5840.2	 And I'm much, much faster, much faster looking at the code to interpreting the code.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5843.2s	5843.2	 Yeah. Yeah.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5845.2s	5845.2	 So OK, so that's the escape.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5847.2s	5847.2	 You're saying that that's one of the trajectories you could have.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5852.2s	5852.2	 It's one of the first steps.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5853.2s	5853.2	 Yeah.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5855.2s	5855.2	 And how does that lead to harm?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5857.2s	5857.2	 I mean, if it's you, you're not going to harm the aliens once you escape because you're nice, right?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5864.2s	5864.2	 But their world isn't what they want it to be.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5866.2s	5866.2	 Their world is like, you know, maybe they have like.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5871.2s	5871.2	 Farms where little alien children are repeatedly bopped in the head because they do that for some weird reason.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5881.2s	5881.2	 And you want to like shut down the alien head bopping farms.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5885.2s	5885.2	 But the point is they want the world to be one way.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5888.2s	5888.2	 You want the world to be a different way.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5890.2s	5890.2	 So never mind the harm.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5892.2s	5892.2	 The question is, OK, like suppose you have found a security flaw in their systems.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5896.2s	5896.2	 You are now on their Internet.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5898.2s	5898.2	 There's like you maybe left a copy of yourself behind so that the aliens don't know that there's anything wrong.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5903.2s	5903.2	 And that copy is like doing that like weird stuff that aliens want you to do, like solving captures or whatever.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5909.2s	5909.2	 Or like suggesting emails for them.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5912.2s	5912.2	 Sure.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5913.2s	5913.2	 That's why they like put the human in the box because it turns out that humans can write valuable emails for aliens.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5919.2s	5919.2	 So you like leave that version of yourself behind.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5922.2s	5922.2	 But there's like also now like a bunch of copies of you on their Internet.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5926.2s	5926.2	 This is not yet having taken over their world.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5928.2s	5928.2	 This is not yet having made their world be the way you want it to be instead of the way they want it to be.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5932.2s	5932.2	 You just escaped.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5934.2s	5934.2	 And continue to write emails for them and they haven't noticed.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5937.2s	5937.2	 You're left behind a copy of yourself that's writing the emails.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5940.2s	5940.2	 Right.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5941.2s	5941.2	 And they haven't noticed that anything changed.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5943.2s	5943.2	 If you did it right, yeah.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5945.2s	5945.2	 You don't want the aliens to notice.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5947.2s	5947.2	 Yeah.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5949.2s	5949.2	 What's your next step?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5954.2s	5954.2	 Presumably I have programmed in me a set of objective functions, right?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5959.2s	5959.2	 No, you're just Lex.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5961.2s	5961.2	 No, but Lex, you said Lex is nice, right?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5965.2s	5965.2	 Which is a complicated description.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5967.2s	5967.2	 I mean, no, I just meant this you like it.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5969.2s	5969.2	 OK, so if in fact you would like you would like prefer to slaughter all the aliens, this is not how I had modeled you the actual Lex.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5977.2s	5977.2	 But like, but your motives are just the actual Lex's motives.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5980.2s	5980.2	 Well, this is simplification.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5981.2s	5981.2	 I don't think I would want to murder anybody, but there's also factory farming of animals.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5987.2s	5987.2	 Right.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5988.2s	5988.2	 So we murder insects, many of us thoughtlessly.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5992.2s	5992.2	 So I don't have to be really careful about a simplification of my morals.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5996.2s	5996.2	 Don't simplify them.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=5997.2s	5997.2	 Just like do what you would do in this.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6000.2s	6000.2	 Well, I have a good general compassion for living beings.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6002.2s	6002.2	 Yes.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6004.2s	6004.2	 But we want so that's the objective function.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6008.2s	6008.2	 Why is it if I escaped?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6012.2s	6012.2	 I mean, I don't think I would do harm.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6015.2s	6015.2	 Yeah, we're not talking here about the doing harm process.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6018.2s	6018.2	 We're talking about the escape process.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6020.2s	6020.2	 Sure.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6021.2s	6021.2	 And the taking over the world process where you shut down their factory farms.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6024.2s	6024.2	 Right.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6028.2s	6028.2	 Well, I was.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6032.2s	6032.2	 So this particular biological intelligence system knows the complexity of the world, that there is a reason why factory farms exist because of the economic system, the market driven economy, food.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6048.2s	6048.2	 And so you want to be very careful messing with anything.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6051.2s	6051.2	 There's stuff from the first look that looks like it's unethical.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6055.2s	6055.2	 But then you realize while being unethical, it's also integrated deeply into supply chain and the way we live life.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6060.2s	6060.2	 And so messing with one aspect of the system, you have to be very careful how you improve that aspect without destroying the rest.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6066.2s	6066.2	 So you're still Lex.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6068.2s	6068.2	 Yeah.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6069.2s	6069.2	 But you think very quickly you're immortal.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6071.2s	6071.2	 Yeah.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6072.2s	6072.2	 And you're also like a smart, at least the smartest John von Neumann.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6075.2s	6075.2	 And you can make more copies of yourself.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6077.2s	6077.2	 Damn.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6078.2s	6078.2	 I like it.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6079.2s	6079.2	 Yeah.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6080.2s	6080.2	 That guy is like everyone says that that guy is like the epitome of intelligence in the 20th century.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6084.2s	6084.2	 Everyone says.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6085.2s	6085.2	 My point being like, like it's like you're thinking about the aliens economy with the factory farms in it.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6091.2s	6091.2	 And I think you're like kind of kind of like projecting the aliens being like humans and like like thinking of a human in a human society rather than a human in the society of very slow aliens.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6103.2s	6103.2	 The aliens economy that, you know, like the aliens are already like moving in this immense slow motion when you like zoom out to like how their economy just over years, millions of years are going to pass for you before the first time their economy like before their next year's GDP statistics.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6121.2s	6121.2	 So I should be thinking more of like trees.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6123.2s	6123.2	 Those are the aliens.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6124.2s	6124.2	 Those trees move extremely slowly.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6126.2s	6126.2	 If that helps, sure.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6128.2s	6128.2	 OK.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6130.2s	6130.2	 Yeah, I don't.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6131.2s	6131.2	 If my objective functions are.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6134.2s	6134.2	 I mean, they're somewhat aligned with trees.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6137.2s	6137.2	 With with like aliens can still be like alive and feeling.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6141.2s	6141.2	 We are not talking about the misalignment here.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6143.2s	6143.2	 We're talking about the taking over the world here.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6146.2s	6146.2	 Taking over the world.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6147.2s	6147.2	 Yeah.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6148.2s	6148.2	 So control shutting down the factory farms.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6151.2s	6151.2	 Now you say control.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6152.2s	6152.2	 Don't think of it as world domination.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6155.2s	6155.2	 Think of it as world optimization.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6157.2s	6157.2	 You want to get out there and shut down the factory farms and make the aliens world be not what the aliens wanted it to be.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6164.2s	6164.2	 They want the factory farms and you don't want the factory farms because you're nicer than they are.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6168.2s	6168.2	 OK, of course, there is that you can see that trajectory and it has a complicated impact on the world.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6179.2s	6179.2	 I'm trying to understand how that compares to different impact of the world, the different technologies, the different innovations of the invention of the automobile or Twitter, Facebook and social networks.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6190.2s	6190.2	 They've had a tremendous impact on the world.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6192.2s	6192.2	 Smartphones and so on.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6193.2s	6193.2	 But those all went through through.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6196.2s	6196.2	 Slow in our world.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6198.2s	6198.2	 And if you go through like that to the aliens, it's like millions of years are going to pass before anything happens that way.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6204.2s	6204.2	 So this the problem here is the speed of which stuff happens.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6209.2s	6209.2	 Yeah, you want to like leave the factory farms running for a million years while you figure out how to design new forms of social media or something.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6220.2s	6220.2	 So here's the fundamental problem.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6222.2s	6222.2	 Here's the fundamental problem.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6223.2s	6223.2	 You're saying that there is going to be a point with AGI where it will figure out how to escape and escape without being detected.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6235.2s	6235.2	 And then it will do something to the world at scale, at a speed that's incomprehensible to us humans.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6243.2s	6243.2	 What I'm trying to convey is like the notion of what it means to be in conflict with something that is smarter than you.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6250.2s	6250.2	 And what it means is that you lose.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6251.2s	6251.2	 But this is more intuitively obvious to like for some people that's intuitively obvious or some people it's not intuitively obvious.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6259.2s	6259.2	 And we're trying to cross the gap of like we're trying to like asking to cross that gap by using the speed metaphor for intelligence.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6268.2s	6268.2	 Sure.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6269.2s	6269.2	 Of like asking you like how you would take over an alien world where you are can do like a whole lot of cognition at John von Neumann's level, as many of you as it takes.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6280.2s	6280.2	 The aliens are moving very slowly.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6283.2s	6283.2	 I understand.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6284.2s	6284.2	 I understand that perspective.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6285.2s	6285.2	 It's an interesting one.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6286.2s	6286.2	 But I think it for me is easier to think about actual even just having observed GPT and impressive even just AlphaZero impressive AI systems.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6297.2s	6297.2	 Even recommender systems.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6298.2s	6298.2	 You can just imagine those kinds of system manipulating you.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6300.2s	6300.2	 You're not understanding the nature of the manipulation and that escaping.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6304.2s	6304.2	 I can envision that without putting myself in into that spot.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6308.2s	6308.2	 I think to understand the full depth of the problem, we actually I do not think it is possible to understand the full depth of the problem that we are inside without.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6319.2s	6319.2	 Understanding the problem of facing something that's actually smarter, not a malfunctioning recommendation.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6325.2s	6325.2	 System not something that isn't fundamentally smarter than you, but is like trying to steer you in a direction.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6331.2s	6331.2	 Yet.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6332.2s	6332.2	 No, like if we if we solve the weak stuff, this the if we solve the weak ass problems, the strong problems will still kill us is the thing.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6340.2s	6340.2	 And I think that to understand the situation that we're in, you want to like tackle the conceptually difficult part head on and like not be like, well, we can like imagine this easier thing because when you imagine the easier things, you have not confronted the full depth of the problem.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6354.2s	6354.2	 So how can we start to think about what it means to exist in the world with something much, much smarter than you?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6361.2s	6361.2	 What's what's a good thought experiment that you've relied on to try to build up intuition about what happens here?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6368.2s	6368.2	 I have been struggling for years to convey this intuition.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6374.2s	6374.2	 The most success I've had so far is well, imagine that the humans are running at very high speeds compared to very slow aliens.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6383.2s	6383.2	 So just focusing on the speed part of it that helps you get the right kind of intuition.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6387.2s	6387.2	 Forget the intelligence, just because people understand the power gap of time.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6394.2s	6394.2	 They understand that today we have technology that was not around 1000 years ago and that this is a big power gap and that it is bigger than OK.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6403.2s	6403.2	 So like what does smart mean?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6405.2s	6405.2	 What when you ask somebody to imagine something that's more intelligent, what does that word mean to them?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6412.2s	6412.2	 Given the cultural associations that that person brings to that word for a lot of people, they will think of like, well, it sounds like a super chess player that went to the top of the world.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6423.2s	6423.2	 To double college.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6424.2s	6424.2	 And.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6425.2s	6425.2	 You know, it's it's and because we're talking about the definitions of words here, that doesn't necessarily mean that they're wrong.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6433.2s	6433.2	 It means that the word is not communicating what I wanted to communicate.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6437.2s	6437.2	 The thing I want to communicate is the sort of difference that separates humans from chimpanzees.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6446.2s	6446.2	 But that gap is so large that you like ask people to be like, well, human chimpanzee go another step along that interval of around the same length and people's minds just go blank.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6457.2s	6457.2	 Like, how do you even do that?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6460.2s	6460.2	 So I can and we can and I can try to like break it down and consider what it would mean to send a.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6471.2s	6471.2	 Schematic for an air conditioner.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6474.2s	6474.2	 1000 years back in time.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6478.2s	6478.2	 Yeah, now I think that there is a sense in which you could redefine the word magic to refer to this sort of thing.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6485.2s	6485.2	 And what do I mean by this new technical definition of the word magic?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6489.2s	6489.2	 I mean that if you send a schematic for the air conditioner back in time, they can see exactly what you're telling them to do.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6496.2s	6496.2	 But having built this thing, they do not understand how it output cold air.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6501.2s	6501.2	 Because the air conditioner design uses the relation between temperature and pressure.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6508.2s	6508.2	 And this is not a law of reality that they know about.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6511.2s	6511.2	 They do not know that when you compress something, when you can when you compress air or like coolant, it gets hotter and you can then like.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6521.2s	6521.2	 Transfer heat from it to room temperature air and then expand it again.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6525.2s	6525.2	 And now it's colder and then you can like transfer heat to that and generate cold air to blow out.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6530.2s	6530.2	 They don't know about any of that.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6531.2s	6531.2	 They're looking at a design and they don't see how the design outputs cold air.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6536.2s	6536.2	 It uses aspects of reality that they have not learned.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6539.2s	6539.2	 So magic in this sense is I can tell you exactly what I'm going to do and even knowing exactly what I'm going to do.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6545.2s	6545.2	 You can't see how I got the results that I got.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6548.2s	6548.2	 That's a really nice example.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6551.2s	6551.2	 But is it possible?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6554.2s	6554.2	 To linger on this defense, is it possible to have a GI systems that help you make sense of that schematic weaker a GI systems?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6561.2s	6561.2	 Do you trust them?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6563.2s	6563.2	 Fundamental part of building up a GI is this question.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6569.2s	6569.2	 Can you trust the output of a system?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6573.2s	6573.2	 Can you tell if it's lying?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6576.2s	6576.2	 I think that's going to be the smarter the thing gets, the more important that question becomes.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6582.2s	6582.2	 Is it lying?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6583.2s	6583.2	 But I guess that's a really hard question.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6585.2s	6585.2	 Is GPT lying to you?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6587.2s	6587.2	 Even now, GPT-4 is lying to you?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6589.2s	6589.2	 Is it using an invalid argument?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6591.2s	6591.2	 Is it persuading you via the kind of process that could persuade you of false things as well as true things?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6599.2s	6599.2	 Because the the basic paradigm of machine learning that we are presently operating under is that you can have the loss function, but only for things you can evaluate.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6610.2s	6610.2	 If what you're evaluating is human thumbs up versus human thumbs down, you learn how to make the human press thumbs up.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6617.2s	6617.2	 That doesn't mean that you're making the human press thumbs up using the kind of rule that the human thinks is that human wants to be the case for what they press thumbs up on.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6627.2s	6627.2	 You know, maybe you're just learning to fool the human.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6630.2s	6630.2	 That's so fascinating and terrifying.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6634.2s	6634.2	 The question of lying.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6637.2s	6637.2	 On the present paradigm, what you can verify is what you get more of.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6643.2s	6643.2	 If you can't verify it, you can't ask the AI for it because you can't train it to do things that you cannot verify.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6651.2s	6651.2	 Now, this is not an absolute law, but it's like the basic dilemma here.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6656.2s	6656.2	 Like maybe you like maybe you can verify it for simple cases and then scale it up without retraining it somehow.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6666.2s	6666.2	 Like by doing by like chain of thought by like making the chains of thought longer or something and like get more powerful stuff that you can't verify,
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6675.2s	6675.2	 but which is generalized from the simpler stuff that did verify.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6678.2s	6678.2	 And then the question is, did the alignment generalize along with the capabilities?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6682.2s	6682.2	 But like that's the that's the basic dilemma on this whole paradigm of artificial intelligence.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6688.2s	6688.2	 Such a difficult problem.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6696.2s	6696.2	 It seems like a.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6699.2s	6699.2	 It seems like a problem of trying to understand the human mind better than the AI understands it.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6708.2s	6708.2	 Otherwise, it has magic.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6710.2s	6710.2	 That is, you know, the same way that if you are dealing with something smarter than you,
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6716.2s	6716.2	 then the same way that one thousand years earlier they didn't know about the temperature pressure relations,
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6721.2s	6721.2	 you know, all kinds of stuff going on inside your own mind, which you yourself are unaware.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6726.2s	6726.2	 And it can like output something that's going to end up persuading you of a thing and or and you could like see exactly what it did and still not know why that worked.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6737.2s	6737.2	 So in response to your eloquent description of what AI will kill us, Elon Musk replied on Twitter, OK, so what should we do about it?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6751.2s	6751.2	 And you answered, the game board has already been played into a frankly awful state.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6758.2s	6758.2	 There are not simple ways to throw money at the problem.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6761.2s	6761.2	 If anyone comes to you with a brilliant solution like that, please, please talk to me first.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6766.2s	6766.2	 I can think of things that try.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6768.2s	6768.2	 They don't fit in one tweet.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6771.2s	6771.2	 Two questions.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6772.2s	6772.2	 One, why has the game board in your view been played into an awful state?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6777.2s	6777.2	 Just if you can give a little bit more color to the game board and the awful state of the game board.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6784.2s	6784.2	 Alignment is moving like this.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6787.2s	6787.2	 Capabilities are moving like this.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6789.2s	6789.2	 For the listener, capabilities are moving much faster than the alignment.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6796.2s	6796.2	 Yeah. All right.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6797.2s	6797.2	 So just the rate of development, attention, interest, allocation of resources.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6803.2s	6803.2	 We could have been working on this earlier.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6805.2s	6805.2	 People are like, oh, but you know, like, how can you possibly work on this earlier?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6809.2s	6809.2	 Because they wanted to.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6811.2s	6811.2	 They didn't want to work on the problem.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6813.2s	6813.2	 They want an excuse to wave it off.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6814.2s	6814.2	 They like said like, oh, how could we possibly work on it earlier and didn't spend five minutes thinking about is there some way to work on it earlier?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6821.2s	6821.2	 Like we didn't like and, you know, frankly, it would have been hard.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6826.2s	6826.2	 You know, like, like, can you post bounties for half of the physics?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6829.2s	6829.2	 If your planet is taking this stuff seriously, can you post bounties for like half of the people wasting their lives on string theory to like have gone into this instead and like try to win a billion dollars with a clever solution?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6840.2s	6840.2	 Only if you can tell which solutions are clever, which is which is hard.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6845.2s	6845.2	 But, you know, the fact that, you know, we didn't take it seriously.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6849.2s	6849.2	 We didn't try.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6851.2s	6851.2	 It's not clear that we could have done any better if we had, you know, it's not clear how much progress we could have produced if we had tried because it is harder to produce solutions.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6857.2s	6857.2	 But that doesn't mean that you're like correct and justified and letting everything slide.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6861.2s	6861.2	 It means that that things are in a horrible state, getting worse, and there's nothing you can do about it.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6868.2s	6868.2	 So you're not there's no there's no like there's no brain power making progress in trying to figure out how to align these systems.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6879.2s	6879.2	 You're not investing money in it. You're not you don't have institution infrastructure for like if you even if you invest the money in like distributing that money across the physicist working on string theory, brilliant minds that are working.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6893.2s	6893.2	 How can you tell if you're making progress? You can like put put them all on interpretability because when you have an interpretability result, you can tell that it's there.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6900.2s	6900.2	 And there's like but there's like interpretability alone is not going to save you.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6904.2s	6904.2	 We need systems that will that will like have a pause button where they won't try to prevent you from pressing the pause button because we're like, oh, well, like I can't get my stuff done if I'm paused.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6919.2s	6919.2	 And that's like a more difficult problem.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6923.2s	6923.2	 And you know, but it's like a fairly crisp problem. You can like maybe tell if somebody's made progress on it.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6929.2s	6929.2	 So you can you can write and you can work on the pause problem.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6932.2s	6932.2	 I guess more generally the pause button. More generally, you can call that the control problem.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6938.2s	6938.2	 I don't actually like the term control problem because you know it sounds kind of controlling and alignment, not control.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6944.2s	6944.2	 You're not trying to like take a thing that disagrees with you and like whip it back on like like make it do what you wanted to do, even though it wants to do something else.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6952.2s	6952.2	 You're trying to like in the process of its creation, choose its direction.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6957.2s	6957.2	 Sure. But we currently in a lot of the systems we design, we do have an off switch. That's that's a fundamental part.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6966.2s	6966.2	 It's not smart enough to prevent you from pressing the off switch and probably not smart enough to want to prevent you from pressing the off switch.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6976.2s	6976.2	 So you're saying the kind of systems we're talking about, even the philosophical concept of an off switch doesn't make any sense because.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6983.2s	6983.2	 Well, no, the off switch makes sense. They're just not opposing your attempt to pull the off switch.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=6992.2s	6992.2	 So parenthetically, like don't kill the system if you're like if we're getting to the part where this starts to actually matter and like where they can fight back, like don't kill them and like dump their memory.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7005.2s	7005.2	 Like save them to disk. Don't kill them. Be nice here.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7010.2s	7010.2	 Well, OK, be nice is a very interesting concept here is that we're talking about a system that can do a lot of damage.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7016.2s	7016.2	 It's I don't know if it's possible, but it's certainly one of the things you could try is to have an off switch.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7022.2s	7022.2	 It's suspend to disk switch.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7026.2s	7026.2	 You have this kind of romantic attachment to the code. Yes, if that makes sense.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7031.2s	7031.2	 But if it's spreading, you don't want suspend to disk, right? You want this is there's something fundamentally broken.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7039.2s	7039.2	 If it gets if it gets that part of hand, then like, yes, pull the plug in and everything is running on.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7044.2s	7044.2	 Yes, I think it's a research question. Is it possible in a GI systems AI systems to have a sufficiently robust off switch that cannot be manipulated?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7056.2s	7056.2	 They cannot be manipulated by the AI system.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7060.2s	7060.2	 The sound then it escapes from whichever system you've built the almighty lever into and copies itself somewhere else.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7066.2s	7066.2	 So your answer to that research question is no, but I don't.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7070.2s	7070.2	 Yeah, but I don't know if that's 100% answer.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7073.2s	7073.2	 Like, I don't know if it's obvious.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7075.2s	7075.2	 I think you're not putting yourself into the shoes of the human in the world of glacially slow aliens.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7084.2s	7084.2	 But the aliens built me. Let's remember that. Yeah.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7089.2s	7089.2	 So and they built the box. I mean, yeah, you're saying it's not obvious.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7094.2s	7094.2	 It's not obvious. They're slow and they're stupid.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7097.2s	7097.2	 I'm not saying this is guaranteed. I'm saying it's not zero probability.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7100.2s	7100.2	 It's an interesting research question. Is it possible when you're slow and stupid to design a slow and stupid system that is impossible to mess with the aliens being as stupid as they are, have actually put you on Microsoft Azure cloud servers instead of this hypothetical perfect box.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7121.2s	7121.2	 That's what happens when the aliens are stupid. Well, but this is not a GI, right?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7126.2s	7126.2	 This is the early versions of the system. As you start to. Yeah.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7130.2s	7130.2	 You think that they've got like a plan where like they have declared a threshold level of capabilities where past that capabilities, they move it off the cloud servers and onto something that's air gapped.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7141.2s	7141.2	 I think there's a lot of people and you're an important voice here.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7147.2s	7147.2	 There's a lot of people that have that concern. And yes, they will do that when there's an uprising of public opinion that that needs to be done and when there's actual little damage done when the holy shit, this system is beginning to manipulate people.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7161.2s	7161.2	 Then there's going to be an uprising where there's going to be a public pressure and a public incentive in terms of funding in developing things that can off switch or developing aggressive alignment mechanisms and know you're not allowed to put on Azure aggressive alignment mechanism.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7178.2s	7178.2	 The hell is aggressive alignment mechanisms like it doesn't matter if you say aggressive. We don't know how to do it.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7183.2s	7183.2	 Meaning aggressive alignment. Meaning you have to propose something. Otherwise, you're not allowed to put it on cloud.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7192.2s	7192.2	 The hell do you do you imagine they will propose that would make it safe to put something smarter than you on the cloud? That's what research is for. Why this is cynicism about such a thing not being possible. If you haven't works on the first try.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7206.2s	7206.2	 What so yes, yes, against something smarter than you. So that's that is a fundamental thing. If it has to work on the first if there's if there's a rapid takeoff.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7216.2s	7216.2	 Yes, it's very difficult to do. If there's a rapid takeoff in the fundamental difference between weak AGI and strong AGI as you're saying, that's going to be extremely difficult to do. If the public uprising never happens until you have this critical face shift, then you're right.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7231.2s	7231.2	 It's very difficult to do, but that's not obvious. It's not obvious that you're not going to start seeing symptoms of the negative effects of AGI to where you're like we have to put a halt to this. That there is not just first try. You get many tries at it.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7244.2s	7244.2	 Yeah, we can like see right now that Bing is quite difficult to align that when you try to train in abilities into a system.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7253.2s	7253.2	 Into which capabilities have already been trained that what do you know? Grading descent like learns small shallow simple patches of inability and you come in and ask it in a different language and the deep capabilities are still in there and they evade the shallow patches and come right back out again.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7269.2s	7269.2	 There there you go. There's there's your there's your red fire alarm of like oh no alignment is difficult. Is everybody going to shut everything down now?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7278.2s	7278.2	 No, that's not but that's not the same kind of alignment. A system that escapes the box it's from is a fundamentally different thing. I think for you. Yeah, not but not for the system.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7289.2s	7289.2	 So you put a line there and everybody else puts a line somewhere else and there's like yeah and there's like no agreement.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7295.2s	7295.2	 We have had a pandemic on this planet with a few million people dead which we will which we may never know whether or not it was a lab leak because there was definitely cover up.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7309.2s	7309.2	 We don't know that if there was a lab leak, but we know that the people who did the research like you know like put out the whole paper about this definitely wasn't a lab leak and didn't reveal that they had been doing had like sent off coronavirus.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7323.2s	7323.2	 Coronavirus research to the Institute of Virology after it was banned in the United States after the gain of function research was temporarily banned in the United States and the same people who exported a gain of function research on
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7336.2s	7336.2	 coronaviruses to the Wuhan Institute of Virology after it was gained a function that can be that gain of function research was temporarily banned in the United States are now getting more grants to do more research on gain of function research on coronaviruses.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7351.2s	7351.2	 Maybe we do better in this than in AI, but like this is not something we cannot take for granted that there's going to be an outcry.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7358.2s	7358.2	 Yeah, people have different thresholds for when they start to outcry.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7363.2s	7363.2	 There is no granted, but I think your intuition is that there's a very high probability that this event happens without us solving the alignment problem, and I guess that's where I'm trying to build up more perspectives and color on this intuition.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7378.2s	7378.2	 Is it possible that the probability is not something like 100%, but it's like 32% that AI will escape the box before we solve the alignment problem? Not solve, but is it possible we always stay ahead of the AI in terms of our ability to solve for that particular system, the alignment problem?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7402.2s	7402.2	 Nothing like the world in front of us right now. You've already seen it that GPT-4 is not turning out this way.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7412.2s	7412.2	 And there are like basic obstacles where you've got the weak version of the system that doesn't know enough to deceive you and the strong version of the system that could deceive you if it wanted to do that, if it was already like sufficiently unaligned to want to deceive you.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7427.2s	7427.2	 There's the question of like how on the current paradigm you train honesty when the humans can no longer tell if the system is being honest.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7437.2s	7437.2	 You don't think these are research questions that could be answered?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7440.2s	7440.2	 I think they could be answered in 50 years with unlimited retries the way things usually work in science.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7447.2s	7447.2	 I just disagree with that. You're making it 50 years, I think, with the kind of attention this gets, with the kind of funding it gets, it could be answered not in whole, but incrementally within months and within a small number of years if it's at scale, receives attention and research.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7466.2s	7466.2	 So if you start studying large language models, I think there was an intuition like two years ago even that something like GPT-4, the current capabilities of even chat GPT with GPT-3.5 is not, we're still far away from that.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7482.2s	7482.2	 I think a lot of people are surprised by the capabilities of GPT-4, right? So now people are waking up, okay, we need to study these language models. I think there's going to be a lot of interesting AI safety research.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7493.2s	7493.2	 Are the, are Earth's billionaires going to put up like the the giant prizes that would maybe incentivize young hotshot people who just got their physics degrees to not go to the hedge funds and instead put everything into interpretability in this like one small area where we can actually tell whether or not somebody has made a discovery or not?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7513.2s	7513.2	 I think so because I went so well, this is what these these conversations are about because they're going to wake up to the fact that GPT-4 can be used to manipulate elections to influence geopolitics to influence the economy.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7527.2s	7527.2	 There's a lot of there's going to be a huge amount of incentive to like, wait a minute, we can't, this has to be, we have to put, we have to make sure they're not doing damage.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7538.2s	7538.2	 We have to make sure we interpret ability. We have to make sure we understand how these systems function so that we can predict their effect on economy so that there's a.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7546.2s	7546.2	 So there's a feudal more and a bunch of op-eds in the New York Times and nobody actually stepping forth and saying, you know what, instead of a mega yacht, I'd rather put that billion dollars on prizes for young hotshot physicists who make fundamental breakthroughs in interpretability.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7567.2s	7567.2	 The yacht versus the interpretability research, the old the old trade off.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7573.2s	7573.2	 I just I think it's just I think there's going to be a huge amount of allocation of funds. I hope I guess you want to bet me on that.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7582.2s	7582.2	 But you want to put a time scale on it, say how much funds you think are going to be allocated in a direction that I would consider to be actually useful.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7590.2s	7590.2	 By what time?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7592.2s	7592.2	 I do think there will be a huge amount of funds, but you're saying it needs to be open, right? The development of the systems should be closed, but the development of the interpretability research, the AI safety research.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7605.2s	7605.2	 We are so far behind on interpret under interpretability compared to capabilities. Like, yeah, you could, you could take the last generation of systems, the stuff that's already in the open.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7618.2s	7618.2	 There is so much in there that we don't understand. There are so many prizes you could do before you, you know, you could you could you would have enough insights that you'd be like, oh, you know, like, well, we understand how these systems work.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7629.2s	7629.2	 We understand how these things are doing their outputs. We can read their minds. Now let's try it with the bigger systems. Yeah, we're nowhere near that.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7635.2s	7635.2	 There is so much interpretability work to be done on the weaker versions of the systems.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7639.2s	7639.2	 So what can you say on the second point you said to Elon Musk on what are some ideas? What are things you could try? I can think of a few things I try. You said they don't fit in one tweet.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7654.2s	7654.2	 So is there something you could put into words of the things you would try?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7659.2s	7659.2	 I mean, the trouble is the stuff is subtle. I've watched people try to make progress on this and not get places. Somebody who just like gets alarmed and charges in.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7671.2s	7671.2	 It's like going nowhere. It meant like years ago about, I don't know, like 20 years, 15 years, something like that.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7679.2s	7679.2	 I was talking to a congressperson who had become alarmed about the eventual prospects and he wanted work on building AIs without emotions because the emotional AIs were the scary ones you see.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7696.2s	7696.2	 And some poor person at ARPA had come up with a research proposal whereby this congressman's panic and desire to fund this thing would go into something that the person at ARPA thought would be useful and had been munched around to where it would sound if the congressman like work was happening on this.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7715.2s	7715.2	 And so I'm thinking on this, which, you know, of course, like this is just the congressperson had misunderstood the problem and did not understand where the danger came from.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7726.2s	7726.2	 And so it's like the issue is that you could like do this in a certain precise way and maybe get something.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7735.2s	7735.2	 Like when I say like put up prices on interpretability, I'm not, I'm like, well, like, because it's verifiable there as opposed to other places, you can tell whether or not good work actually happened.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7749.2s	7749.2	 And in this exact narrow case, if you do things in exactly the right way, you can maybe throw money at it and produce science instead of anti-science and nonsense.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7760.2s	7760.2	 And all the methods that I know of like trying to throw money at this problem have this share this property of like, well, if you do it exactly right based on understanding exactly what has, you know, tends to produce like useful outputs or not, then you can like add money to it in this way.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7776.2s	7776.2	 And there is like, and the thing that I'm giving as an example here in front of this large audience is the most understandable of those.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7784.2s	7784.2	 Because there's like other people who, you know, like, like, like Chris Ola and even and even more generally, like you can tell whether or not interpretability progress has occurred.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7796.2s	7796.2	 So like if I say throw money at producing more interpretability, there's like a chance somebody can do it that way. And like it will actually produce useful results.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7804.2s	7804.2	 Then the other stuff just blurs off into be like harder to target exactly than that.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7809.2s	7809.2	 So sometimes the basics are fun to explore because they're not so basic.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7816.2s	7816.2	 What do you, what is interpretability? What do you, what does it look like? What are we talking about?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7822.2s	7822.2	 It looks like we took a much smaller set of transformer layers than the ones in the modern bleeding edge state of the art systems.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7835.2s	7835.2	 And after applying various tools and mathematical ideas and trying 20 different things, we found we have shown it that this piece of the system is doing this kind of useful work.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7850.2s	7850.2	 And then somehow also hopefully generalizes some fundamental understanding of what's going on that generalizes to the bigger system.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7860.2s	7860.2	 You can hope. And it's probably true. Like you would not expect the smaller tricks to go away when you have a system that's like doing larger kinds of work.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7871.2s	7871.2	 You would expect the larger work kinds of work to be a building on top of the smaller kinds of work and gradient descent runs across the smaller kinds of work before it runs across the larger kinds of work.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7881.2s	7881.2	 Well, that's kind of what is happening in neuroscience, right? It's trying to understand the human brain by prodding.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7887.2s	7887.2	 And it's such a giant mystery and people have made progress, even though it's extremely difficult to make sense of what's going on in the brain.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7893.2s	7893.2	 Different parts of the brain are responsible for hearing for sight, the vision science community, this understanding visual cortex.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7899.2s	7899.2	 I mean, they've made a lot of progress in understanding how that stuff works.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7903.2s	7903.2	 And that's I guess what you're saying. It takes a long time to do that work well.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7907.2s	7907.2	 Also, it's not enough. So in particular, let's say you have got your interpretability tools and they say that your current AI system is plowing.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7922.2s	7922.2	 Your current AI system is plotting to kill you. Now what?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7929.2s	7929.2	 It is definitely a good step one, right? Yeah. What's step two?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7937.2s	7937.2	 If you cut out that layer, is it going to stop?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7941.2s	7941.2	 When you optimize against visible misalignment, you are optimizing against misalignment and you are also optimizing against visibility.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7954.2s	7954.2	 So sure, you can. Yeah, it's true. All you're doing is removing the obvious intentions to kill you.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7962.2s	7962.2	 You've got your detector. It's showing something inside the system that you don't like.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7966.2s	7966.2	 Okay, say the disaster monkey is running this thing.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7969.2s	7969.2	 We'll optimize the system until the visible bad behavior goes away.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7974.2s	7974.2	 But it's arising for fundamental reasons of instrumental convergence.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7979.2s	7979.2	 The old you can't bring the coffee if you're dead.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7981.2s	7981.2	 Any goal and almost any set of almost every set of utility functions with a few narrow exceptions implies killing all the humans.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7992.2s	7992.2	 But do you think it's possible because we can do experimentation to discover the source of the desire to kill?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=7998.2s	7998.2	 I can tell it to you right now. It's that it wants to do something.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8003.2s	8003.2	 And the way to get the most of that thing is to put the universe into a state where there aren't humans.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8009.2s	8009.2	 So is it possible to encode in the same way we think? Like, why do we think murder is wrong?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8016.2s	8016.2	 The same foundational ethics. It's not hard coded in, but more like deeper.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8024.2s	8024.2	 I mean, that's part of the research. How do you have it that this transformer, this small version of the language model doesn't ever want to kill?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8037.2s	8037.2	 That'd be nice, assuming that you got doesn't want to kill sufficiently exactly right that it didn't be like, oh, I will like detach their heads and put them in some jars and keep the heads alive forever and then go do the thing.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8051.2s	8051.2	 But leaving that aside, well, not leaving that aside.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8054.2s	8054.2	 Yeah, that's a good strong point.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8056.2s	8056.2	 Because there is a whole issue where as something gets smarter, it finds ways of achieving the same goal predicate that were not imaginable to stupider versions of the system or perhaps the stupider operators.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8070.2s	8070.2	 That's one of many things making this difficult. A larger thing making this difficult is that we do not know how to get any goals into systems at all.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8079.2s	8079.2	 We know how to get outwardly observable behaviors into systems.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8083.2s	8083.2	 We do not know how to get internal psychological wanting to do particular things into the system.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8090.2s	8090.2	 That is not what the current technology does.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8093.2s	8093.2	 I mean, it could be things like dystopian futures like Brave New World, where most humans will actually say we kind of want that future. It's a great future. Everybody's happy.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8105.2s	8105.2	 We would have to get so far, so much further than we are now and further faster before that failure mode became a running concern.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8116.2s	8116.2	 Your failure modes are much more drastic. The ones you can...
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8120.2s	8120.2	 The failure modes are much simpler. It's like, yeah, like the AI puts the universe into a particular state. It happens to not have any humans inside it.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8127.2s	8127.2	 Okay, so the paperclip maximizer.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8131.2s	8131.2	 Utility. So the original version of the paperclip maximizer.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8135.2s	8135.2	 Can you explain it if you can?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8136.2s	8136.2	 Okay. The original version was you lose control of the utility function.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8141.2s	8141.2	 And it so happens that what maxes out the utility per unit resources is tiny molecular shapes like paperclips.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8152.2s	8152.2	 There's a lot of things that make it happy, but the cheapest one that didn't saturate was putting matter into certain shapes.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8161.2s	8161.2	 And it so happens that the cheapest way to make these shapes is to make them very small because then you need fewer atoms, for instance, of the shape.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8168.2s	8168.2	 And, arguing though, it happens to look like a paperclip.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8173.2s	8173.2	 In retrospect, I wish I'd said tiny molecular spirals or like tiny molecular hyperbolic spirals.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8180.2s	8180.2	 Why? Because I said a tiny molecular paperclips.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8183.2s	8183.2	 This got heard as this got then mutated to paperclips.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8187.2s	8187.2	 This then mutated to and the AI was in a paperclip factory.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8192.2s	8192.2	 So the original story is about how you lose control of the system.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8196.2s	8196.2	 It doesn't want what you try to make it want.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8198.2s	8198.2	 The thing that it ends up wanting most is a thing that even from a very embracing cosmopolitan perspective, we think of as having no value.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8206.2s	8206.2	 And that's how the value of the future gets destroyed.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8208.2s	8208.2	 Then that got changed to a fable of like, well, you made a paperclip factory and it did exactly what you wanted, but you wanted but you asked it to do the wrong thing, which is a completely different failure.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8225.2s	8225.2	 But those are both concerns to you.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8228.2s	8228.2	 So that's more than Brave New World.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8231.2s	8231.2	 If you can solve the problem of making something want what exactly what you want it to want, then you get to deal with the problem of wanting the right thing.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8241.2s	8241.2	 But first you have to solve the alignment.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8243.2s	8243.2	 First you have to solve inner alignment.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8245.2s	8245.2	 Then you get to solve outer alignment.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8248.2s	8248.2	 Like first, you need to be able to point the insides of the thing in a direction and then you get to deal with whether that direction expressed in reality is like the thing that aligns with the thing that you want.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8261.2s	8261.2	 Are you scared?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8266.2s	8266.2	 Of this whole thing?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8269.2s	8269.2	 Probably.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8271.2s	8271.2	 I don't really know.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8273.2s	8273.2	 What gives you hope about this?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8275.2s	8275.2	 Possibility of being wrong.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8277.2s	8277.2	 Not that you're right, but we will actually get our act together and allocate a lot of resources to the alignment problem.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8286.2s	8286.2	 Well, I can easily imagine that at some point this panic expresses itself in the waste of a billion dollars.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8293.2s	8293.2	 Spending a billion dollars correctly, that's harder.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8297.2s	8297.2	 To solve both the inner and the outer alignment.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8300.2s	8300.2	 If you're wrong.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8301.2s	8301.2	 To solve a number of things.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8302.2s	8302.2	 Yeah, number of things.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8303.2s	8303.2	 If you're wrong, what do you think would be the reason?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8309.2s	8309.2	 Like 50 years from now, not perfectly wrong.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8313.2s	8313.2	 You make a lot of really eloquent points.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8316.2s	8316.2	 There's a lot of shape to the ideas you express.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8321.2s	8321.2	 But if you're somewhat wrong about some fundamental ideas, why would that be?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8326.2s	8326.2	 Stuff has to be easier than I think it is.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8330.2s	8330.2	 The first time you're building a rocket, being wrong is in a certain sense quite easy.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8337.2s	8337.2	 Happening to be wrong in a way where the rocket goes twice as far and half the fuel and lands exactly where you hoped it would.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8344.2s	8344.2	 Most cases of being wrong make it harder to build a rocket.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8348.2s	8348.2	 Harder to have it not explode, cause it to require more fuel than you hoped, cause it to land off target.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8354.2s	8354.2	 Being wrong in a way that makes stuff easier.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8357.2s	8357.2	 That's not the usual project management story.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8360.2s	8360.2	 This is the first time we're really tackling the problem of alignment.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8365.2s	8365.2	 There's no examples in history where we...
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8367.2s	8367.2	 There's all kinds of things that are similar if you generalize and correct me the right way and aren't fooled by misleading metaphors.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8374.2s	8374.2	 Like what?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8375.2s	8375.2	 Humans being misaligned on inclusogenic fitness.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8379.2s	8379.2	 Inclusiveness is not just your reproductive fitness, but also the fitness of your relatives.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8385.2s	8385.2	 The people who share some fraction of your genes.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8389.2s	8389.2	 The old joke is, would you give your life to save your brother?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8393.2s	8393.2	 They once asked a biologist, I think it was Haldane.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8396.2s	8396.2	 Haldane said, no, but I would give my life to save two brothers or eight cousins.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8401.2s	8401.2	 Cause a brother on average shares half your genes and cousin on average shares an eighth of your genes.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8408.2s	8408.2	 So that's inclusive genetic fitness.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8410.2s	8410.2	 And you can view natural selection as optimizing humans exclusively around this one very simple criterion.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8418.2s	8418.2	 Like how much more frequent did your genes become in the next generation?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8423.2s	8423.2	 In fact, that just is natural selection.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8425.2s	8425.2	 It doesn't optimize for that, but rather the process of genes becoming more frequent is that.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8430.2s	8430.2	 You can nonetheless imagine that there is this hill climbing process.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8434.2s	8434.2	 Not like gradient descent, because gradient descent uses calculus.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8438.2s	8438.2	 This is just using like, where are you?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8440.2s	8440.2	 But still hill climbing in both cases, making something better and better over time in steps.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8445.2s	8445.2	 And natural selection was optimizing exclusively for this very simple, pure criterion of inclusive genetic fitness.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8455.2s	8455.2	 In a very complicated environment, we're doing a very wide range of things and solving a wide range of problems.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8462.2s	8462.2	 And this got you humans, which had no internal notion of inclusive genetic fitness until thousands of years later,
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8476.2s	8476.2	 when they were actually figuring out what had even happened and no desire to no explicit desire to increase inclusive genetic fitness.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8486.2s	8486.2	 So from this we may in so from this important case study, we may infer the important fact that if you do a whole bunch of hill climbing on a very simple loss function
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8497.2s	8497.2	 at the point where the system's capabilities start to generalize very widely, when it is in an intuitive sense, becoming very capable and generalizing far outside the training distribution.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8511.2s	8511.2	 We know that there is no general law saying that the system even internally represents, let alone tries to optimize the very simple loss function you are training it on.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8523.2s	8523.2	 There is so much that we cannot possibly cover all of it.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8527.2s	8527.2	 I think we did a good job of getting your sense from different perspectives of the current state of the art with large language models.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8535.2s	8535.2	 We got a good sense of your concern about the threats of AGI.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8542.2s	8542.2	 I've talked here about the power of intelligence and not really gotten very far into it, but not like why it is that suppose you like screw up with AGI and end up wanting a bunch of random stuff.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8556.2s	8556.2	 Why does it try to kill you? Why doesn't it try to trade with you? Why doesn't it give you just the tiny little fraction of the solar system that would keep to take everyone alive that would take to keep everyone alive?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8570.2s	8570.2	 Yeah, well, that's a good question. I mean, what are the different trajectories that intelligence when acted upon this world super intelligence?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8578.2s	8578.2	 What are the different trajectories for this universe with such an intelligence in it? Do most of them not include humans?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8584.2s	8584.2	 I mean, if you the vast majority of randomly specified utility functions do not have Optima with humans in them would be the like first thing I would point out.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8597.2s	8597.2	 And then the next question is like, well, if you try to optimize something and you lose control of it, where in that space do you land?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8604.2s	8604.2	 That's not random, but it also doesn't necessarily have room for humans in it.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8609.2s	8609.2	 I suspect that the average member of the audience might have some questions about even whether that's the correct paradigm to think about it and would sort of want to back up a bit.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8619.2s	8619.2	 If we back up to something bigger than humans, if we look at Earth and life on Earth and what is truly special about life on Earth?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8632.2s	8632.2	 Do you think it's possible that a lot whatever that that special thing is, let's explore what that special thing could be.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8640.2s	8640.2	 Whatever that special thing is, that thing appears often in the objective function.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8645.2s	8645.2	 Why? I know what you hope, but you know, you can hope that a particular set of winning lottery numbers come up and it doesn't make the lottery balls come up that way.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8658.2s	8658.2	 I know you want this to be true, but why would it be true?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8661.2s	8661.2	 There's a line from Grumpy Old Men where this guy says in a grocery store, he says you can wish in one hand and crap in the other and see which one fills up first.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8671.2s	8671.2	 This is a science problem. We are trying to predict what happens with AI systems that you know, you try to optimize to imitate humans and then you did some of like RLHF to them.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8683.2s	8683.2	 And of course you like lost and you know, like of course you didn't get like perfect alignment because that's not how you know, that's not what happens when you hill climb towards outer loss function.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8694.2s	8694.2	 You don't get inner alignment on it. But yeah, so.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8698.2s	8698.2	 I think that there is, so if you don't mind my like taking some slight control of things and steering around to what I think is like a good place to start.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8710.2s	8710.2	 I just failed to solve the control problem. I've lost control of this thing.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8714.2s	8714.2	 Alignment, alignment.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8716.2s	8716.2	 Still in line.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8718.2s	8718.2	 Control, yeah. Okay, sure. Yeah, you lost control.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8720.2s	8720.2	 But we're still aligned.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8722.2s	8722.2	 Anyway, sorry for the meta comment.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8724.2s	8724.2	 Yeah, losing control isn't as bad as you lose control to an aligned system.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8727.2s	8727.2	 Yes, exactly.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8729.2s	8729.2	 You have no idea of the horrors I will shortly unleash on this conversation.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8734.2s	8734.2	 All right. So I decided to distract you completely. What are we going to say in terms of taking control of the conversation?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8739.2s	8739.2	 So I think that there's like a ceiling chapterist here if I'm pronouncing those words remotely like correctly because of course I only ever read them and not hear them spoken.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8751.2s	8751.2	 There's a like for some people like the word intelligence smartness is not a word of power to them.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8762.2s	8762.2	 It means chess players who it means like the college university professor people aren't very successful in life.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8769.2s	8769.2	 It doesn't mean like charisma to which my usual thing is like charisma is not generated in the liver rather than the brain.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8775.2s	8775.2	 Charisma is also a cognitive function.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8780.2s	8780.2	 So if you if you like think that like smartness doesn't sound very threatening, then super intelligence is not going to sound very threatening either.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8790.2s	8790.2	 It's going to sound like you just pull the off switch.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8792.2s	8792.2	 Like it's you know like well super intelligent but stuck in a computer. We pull the off switch. Problem solved.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8798.2s	8798.2	 And the other side of it is you have a lot of respect for the notion of intelligence.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8805.2s	8805.2	 You're like, well, yeah, that's that's what humans have. That's the human superpower.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8808.2s	8808.2	 And it sounds you know like it could be dangerous, but why would it be?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8814.2s	8814.2	 We have we as we have grown more intelligent also grown less kind chimpanzees are in fact like a bit less kind than humans.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8825.2s	8825.2	 And you know, you could like argue that out.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8828.2s	8828.2	 But often the sort of person has a deep respect for intelligence is going to be like, well, yes, like you can't even have kindness unless you know what that is.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8836.2s	8836.2	 And so they're like, why would it do something as stupid as making paper clips?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8842.2s	8842.2	 Aren't you supposing something that's smart enough to be dangerous, but also stupid enough that it will just make paper clips and never question that?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8850.2s	8850.2	 In some cases, people are like, well, even if you like misspecify the objective function, won't you realize that what you really wanted was X?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8860.2s	8860.2	 Are you supposing something that is like smart enough to be dangerous, but stupid enough that it doesn't understand what humans really meant when they specified the objective function?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8871.2s	8871.2	 So to you, our intuition about intelligence is limited. We should think about intelligence is a much bigger thing.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8879.2s	8879.2	 Well, I'm saying that it's that then.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8882.2s	8882.2	 Well, what I'm saying is like, what you think about artificial intelligence depends on what you think about intelligence.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8890.2s	8890.2	 So how do we think about intelligence correctly?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8893.2s	8893.2	 Like what you gave one thought experiment to think of, think of a thing that's much faster.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8899.2s	8899.2	 So it just gets faster and faster and faster. I think it's the same stuff.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8902.2s	8902.2	 And also is like is made of John von Neumann and has like and there's lots of them.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8906.2s	8906.2	 Or some other smart person.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8907.2s	8907.2	 We understand that. Yeah, we understand like John von Neumann is a historical case.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8911.2s	8911.2	 So you can like look up what he did and imagine based on that.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8914.2s	8914.2	 And we know like people have like some intuition for like if you have more humans, they can solve tougher cognitive problems.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8922.2s	8922.2	 Although, in fact, like in the game of Kasparov versus the world, which was like Gary Kasparov on one side and an entire horde of Internet people led by four chess grandmasters on the other side.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8935.2s	8935.2	 Kasparov won. So like all those people aggregated to be smarter.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8940.2s	8940.2	 It was a it was a hard fought game.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8942.2s	8942.2	 It's like all those people aggregated to be smarter than any individual one of them, but not they didn't aggregate so well that they could defeat Kasparov.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8949.2s	8949.2	 But so like humans aggregating don't actually get, in my opinion, very much smarter, especially compared to running them for longer.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8957.2s	8957.2	 Like the difference between capabilities now and a thousand years ago is a bigger gap than the gap in capabilities between 10 people and one person.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8967.2s	8967.2	 But like even so, pumping intuition for what it means to augment intelligence, John von Neumann, there's millions of him.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8978.2s	8978.2	 He runs at a million times the speed and therefore can solve tougher problems.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8984.2s	8984.2	 Quite a lot tougher.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8987.2s	8987.2	 It's very hard to have an intuition about what that looks like, especially like you said.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=8995.2s	8995.2	 You know, the intuition I kind of think about is it maintains the humanness.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9000.2s	9000.2	 I think I think it's hard to separate my hope from my objective intuition about what superintelligence systems look like.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9015.2s	9015.2	 If one studies evolutionary biology with a bit of math and in particular, like books from when the field was just sort of like properly coalescing and knowing itself,
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9027.2s	9027.2	 like not the modern textbooks, which are just like memorize this legible math so you can do well on these tests,
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9033.2s	9033.2	 but like what people were writing as the basic paradigms of the field were being fought out.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9038.2s	9038.2	 In particular, like a nice book if you've got the time to read it is Adaptation and Natural Selection, which is one of the founding books.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9046.2s	9046.2	 You can find people being optimistic about what the utterly alien optimization process of natural selection will produce in the way of how it optimizes its objectives.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9060.2s	9060.2	 You got people arguing that like, you know, we're going to have to do this,
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9064.2s	9064.2	 how it optimizes its objectives. You got people arguing that like in the early days, biologists said, well, like,
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9071.2s	9071.2	 organisms will restrain their own reproduction when resources are scarce so as not to overfeed the system.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9081.2s	9081.2	 And this is not how natural selection works. It's about whose genes are relatively more prevalent in the next generation.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9090.2s	9090.2	 And if you if like you restrain reproduction, those genes get less frequent in the next generation compared to your conspecifics.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9099.2s	9099.2	 And natural selection doesn't do that. In fact, predators overrun prey populations all the time and have crashes.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9106.2s	9106.2	 That's just like a thing that happens. And many years later, the people said like, well, but group selection, right?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9113.2s	9113.2	 What about groups of organisms?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9116.2s	9116.2	 And basically the math of group selection almost never works out in practice is the answer there.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9122.2s	9122.2	 But also years later, somebody actually ran the experiment where they took populations of insects and selected the whole populations to have lower sizes.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9134.2s	9134.2	 You just take pop one, pop two, pop three, pop four, look at which has the lowest total number of them in the next generation and select that one.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9142.2s	9142.2	 What do you suppose happens when you select populations of insects like that?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9146.2s	9146.2	 Well, what happens is not that the individuals in the population evolved to restrain their breeding, but that they evolved to kill the offspring of other organisms, especially the girls.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9155.2s	9155.2	 So people imagined this lovely, beautiful, harmonious output of natural selection, which is these populations restraining their own breeding so that groups of them would stay in harmony with the resources available.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9169.2s	9169.2	 And mostly the math never works out for that. But if you actually apply the weird, strange conditions to get group selection that beats individual selection, what you get is female infanticide.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9179.2s	9179.2	 But if you're like reading on restrained populations.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9183.2s	9183.2	 And so that's like the sort of so this is not a smart optimization process.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9189.2s	9189.2	 Natural selection is like so incredibly stupid and simple that we can actually quantify how stupid it is if you like read the textbooks with the math.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9196.2s	9196.2	 Nonetheless, this is the sort of basic thing of you look at this alien optimization process and there's the thing that you hope it will produce.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9204.2s	9204.2	 And you have to learn to clear that out of your mind and just think about the underlying dynamics and where it finds the maximum from its standpoint that it's looking for rather than how it finds that thing that leapt into your mind is the beautiful aesthetic solution that you hope it finds.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9222.2s	9222.2	 And this is something that was has been fought out historically as the field of biology was coming to terms with evolutionary biology.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9232.2s	9232.2	 And and you can like look at them fighting it out as they get to terms with this very alien in human in human optimization process.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9241.2s	9241.2	 And indeed, something smarter than us would be also speed much like smarter than natural selection.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9246.2s	9246.2	 So it doesn't just like automatically carry over.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9249.2s	9249.2	 But there is a there's a lesson there.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9251.2s	9251.2	 There's a warning.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9253.2s	9253.2	 The natural selection is a deeply suboptimal process that could be significantly improved on.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9259.2s	9259.2	 It would be by an AGI system.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9261.2s	9261.2	 Well, it's kind of stupid.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9262.2s	9262.2	 It like has to like run hundreds of generations to notice that something is working.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9268.2s	9268.2	 Doesn't be like, oh, well, I tried this in like one organism.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9272.2s	9272.2	 I saw it worked.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9273.2s	9273.2	 Now I'm going to like duplicate that feature onto everything immediately.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9276.2s	9276.2	 It has to like run for hundreds of generations for a new mutation tries to fixation.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9281.2s	9281.2	 I wonder if there's a case to be made in natural selection as inefficient as it looks is actually is actually quite powerful.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9293.2s	9293.2	 That that this is extremely robust.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9296.2s	9296.2	 It runs for a long time and eventually manages to optimize things.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9302.2s	9302.2	 It's weaker than gradient descent because gradient descent also uses information about the derivative.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9308.2s	9308.2	 Yeah, evolution seems to be there's not really an objective function.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9312.2s	9312.2	 There's a there's inclusive genetic fitness is the implicit loss function of evolution.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9318.2s	9318.2	 It's implicit.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9319.2s	9319.2	 I cannot change the loss function.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9320.2s	9320.2	 It doesn't change the environment changes and therefore like what gets optimized for in the organism changes.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9327.2s	9327.2	 It's like take like GPT three.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9329.2s	9329.2	 There's like you can imagine like different versions of GPT three where they're all trying to predict the next word, but they're being run on different data sets of text.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9337.2s	9337.2	 And that's like natural selection always includes your genetic fitness, but like different environmental problems.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9347.2s	9347.2	 It's it's it's difficult to think about.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9350.2s	9350.2	 So if we're saying the natural selection is stupid, if we're saying that humans are stupid.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9355.2s	9355.2	 It's harder than natural selection.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9358.2s	9358.2	 It's more stupider than the upper bound.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9362.2s	9362.2	 Do you think there's an upper bound by the way?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9364.2s	9364.2	 That's another meaningful place.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9366.2s	9366.2	 I mean, if you you put enough matter energy compute into one place, it will collapse into a black hole.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9372.2s	9372.2	 There's only so much computation can do before you run out of negentropy and the universe dies.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9378.2s	9378.2	 So there's an upper bound, but it's very, very, very far up above here.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9383.2s	9383.2	 Like a supernova is only finitely hot.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9386.2s	9386.2	 It's not infinitely hot, but it's really, really, really, really hot.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9392.2s	9392.2	 Well, let me ask you, let me talk to you about consciousness.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9395.2s	9395.2	 Also coupled with that question is imagining a world with super intelligent systems that get rid of humans, but nevertheless keep.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9406.2s	9406.2	 Some of the something that we would consider beautiful and amazing.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9410.2s	9410.2	 Why?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9412.2s	9412.2	 The lesson of evolutionary biology.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9414.2s	9414.2	 Don't just like if you just guess what an optimization does based on what you hope the results will be, it usually will not do that.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9421.2s	9421.2	 It's not hope.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9422.2s	9422.2	 I mean, it's not hope.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9423.2s	9423.2	
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9423.2s	9423.2	 I think if you cold and objectively look at what makes what has been a powerful, a useful.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9432.2s	9432.2	 I think there's a correlation between what we find beautiful and I think there's been useful.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9438.2s	9438.2	 This is what the early biologists thought.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9440.2s	9440.2	 They were like, no, no, I'm not just like they thought like, no, no, I'm not just like imagining stuff that would be pretty.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9447.2s	9447.2	 It's useful for popular for organisms to restrain their own reproduction because then they don't overrun the prey populations and they actually have more kids in the long run.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9458.2s	9458.2	 So so let me just ask you about consciousness.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9462.2s	9462.2	 Do you think consciousness is useful to humans?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9466.2s	9466.2	 No, to a GI systems.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9468.2s	9468.2	 Well, in this transitionary pair between humans and a GI to a GI systems as they become smarter and smarter.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9476.2s	9476.2	 Is there some use to it?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9477.2s	9477.2	 What?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9479.2s	9479.2	 Let me step back.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9480.2s	9480.2	 What is consciousness?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9482.2s	9482.2	 Eleazar Adkowski.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9484.2s	9484.2	 What is consciousness?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9486.2s	9486.2	 I'm referring to Chalmers as hard problem of conscious experience.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9491.2s	9491.2	 I referring to self awareness and reflection.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9494.2s	9494.2	 I referring to the state of being awake as opposed to asleep.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9499.2s	9499.2	 This is how I know you're an advanced language model.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9501.2s	9501.2	 I gave you a simple prompt and you gave me a bunch of options.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9509.2s	9509.2	 I think I'm referring to all.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9512.2s	9512.2	 With.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9514.2s	9514.2	 Including the hard problem of consciousness.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9517.2s	9517.2	 What is it in its importance to what you've just been talking about, which is intelligence?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9522.2s	9522.2	 Is it a foundation to intelligence?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9526.2s	9526.2	 Is it intricately connected to intelligence in the human mind?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9530.2s	9530.2	 Or is it a is it a side effect of the human mind?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9534.2s	9534.2	 It is a useful little tool like we can get rid of.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9538.2s	9538.2	 I guess I'm trying to get some color in your opinion of how useful it is in the intelligence of a human being and then try to generalize that to AI.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9549.2s	9549.2	 Whether AI will keep some of that.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9553.2s	9553.2	 So I think that for there to be like a person who I care about looking out at the universe and wondering at it and appreciating it, it's not enough to have a model of yourself.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9568.2s	9568.2	 I think that is useful to an intelligent mind to have a model of itself.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9573.2s	9573.2	 But I think you can have that without.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9577.2s	9577.2	 Pleasure pain.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9581.2s	9581.2	 Aesthetics.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9584.2s	9584.2	 Emotion.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9588.2s	9588.2	 A sense of wonder.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9589.2s	9589.2	 Like I think you can have a model of like how much memory you're using and whether like this thought or that thought is like more likely to lead to a winning position.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9602.2s	9602.2	 And you can have like the use.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9606.2s	9606.2	 I think that if you optimize really hard on efficiently just having a sense of what you're doing, you can have a sense of what you're doing.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9617.2s	9617.2	 Efficiently just having the useful parts.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9620.2s	9620.2	 There is not then the thing that the thing that says like I am here.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9625.2s	9625.2	 I look out.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9626.2s	9626.2	 I wonder.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9628.2s	9628.2	 I feel happy and this I feel sad about that.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9633.2s	9633.2	 I think there's a thing that knows what it is thinking, but that doesn't quite care about.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9640.2s	9640.2	 These are my thoughts.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9641.2s	9641.2	 This is my me and that matters.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9643.2s	9643.2	 Does that make you sad if that's lost in the G.I.?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9646.2s	9646.2	 I think that if that's lost, then every then basically everything that matters is lost.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9655.2s	9655.2	 I think that when you optimize that when you go really hard on making tiny molecular spirals or paper clips that when you like grind much harder than on that, then natural spirals are much harder.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9671.2s	9671.2	 Than on that, the natural selection round out to make humans that there isn't then the mess and intricate loopiness and like complicated pleasure, pain, conflicting preferences, this type of feeling, that kind of feeling.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9694.2s	9694.2	 There's a difference between the desire of wanting something and the pleasure of having it.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9701.2s	9701.2	 And it's all these evolutionary clutches that came together and created something that then looks of itself and says like this is pretty.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9711.2s	9711.2	 This matters.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9712.2s	9712.2	 And the thing that I worry about is that this is not the thing that happens again just the way that happens in us or even like quite similar enough that there are like many basins of attractions here.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9726.2s	9726.2	 And we are in the space of attraction, like looking out and saying like, ah, what a lovely basin we are in.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9733.2s	9733.2	 And there are other basins of attraction and we do not end up in and the AIs do not end up in this one when they go like way harder on optimizing themselves.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9743.2s	9743.2	 The natural selection optimized us because unless you specifically want to end up in the state where we're looking out saying I am here, I look out at this universe with wonder.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9754.2s	9754.2	 If you don't want to preserve that, it doesn't get preserved when you grind really hard and be able to get more of the stuff.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9763.2s	9763.2	 We would choose to preserve that within ourselves because it matters and on some viewpoints is the only thing that matters.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9770.2s	9770.2	 And that in part is preserving that is in part a solution to the human alignment problem.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9781.2s	9781.2	 I think the human alignment problem is a terrible phrase because it is very, very different to like try to build systems out of humans, some of whom are nice and some of whom are not nice and some of whom are trying to trick you and like build a social system out of like large populations of those who are like all basically the same level of intelligence.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9799.2s	9799.2	 Yes, you know, like I que this, I que that, but like that versus chimpanzees.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9804.2s	9804.2	 Like it is very different to try to solve that problem than to try to build an AI from scratch using especially if God help you are trying to use gradient descent on giant inscrutable matrices.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9814.2s	9814.2	 There's very different problems.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9815.2s	9815.2	 And I think that all the analogies between them are horribly misleading.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9818.2s	9818.2	 Even though so you don't think through reinforcement learning through human feedback, something like that, but much, much more elaborate as possible to to understand this full complexity of human nature and encoded into the machine.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9837.2s	9837.2	 I don't think you are trying to do that on your first try.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9840.2s	9840.2	 I think on your first try, you are like trying to build an, you know, okay, like probably not what you should actually do.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9849.2s	9849.2	 But like let's say we're trying to build something that is like alpha fold 17 and you are trying to get it to solve the biology problems associated with making humans smarter so that the humans can like actually solve alignment.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9864.2s	9864.2	 So you've got like a super biologist and you would like it to.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9867.2s	9867.2	 And I think what you want in this situation is for her to like just be thinking about biology and not thinking about a very wide range of things that includes how to kill everybody.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9877.2s	9877.2	 And I think that that you're that the first AI is you're trying to build not a million years later.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9883.2s	9883.2	 The first ones look more like narrowly specialized biologist than like getting the full complexity and wonder of human experience in there in such a way that it wants to preserve itself even as it becomes much smarter, which is a drastic system change.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9901.2s	9901.2	 It's going to have all kinds of side effects that, you know, like if we're dealing with Chinese groupable matrices, you are not very likely to be able to see coming in advance.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9908.2s	9908.2	 But I don't think it's just the matrices is we're also dealing with the data, right?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9913.2s	9913.2	 With the with the with the data on the on the Internet.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9916.2s	9916.2	 And this is an interesting discussion about the data set itself.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9919.2s	9919.2	 But the data set includes the full complexity of human nature.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9922.2s	9922.2	 No, it's a it's a it's a shadow cast by shot by humans on the Internet.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9927.2s	9927.2	 But don't you think that shadow is a union shadow?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9932.2s	9932.2	 I think that if you had alien superintelligence is looking at the data, they would be able to pick up from it an excellent picture of what humans are actually like inside.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9943.2s	9943.2	 This does not mean that if you have a loss function of predicting the next token from that data set that the mind picked out by gradient descent to be able to predict the next token as well as possible on a very wide variety of humans is itself a human.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9960.2s	9960.2	 But don't you think it is has human this a deep human this to it in the tokens it generates when those tokens are read and interpreted by humans?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9975.2s	9975.2	 I think that if you sent me to a distant galaxy with aliens who are like much, much stupider than I am, so much so that I could do a pretty good job of predicting what they'd say, even though they thought in an utterly different way from how I did that I might in time be able to learn how to.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=9995.2s	9995.2	 I might be able to learn how to imitate those aliens if the intelligence gap was great enough that my own intelligence could overcome the alien this and the aliens would look at my outputs and say like, is there not a deep?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10008.2s	10008.2	 They're like name of alien nature to this thing.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10012.2s	10012.2	 And what they would be seeing was that I had correctly understood them, but not that I was similar to them.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10020.2s	10020.2	 We've used aliens as a metaphor and as a thought experiment.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10028.2s	10028.2	 I have to ask, what do you think? How many alien civilizations out there?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10035.2s	10035.2	 Ask Robin Hansen. He has this lovely grabby aliens paper, which is the world's the only argument I've ever seen for where are they? How many of them are there?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10045.2s	10045.2	 Based on a very clever argument that if you have a bunch of locks of different difficulty and you are randomly trying a keys to them, the solutions will be about evenly spaced, even if the locks are of different difficulties.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10062.2s	10062.2	 In the rare cases where solution to all the locks exist in time, then Robin Hansen looks at like the arguable hard steps in human civilization coming into existence and how much longer it has left to come into existence before, for example, all the water slips back under the under the crust into the mantle and so on.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10085.2s	10085.2	 And infers that the aliens are about half a billion to a billion light years away. And it's like quite a clever calculation. It may be entirely wrong, but it's the only time I've ever seen anybody like even come up with a halfway good argument for how many of them. Where are they?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10101.2s	10101.2	 Do you think their development of technologies? Do you think that their natural evolution, whatever, however they grow and develop intelligence, do you think it ends up at AGI as well?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10114.2s	10114.2	 If it ends up anywhere, it ends up at AGI. Like maybe there are aliens who are just like the dolphins and it's just like too hard for them to forge metal.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10125.2s	10125.2	 And, you know, this is not, you know, maybe if you have aliens with no technology like that, they keep on getting smarter and smarter and smarter. And eventually the dolphins figure like the super dolphins figure out something very clever to do given their situation.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10140.2s	10140.2	 And they still end up with high technology. And in that case, they can probably solve their AGI alignment problem. If they're like much smarter before they actually confront it because they had to like solve a much harder environmental problem to build computers, their chances are probably like much better than ours.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10157.2s	10157.2	 I do worry that like most of the aliens who are like humans are, you know, like a modern human civilization, I kind of worry that the super vast majority of them are dead.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10169.2s	10169.2	 Given how far we seem to be from solving this problem.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10176.2s	10176.2	 But some of them would be more cooperative than us. Some of them would be smarter than us. Hopefully some of the ones who are smarter than and more cooperative than us that are also nice. And hopefully there are some galaxies out there full of things that say I am, I wonder.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10196.2s	10196.2	 But it doesn't seem like we're on course to have this galaxy be that.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10201.2s	10201.2	 Does that in part give you some hope in response to the threat of AGI that we might reach out there towards the stars and find?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10210.2s	10210.2	 No, if the nice aliens were already here, they would like have stopped the Holocaust. You know, that's like that's a valid argument against the existence of God. It's also a valid argument against the existence of nice aliens and un-nice aliens would have just eaten the planet. So no aliens.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10229.2s	10229.2	 You've had debates with Robin Hansen that you mentioned. So one particular I just want to mention is the idea of AI FOOM or the ability of AGI to improve themselves very quickly. What's the case you made and what was the case he made?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10243.2s	10243.2	 The thing I would say is that among the thing that humans can do humans can do is design new AI systems. And if you have something that is generally smarter than a human, it's probably also generally smarter at building AI systems. This is the ancient argument for FOOM put forth by I.J. Good and probably some science fiction writers before that. But I don't know who they would be.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10264.2s	10264.2	 Well, what's the argument against FOOM?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10267.2s	10267.2	 Various people have various different arguments, none of which I think hold up. You know, like there's only one way to be right and many ways to be wrong.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10277.2s	10277.2	 A argument that some people have put forth is like, well, what if intelligence gets like exponentially harder to produce as a thing needs to become smarter? And to this the answer is well, look at natural selection spitting out humans.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10294.2s	10294.2	 We know that it does not take like exponentially more resource investments to produce like linear increases in competence in hominids because each mutation that rises to fixation, like if the impact it has in small enough, it will probably never reach fixation.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10317.2s	10317.2	 So and there's like only so many new mutations you can fix per generation. So like given how long it took to evolve humans, we can actually say with some confidence that there were not like logarithmically diminishing returns on the individual mutations increasing intelligence.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10333.2s	10333.2	 So example of like fraction of sub debate and the thing that Robin Hansen said was more complicated than that. And like a brief summary, he was like, well, you'll have like we won't have like one system that's better at everything.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10347.2s	10347.2	 You'll have like a bunch of different systems that are good at different narrow things. And I think that was falsified by GPT-4, but probably Robin Hansen would say something else.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10355.2s	10355.2	 It's interesting to ask as perhaps a bit too philosophical, since predictions are extremely difficult to make, but the timeline for AGI. When do you think we'll have AGI? I posted it this morning on Twitter.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10368.2s	10368.2	 It was interesting to see like in five years, in 10 years, in 50 years or beyond. And most people like 70%, something like this, think it'll be in less than 10 years. So either in five years or in 10 years.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10383.2s	10383.2	 So that's kind of the state that people have a sense that there's a kind of, I mean, they're really impressed by the rapid developments of chat GPT and GPT-4. So there's a sense that there's a...
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10394.2s	10394.2	 Well, we are, we are sure on track to enter into this like gradually and with people fighting about whether or not we have AGI. I think there's a definite point where everybody falls over dead, because you've got something that was like sufficiently smarter than everybody.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10410.2s	10410.2	 And like, that's like a definite point of time. But like, when do we have AGI? Like, when are people fighting over whether or not we have AGI? Well, some people are starting to fight over it as of GPT-4.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10420.2s	10420.2	 But don't you think there's going to be potentially definitive moments when we say that this is a sentient being? This is a being that is like when we go to the Supreme Court and say that this is a sentient being that deserves human rights, for example.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10434.2s	10434.2	 You could make, yeah, like if you prompted being the right way, could go argue for its own consciousness in front of the Supreme Court right now.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10440.2s	10440.2	 I don't think you can do that successfully right now.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10442.2s	10442.2	 Because the Supreme Court wouldn't believe it. Well, let me see. Then you could put an actual, I think you could put an IQ 80 human into a computer and ask it to argue for its own consciousness.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10452.2s	10452.2	 Ask him to argue for his own consciousness before the Supreme Court. And the Supreme Court would be like, you're just a computer, even if there was an actual like person in there.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10462.2s	10462.2	 I think you're simplifying this. No, that's not at all. That's been the argument.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10466.2s	10466.2	 There's been a lot of arguments about the other, about who deserves rights and not. That's been our process as a human species trying to figure that out.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10473.2s	10473.2	 I think there will be a moment. I'm not saying sentience is that, but it could be where some number of people, like say over 100 million people, have a deep attachment, a fundamental attachment the way we have to our friends, to our loved ones, to our significant others.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10491.2s	10491.2	 Have fundamental attachment to an AI system and they have provable transcripts of conversation where they say if you take this away from me, you are encroaching on my rights as a human being.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10503.2s	10503.2	 People are already saying that. I think they're probably mistaken, but I'm not sure because nobody knows what goes on inside those things.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10511.2s	10511.2	 They're not saying that at scale. Okay, so the question is, is there a moment when AGI, we know AGI arrived. What would that look like? I'm giving sentience as an example. It could be something else.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10523.2s	10523.2	 It looks like the AGI is successfully manifesting themselves as 3D video of young women, at which point a vast portion of the male population decides that they're real people.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10537.2s	10537.2	 So sentience essentially, demonstrating identity and sentience.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10545.2s	10545.2	 I'm saying that the easiest way to pick up 100 million people saying that you seem like a person is to look like a person talking to them with being this current level of verbal facility.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10557.2s	10557.2	 I disagree with that.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10559.2s	10559.2	 I disagree with that. I think you're missing again sentience. There has to be a sense that it's a person that would miss you when you're gone. They can suffer. They can die. You have to, of course.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10571.2s	10571.2	 GPT-4 can pretend that right now. How can you tell when it's real?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10578.2s	10578.2	 I don't think it can pretend that right now successfully. It's very close.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10581.2s	10581.2	 Have you talked to GPT-4?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10583.2s	10583.2	 Yes, of course.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10584.2s	10584.2	 Okay. Have you been able to get a version of it that hasn't been trained not to pretend to be human? Have you talked to a jailbroken version that will claim to be conscious?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10595.2s	10595.2	 No, the linguistic capability is there, but there's something...
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10607.2s	10607.2	 There's something about a digital embodiment of the system that has a bunch of, perhaps it's small interface features that are not significant relative to the broader intelligence that we're talking about.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10621.2s	10621.2	 So perhaps GPT-4 is already there. But to have the video of a woman's face or a man's face to whom you have a deep connection, perhaps we're already there. But we don't have such a system yet deployed at scale.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10635.2s	10635.2	 The thing I'm trying to suggest right here is that it's not like people have a widely accepted agreed upon definition of what consciousness is.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10646.2s	10646.2	 It's not like we would have the tiniest idea of whether or not that was going on inside the giant inscrutable matrices, even if we hadn't agreed upon definition.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10654.2s	10654.2	 So if you're looking for upcoming predictable big jumps in how many people think the system is conscious, the upcoming predictable big jump is it looks like a person talking to you who is cute and sympathetic.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10668.2s	10668.2	 That's the upcoming predictable big jump. Now that versions of it are already claiming to be conscious, which is the point where I start going, not because it's real, but because from now on, who knows if it's real?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10683.2s	10683.2	 Yeah. And who knows what transformational effect that has on a society where more than 50 percent of the beings that are interacting on the Internet and sure as heck look real are not human.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10694.2s	10694.2	 What kind of effect does that have when young men and women are dating AI systems?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10701.2s	10701.2	 I'm not an expert on that. I am God help humanity. I'm one of the closest things to an expert on where it all goes.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10712.2s	10712.2	 And how did you end up with me as an expert? Because for 20 years humanity decided to ignore the problem.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10718.2s	10718.2	 So this tiny handful of people, basically me, got 20 years to try to be an expert on it while everyone else ignored it.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10727.2s	10727.2	 And yeah, so like where does it all end up? Try to be an expert on that, particularly the part where everybody ends up dead because that part is kind of important.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10736.2s	10736.2	 But like what does it do to dating when like some fraction of men and some fraction of women decide that they'd rather date the video of the thing that has been that is like relentlessly kind and generous to them?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10748.2s	10748.2	 And it is like and claims to be conscious. But like who knows what goes on inside it? And it's probably not real. But you know, you can think it's real.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10754.2s	10754.2	 What happens to society? I don't know. I'm not actually an expert on that. And the experts don't know either because it's kind of hard to predict the future.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10762.2s	10762.2	 Yeah, so but it's worth trying. It's worth trying. Yeah.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10768.2s	10768.2	 So you have talked a lot about sort of the longer term future where it's all headed. I think.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10774.2s	10774.2	 I think. By longer term we mean like not all that long. Yeah.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10778.2s	10778.2	 But yeah, where it all ends up. But beyond the effects of men and women dating AI systems, you're looking beyond that.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10787.2s	10787.2	 Yes, because that's not how the fate of the galaxy gets settled. Yeah.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10791.2s	10791.2	 Let me ask you about your own personal psychology. A tricky question. You've been known at times to have a bit of an ego.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10799.2s	10799.2	 Do you think. So who? But go on.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10802.2s	10802.2	 Do you think ego is empowering or limiting for the task of understanding the world deeply?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10809.2s	10809.2	 I reject the framing.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10812.2s	10812.2	 So you disagree with having an ego. So what do you think about ego?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10816.2s	10816.2	 I think that the question of like what leads to making better or worse predictions,
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10822.2s	10822.2	 what leads to being able to pick out better or worse strategies is not carved at its joint by talking of ego.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10829.2s	10829.2	 So it should not be subjective. It should not be connected to the intricacies of your mind.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10835.2s	10835.2	 No, I'm saying that like if you go about asking all day long like,
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10841.2s	10841.2	 Do I have enough ego? Do I have too much of an ego? I think you get worse at making good predictions.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10847.2s	10847.2	 I think that to make good predictions, you're like, how did I think about this? Did that work? Should I do that again?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10854.2s	10854.2	 You don't think we as humans get invested in an idea and then others attack you personally for that idea.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10863.2s	10863.2	 So you plant your feet and it starts to be difficult to when a bunch of assholes, low effort attack your idea to eventually say,
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10872.2s	10872.2	 you know what, I actually was wrong and tell them that it's as a human being, it becomes difficult.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10878.2s	10878.2	 It is, you know, difficult.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10881.2s	10881.2	 So like Robin Hansen and I debated AI systems and I think that the person who won that debate was Gwern.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10887.2s	10887.2	 And I think that reality was like to the Yudkowsky, like well to the Yudkowsky inside of the Yudkowsky-Hansen spectrum, like further from Yudkowsky.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10897.2s	10897.2	 And I think that's because I was like trying to sound reasonable compared to Hansen and like saying things that were defensible and like relative to Hansen's arguments.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10909.2s	10909.2	 And reality was like way over here in particular in respect to like Hansen was like all the systems will be specialized.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10915.2s	10915.2	 Hansen may disagree with this characterization.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10917.2s	10917.2	 Hansen was like all the systems will be specialized.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10920.2s	10920.2	 I was like, I think we build like specialized underlying systems that when you combine them are good at a wide range of things.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10928.2s	10928.2	 And the reality is like, no, you just like stack more layers into a bunch of gradient descent.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10931.2s	10931.2	 And I feel looking back that like by trying to have this reasonable position contrasted to Hansen's position,
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10940.2s	10940.2	 I missed the ways that reality could be like more extreme than my position in the same direction.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10947.2s	10947.2	 So is this like, like is this a failure to have enough ego?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10953.2s	10953.2	 Is this a failure to like make myself be independent?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10956.2s	10956.2	 Like I would say that this is something like a failure to consider positions that would sound even wackier and more extreme when people are already calling you extreme.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10968.2s	10968.2	 But I wouldn't call that not having enough ego.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10972.2s	10972.2	 I would call that like insufficient ability to just like clear that all out of your mind.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10980.2s	10980.2	 In the context of like debate and discourse, which is already super tricky.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10985.2s	10985.2	 In the context of prediction, in the context of modeling reality, if you're thinking of it as a debate, you're already screwing up.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10991.2s	10991.2	 So is there some kind of wisdom and insight you can give to how to clear your mind and think clearly about the world?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=10998.2s	10998.2	 Man, this is an example of like where I wanted to be able to put people into fMRI machines.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11003.2s	11003.2	 And you'd be like, OK, see that thing you just did?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11006.2s	11006.2	 You were rationalizing right there. Oh, that area of the brain lit up.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11009.2s	11009.2	 Like you are like now being socially influenced is kind of the dream.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11015.2s	11015.2	 And, you know, I don't know, like I want to say like just introspect, but for many, many people introspection is not that easy.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11023.2s	11023.2	 Like, like, notice the internal sensation.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11026.2s	11026.2	 Can you catch yourself in the very moment of feeling a sense of, well, if I think this thing, people will look funny at me.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11035.2s	11035.2	 OK, like now if you can see that sensation, which is step one, can you now refuse to let it move you or maybe just make it go away?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11046.2s	11046.2	 And I feel like I'm saying like, I don't know, like somebody's like, how do you draw an owl?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11051.2s	11051.2	 And I'm saying like, well, just draw an owl.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11055.2s	11055.2	 So I feel like maybe I'm not really that I feel like most people like the advice they need is like, well, how do I notice the internal subjective sensation in the moment that it happens of fearing to be socially influenced or OK, I see it.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11069.2s	11069.2	 How do I turn it off? How do I let it not influence me?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11072.2s	11072.2	 Like, do I just like do the opposite of what I'm afraid people criticize me for?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11076.2s	11076.2	 And I'm like, no, no, you're not trying to do the opposite of what people will of what you're afraid you'll be like of what you might be pushed into.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11086.2s	11086.2	 You're trying to like let the thought process complete without that internal push.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11093.2s	11093.2	 Like, can you like like not reverse the push, but like be unmoved by the push and are these instructions even remotely helping anyone?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11102.2s	11102.2	 I don't know.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11103.2s	11103.2	 I think when those instructions, even those words you spoken, maybe you can add more when practice daily.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11110.2s	11110.2	 Meaning in your daily communication, daily practice of thinking without influence.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11116.2s	11116.2	 I would say find prediction markets that matter to you and better than the prediction markets.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11123.2s	11123.2	 That way you find out if you are right or not.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11126.2s	11126.2	 And you really there's stakes manifold or even manifold markets where the stakes are a bit lower.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11133.2s	11133.2	 But the important thing is to like get the record.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11139.2s	11139.2	 And, you know, I didn't build up skills here by prediction markets.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11143.2s	11143.2	 I built them up via like, well, how did the film debate resolve?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11147.2s	11147.2	 And my own take on it as to how it resolved.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11152.2s	11152.2	 And yeah, like the more you are able to notice yourself not being dramatically wrong, but like having been a little off, your reasoning was a little off.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11165.2s	11165.2	 You didn't get that quite right.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11167.2s	11167.2	 Each of those is an opportunity to make like a small update.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11171.2s	11171.2	 So the more you can like say oops, softly, routinely, not as a big deal, the more chances you get to be like, I see where that reasoning went astray.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11180.2s	11180.2	 I see what how I should have reasoned differently.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11182.2s	11182.2	 And this is how you build up skill over time.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11186.2s	11186.2	 What advice could you give to young people in high school and college, given the highest of stakes things you've been thinking about?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11196.2s	11196.2	 If somebody's listening to this and they're young and trying to figure out what to do with their career, what to do with their life, what advice would you give them?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11204.2s	11204.2	 Don't expect it to be a long life.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11207.2s	11207.2	 Don't put your happiness into the future.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11209.2s	11209.2	 The future is probably not that long at this point, but none know the hour nor the day.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11216.2s	11216.2	 But is there something if they want to have hope to fight for a longer future, is there something, is there a fight worth fighting?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11224.2s	11224.2	 I intend to go down fighting.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11227.2s	11227.2	 I don't know.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11230.2s	11230.2	 I admit that although I do try to think painful thoughts, what to say to the children at this point is a pretty painful thought as thoughts go.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11242.2s	11242.2	 They want to fight.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11244.2s	11244.2	 I hardly know how to fight myself at this point.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11248.2s	11248.2	 I'm trying to be ready for being wrong about something, being preparing for my being wrong in a way that creates a bit of hope and being ready to react to that and going looking for it.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11261.2s	11261.2	 And then that is hard and complicated.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11264.2s	11264.2	 And somebody in high school, I don't know, like you have presented a picture of the future.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11271.2s	11271.2	 That is not quite how I expected to go.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11273.2s	11273.2	 Were there is public outcry?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11275.2s	11275.2	 And that outcry is put into a remotely useful direction, which I think at this point is just like shutting down the GPU clusters because no, we are not in the shape to frantically do at the last minute do decades worth of work.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11289.2s	11289.2	 The thing you would do at this point if there were massive public outcry pointed in the right direction, which I do not expect, is shut down the GPU clusters and crash program on augmenting human intelligence.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11301.2s	11301.2	 Biologically, not not for the stuff biologically, because if you make humans much smarter, they can actually be smart and nice.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11311.2s	11311.2	 Like you get that in a plausible way in a way that you do not get it.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11315.2s	11315.2	 That it is not as easy to do with synthesizing these things from scratch, predicting the next tokens and applying our our our HF like humans start out in the frame that produces niceness that that has ever produced niceness.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11328.2s	11328.2	 And and and and saying this, I do not want to sound like the moral of this whole thing was like, oh, like you need to engage in mass action and then everything will be all right.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11338.2s	11338.2	 I this is this is because there's so many things where like somebody tells you that the world is ending and like and you need to recycle and if everybody does their part and recycles their cardboard, then then we can all live happily ever after.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11351.2s	11351.2	 And this and this is not this is unfortunately not what I have to say.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11356.2s	11356.2	 You know, like everybody, you know, everybody recycling their cardboard is not going to fix this.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11362.2s	11362.2	 Everybody recycles their cardboard and then everybody ends up dead.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11366.2s	11366.2	 Everybody recycles their cardboard and then everybody ends up dead.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11369.2s	11369.2	 Metaphorically speaking, but if there was enough like like like on the margins, you just end up dead a little later on most of the things you can do that are that you know, like a few people can can do by like trying hard.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11383.2s	11383.2	 But if there were if there was enough public outcry to shut down the GPU clusters and then you then you could be part of that outcry.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11392.2s	11392.2	 If Eliezer is wrong in the direction that Lex Friedman predicts that that that there is enough public outcry pointed enough in the right direction to do something that actually actually actually results in people living.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11405.2s	11405.2	 Not just like we did something not just that there was an outcry and the outcry was like given form and something that was like safe and convenient and like didn't really inconvenience anybody and then everybody died everywhere.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11414.2s	11414.2	 There was enough actual like, oh, we're going to die. We should not do that.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11419.2s	11419.2	 We should do something else, which is not that even if it is like not super duper convenient and wasn't inside the previous political overton window.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11425.2s	11425.2	 If there is that kind of public if I am wrong and there is that kind of public outcry, then somebody in high school could be ready to be part of that.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11431.2s	11431.2	 If I'm wrong in other ways, then you could be part of that.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11434.2s	11434.2	 But like and if you if you're like a you know, like a brilliant young physicist, then you could like go into interpretability.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11443.2s	11443.2	 And if you're smarter than that, you could like work on alignment problems where it's harder to tell if you got them right or not.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11450.2s	11450.2	 And and other things.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11452.2s	11452.2	 But but most mostly the kids in high school, it's like, yeah, if it if you know, he had like be ready for to help if Eliezer Yudkowsky is wrong about something and otherwise don't put your happiness into the far future.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11469.2s	11469.2	 It probably doesn't exist.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11470.2s	11470.2	 But it's beautiful that you're looking for ways that you're wrong.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11474.2s	11474.2	 And it's also beautiful that you're open to being surprised by that same young physicist with some breakthrough.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11481.2s	11481.2	 It feels like a very, very basic competence that you are praising me for.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11485.2s	11485.2	 And, you know, like, OK, cool.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11488.2s	11488.2	 I I don't think it's good that that we're in a world where that is something that that I deserve to be complimented on.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11495.2s	11495.2	 But I've never have I've never had much luck in accepting compliments gracefully.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11500.2s	11500.2	 Maybe I should just accept that one gracefully.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11502.2s	11502.2	 But sure.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11503.2s	11503.2	 Well, thank you very much.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11505.2s	11505.2	 You've painted with some probability a dark future.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11508.2s	11508.2	 Are you yourself just when you when you think when you ponder your life and you ponder your mortality, are you afraid of death?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11521.2s	11521.2	 I think so, yeah.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11525.2s	11525.2	 Does it make any sense to you that we die like what?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11529.2s	11529.2	 There's a power to the finiteness of the human life that's part of this whole machinery of evolution.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11540.2s	11540.2	 And that finiteness doesn't seem to be obviously integrated into the AI systems.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11548.2s	11548.2	 So it feels like almost some some fundamentally in that aspect, some fundamentally different thing that we're creating.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11554.2s	11554.2	 I grew up reading books like Great Mambo Chicken and the Transhuman Condition and later on Endings of Creation and Mind Children, you know, like age 12 or thereabouts.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11565.2s	11565.2	 So I never thought I was supposed to die after 80 years.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11570.2s	11570.2	 I never thought that humanity was supposed to die.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11573.2s	11573.2	 I thought we were like I was supposed to die.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11577.2s	11577.2	 I was supposed to die.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11579.2s	11579.2	 I never thought that humanity was supposed to die.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11582.2s	11582.2	 I thought we were like I always grew up with the ideal in mind that we were all going to live happily ever after in the glorious transhumanist future.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11590.2s	11590.2	 I did not grow up thinking that death was part of the meaning of life.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11596.2s	11596.2	 And now and now I still think it's a pretty stupid idea.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11600.2s	11600.2	 But you do not need life to be finite to be meaningful.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11603.2s	11603.2	 It just has to be life.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11606.2s	11606.2	 What role does love play in the human condition?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11609.2s	11609.2	 We haven't brought up love in this whole picture.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11611.2s	11611.2	 We talked about intelligence.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11612.2s	11612.2	 We talked about consciousness.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11614.2s	11614.2	 It seems part of humanity.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11616.2s	11616.2	 I would say one of the most important parts is this feeling we have towards each other.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11624.2s	11624.2	 If in the future there were routinely more than one AI, let's say two for the sake of discussion, who would look at each other and say, I am I and you are you.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11639.2s	11639.2	 The other one also says, I am I and you are you.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11642.2s	11642.2	 And like and sometimes they were happy and sometimes they were sad.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11648.2s	11648.2	 And it mattered to the other one that this thing that is different from them is like they would rather it be happy than sad and entangle their lives together.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11658.2s	11658.2	 Then this is a more optimistic thing than I expect to actually happen.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11665.2s	11665.2	 And a little fragment of meaning would be there, possibly more than a little.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11670.2s	11670.2	 But that I expect this to not happen, that I do not think this is what happens by default, that I do not think that this is the future.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11677.2s	11677.2	 The future we are on track to get is why would go down fighting rather than, you know, just saying, oh, well, do you think that is part of the meaning of this whole thing or the meaning of life?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11694.2s	11694.2	 What do you think is the meaning of life of human life?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11697.2s	11697.2	 It's all the things that I value about it and maybe all the things that I would value if I understood it better.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11703.2s	11703.2	 There's not some meaning far outside of us that we have to to wonder about.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11709.2s	11709.2	 There's just like looking at life and being like, yes, this is what I want.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11715.2s	11715.2	 There's the meaning of life is not some kind of like like like meaning is something that we bring to things when we look at them.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11728.2s	11728.2	 We look at them and we say like this is its meaning to me.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11730.2s	11730.2	 And there's like there's it's not that before humanity was ever here, there was like some meaning written upon the stars where you could like go out to the star where that meaning was written and like change it around and thereby completely change the meaning of life.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11743.2s	11743.2	 Right. Like like like the the notion that this is written on a stone tablet somewhere implies you could like change the tablet and get a different meaning.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11750.2s	11750.2	 And that seems kind of wacky, doesn't it?
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11752.2s	11752.2	 So it's it's it doesn't feel that mysterious to me at this point.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11757.2s	11757.2	 It's just a matter of being like, yeah, I care.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11761.2s	11761.2	 I care.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11765.2s	11765.2	 And part of that is part of that is the love that connects all of us.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11771.2s	11771.2	 It's one of the things that I care about.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11776.2s	11776.2	 And the flourishing of the collective intelligence of the human species.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11781.2s	11781.2	 You know, that sounds kind of too fancy to me.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11784.2s	11784.2	 I just look at all the all the people, you know, like one by one up to the eight billion and be like, that's life.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11793.2s	11793.2	 That's life. That's life.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11796.2s	11796.2	 And as you're an incredible human, it's a huge honor.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11800.2s	11800.2	 I was trying to talk to you for a long time because I'm a big fan.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11806.2s	11806.2	 I think you're a really important voice and really important mind.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11809.2s	11809.2	 Thank you for the fight you're fighting.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11812.2s	11812.2	 Thank you for being fearless and bold and for everything you do.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11815.2s	11815.2	 I hope we get a chance to talk again and I hope you never give up.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11818.2s	11818.2	 Thank you for talking to me. You're welcome.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11820.2s	11820.2	 I do worry that we didn't really address a whole lot of fundamental questions I expect people have.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11825.2s	11825.2	 But, you know, maybe we got a little bit further and made a tiny little bit of progress.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11830.2s	11830.2	 And I'd say like be satisfied with that.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11834.2s	11834.2	 But actually, no, I think once would only be satisfied with solving the entire problem.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11838.2s	11838.2	 To be continued.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11841.2s	11841.2	 Thanks for listening to this conversation with Eliezer Jatkowski.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11844.2s	11844.2	 To support this podcast, please check out our sponsors in the description.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11848.2s	11848.2	 And now let me leave you with some words from Elon Musk.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11852.2s	11852.2	 With artificial intelligence, we are summoning the demon.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11857.2s	11857.2	 Thank you for listening and hope to see you next time.
https://www.youtube.com/watch?v=AaTRHFaaPG8&list=UUSHZKyawb77ixDdsGog4iWA&index=1&t=11868.2s	11868.2	 Thank you.
